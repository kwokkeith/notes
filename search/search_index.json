{"config":{"lang":["en"],"separator":"[\\s\\-\\.\\_]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to 50.054 Compiler Design and Program Analysis","text":"<ul> <li>Course Handout.</li> <li>Course Schedule.</li> </ul>"},{"location":"notes/advanced_static_analysis/","title":"50.054 - Advanced Topics in Static Analysis","text":""},{"location":"notes/advanced_static_analysis/#learning-outcomes","title":"Learning Outcomes","text":"<ol> <li>Apply Path Sensitive Analysis to Sign Analysis.</li> <li>Apply Static Analysis to detect software security loopholes.</li> </ol>"},{"location":"notes/advanced_static_analysis/#recall-that-sign-analysis","title":"Recall that sign analysis","text":"<p>The Sign Analysis that we developed in the previous class has some limitation.</p> <p><pre><code>// PA1               // s0 = [ x -&gt; top, t -&gt; top, input -&gt; top ]\n1: x &lt;- input        // s1 = s0[ x -&gt; s0(input) ]\n2: t &lt;- x &gt;= 0       // s2 = lub(s1,s5) [ t -&gt; lub(s1,s5)(x) &gt;== 0 ]\n3: ifn t goto 6      // s3 = s2   \n4: x &lt;- x - 1        // s4 = s3[ x -&gt; s3(x) -- + ]\n5: goto 2            // s5 = s4\n6: y &lt;- Math.sqrt(x) // s6 = s3 \n7: r_ret &lt;- y\n8: ret\n</code></pre> The monotonic equations in the comments are defined based on the following lattice.</p> <p><pre><code>graph\n    A[\"\u22a4\"]---B[-]\n    A---C[0]\n    A---D[+]\n    B---E\n    C---E\n    D---E[\u22a5]</code></pre> And the abstract value operators are defined as </p> -- \\(\\top\\) + - 0 \\(\\bot\\) \\(\\top\\) \\(\\top\\) \\(\\top\\) \\(\\top\\) \\(\\top\\) \\(\\bot\\) + \\(\\top\\) \\(\\top\\) + + \\(\\bot\\) - \\(\\top\\) - \\(\\top\\) - \\(\\bot\\) 0 \\(\\top\\) - + 0 \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) &gt;== \\(\\top\\) + - 0 \\(\\bot\\) \\(\\top\\) \\(\\top\\) \\(\\top\\) \\(\\top\\) \\(\\top\\) \\(\\bot\\) + \\(\\top\\) \\(\\top\\) \\(\\top\\) + \\(\\bot\\) - \\(\\top\\) 0 \\(\\top\\) 0 \\(\\bot\\) 0 \\(\\top\\) 0 + + \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) <p>By converting the equation system into monotonic function </p> <pre><code>f1((s0, s1, s2, s3, s4, s5, s6)) = ( \n    [ x -&gt; top, t -&gt; top, input -&gt; top ], \n    s0[ x -&gt; s0(input) ], \n    lub(s1,s5) [ t -&gt; lub(s1,s5)(x) &gt;== 0 ],\n    s2, \n    s3[ x -&gt; s3(x) -- + ],\n    s4,\n    s3\n)\n</code></pre> <p>when we apply the fixed point algorithm to the <code>f1</code> and the VarSign lattice, we have the following solution</p> <pre><code>s0 = [ x -&gt; top, t -&gt; top, input -&gt; top ], \ns1 = [ x -&gt; top, t -&gt; top, input -&gt; top ],\ns2 = [ x -&gt; top, t -&gt; top, input -&gt; top ],\ns3 = [ x -&gt; top, t -&gt; top, input -&gt; top ],\ns4 = [ x -&gt; top, t -&gt; top, input -&gt; top ],\ns5 = [ x -&gt; top, t -&gt; top, input -&gt; top ],\ns6 = [ x -&gt; top, t -&gt; top, input -&gt; top ]\n</code></pre> <p>At label 6, <code>x</code>'s sign is \\(\\top\\). Such a problem exists in general as static analyses are approximation. Some point in the above analysis causes the result being overly approximated. </p> <ul> <li>Could it be due to the problem of how the abstract operators <code>--</code> and <code>&gt;==</code> are defined? <ul> <li>No, they are as best as we could infer given the variables (operands) are not assigned with concrete values. </li> </ul> </li> <li> <p>Could it be due to the lattice having too few elements (abstract values)? No, it remains as top, even if we introduce new elements such as <code>+0</code> and <code>-0</code> </p> <ul> <li>Let's say we adjust the lattice</li> </ul> <p><pre><code>graph\n    A[\"\u22a4\"]---A1[+0]\n    A[\"\u22a4\"]---A2[-0]\n    A2---B[-]\n    A2---C[0]\n    A1---C[0]\n    A1---D[+]\n    B---E\n    C---E\n    D---E[\u22a5]</code></pre> * and the abstract operators</p> -- \\(\\top\\) +0 -0 + - 0 \\(\\bot\\) \\(\\top\\) \\(\\top\\) \\(\\top\\) \\(\\top\\) \\(\\top\\) \\(\\top\\) \\(\\top\\) \\(\\bot\\) +0 \\(\\top\\) \\(\\top\\) +0 \\(\\top\\) + + \\(\\bot\\) -0 \\(\\top\\) -0 \\(\\top\\) \\(\\top\\) + + \\(\\bot\\) + \\(\\top\\) \\(\\top\\) + \\(\\top\\) + + \\(\\bot\\) - \\(\\top\\) - \\(\\top\\) - \\(\\top\\) - \\(\\bot\\) 0 \\(\\top\\) -0 +0 - + 0 \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) &gt;== \\(\\top\\) +0 -0 + - 0 \\(\\bot\\) \\(\\top\\) +0 +0 +0 +0 +0 +0 \\(\\bot\\) +0 +0 +0 + +0 + +0 \\(\\bot\\) -0 +0 +0 +0 0 +0 +0 \\(\\bot\\) + +0 +0 + +0 + + \\(\\bot\\) - +0 0 +0 0 +0 0 \\(\\bot\\) 0 +0 +0 + 0 + + \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) <ul> <li>It does not help, as it might give <code>t</code> a more precise abtract value but it does not help to improve the result of <code>x</code></li> </ul> </li> </ul> <p>The real cause of the loss of precision is path insensitivity of the sign analysis, i.e. it does not exploit the fact that the path of going in the while loop body is only valid under the pre-condition <code>x&gt;=0</code> and the path of going out of the while loop is only valid under the condition <code>x &lt; 0</code>. </p>"},{"location":"notes/advanced_static_analysis/#path-sensitive-analysis-via-assertion","title":"Path sensitive analysis via assertion","text":"<p>Supposed in the source language level, i.e SIMP, we include the assertion statement. For example, we consider the source program of <code>PA1</code> in SIMP with assertion statements inserted in the body of the while loop and at the following statement of the while loop.</p> <pre><code>// SIMP2\nx = input;\nwhile x &gt;= 0 {\n    assert x &gt;= 0;\n    x = x - 1;\n}\nassert x &lt; 0;\ny = Math.sqrt(x);\nreturn y;\n</code></pre> <p>As we translate the above SIMP program in to Pseudo Assembly, we retain the assertions as instructions</p> <pre><code>// PA2               // s0 = [ x -&gt; top, t -&gt; top, input -&gt; top ]\n1: x &lt;- input        // s1 = s0[ x -&gt; s0(input) ]\n2: t &lt;- x &gt;= 0       // s2 = lub(s1,s6) [ t -&gt; lub(s1,s6)(x) &gt;== 0 ]\n3: ifn t goto 7      // s3 = s2   \n4: assert x &gt;= 0     // s4 = s3[ x -&gt; gte(s3(x), 0) ]\n5: x &lt;- x - 1        // s5 = s4[ x -&gt; s4(x) -- + ]\n6: goto 2            // s6 = s5\n7: assert x &lt; 0      // s7 = s3[ x -&gt; lt(s3(x), 0) ] \n8: y &lt;- Math.sqrt(x) // s8 = s7 \n9: r_ret &lt;- y\n10: ret\n</code></pre> <p>We could add the following monotonic function synthesis case </p> <ul> <li>case \\(l: assert\\ t\\ &gt;=\\ src\\), \\(s_l = join(s_l)[ t \\mapsto gte(join(s_l)(t), join(s_l)(src))]\\)</li> <li>case \\(l: assert\\ t\\ &lt;\\ src\\), \\(s_l = join(s_l)[ t \\mapsto lt(join(s_l)(t), join(s_l)(src))]\\)</li> </ul> <p>Where \\(gte\\) and \\(lt\\) are defined specifically for assertion instructions. The idea is to exploit the comparison operators to \"narrow\" down the range of the abstract signs.</p> gte \\(\\top\\) +0 -0 + - 0 \\(\\bot\\) \\(\\top\\) \\(\\top\\) +0 \\(\\top\\) + \\(\\top\\) +0 \\(\\bot\\) +0 +0 +0 +0 + +0 +0 \\(\\bot\\) -0 -0 0 -0 \\(\\bot\\) 0 -0 \\(\\bot\\) + + + + + + + \\(\\bot\\) - - \\(\\bot\\) - \\(\\bot\\) - \\(\\bot\\) \\(\\bot\\) 0 0 0 0 \\(\\bot\\) 0 0 \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) lt \\(\\top\\) +0 -0 + - 0 \\(\\bot\\) \\(\\top\\) \\(\\top\\) \\(\\top\\) - \\(\\top\\) - - \\(\\bot\\) +0 +0 +0 \\(\\bot\\) +0 \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) -0 -0 0 -0 -0 - - \\(\\bot\\) + + + \\(\\bot\\) + \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) - - - - - - - \\(\\bot\\) 0 0 0 \\(\\bot\\) 0 \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) <p>To show that the above definitions of <code>gte</code> and <code>lt</code> are sound. We can consider the range notation of the abstract values. </p> \\[ \\begin{array}{rcl} \\top &amp; = &amp; [-\\infty, +\\infty] \\\\  +0 &amp; = &amp; [0, +\\infty] \\\\  -0 &amp; = &amp; [-\\infty, 0] \\\\  \\ + &amp; = &amp; [1, +\\infty] \\\\  \\ - &amp; = &amp; [-\\infty, -1] \\\\  0 &amp; = &amp; [0, 0] \\\\  \\bot &amp; = &amp; [+\\infty, -\\infty]  \\end{array} \\] <p>\\([l, h]\\) denotes the set of integers \\(i\\) where \\(l \\leq i \\leq h\\). \\(\\bot\\) is an empty range. We can think of <code>gte</code> as</p> \\[ gte([l_1, h_1], [l_2, h_2]) = [ max(l_1,l_2), min(h_1, +\\infty)] \\] <p>Similiarly we can think of <code>lt</code> as </p> \\[ lt([l_1, h_1], [l_2, h_2]) = [ max(l_1,-\\infty), min(h_1, h_2-1)] \\] <p>where \\(+\\infty - 1 = +\\infty\\)</p> <p>With the adjusted monotonic equations, we can now define the monotonic function <code>f2</code> as follows</p> <pre><code>f2((s0, s1, s2, s3, s4, s5, s6, s7, s8)) = ( \n    [ x -&gt; top, t -&gt; top, input -&gt; top ]\n    s0[ x -&gt; s0(input) ], \n    lub(s1,s6) [ t -&gt; lub(s1,s6)(x) &gt;== 0 ],\n    s2, \n    s3[ x -&gt; gte(s3(x), 0) ],\n    s4[ x -&gt; s4(x) -- + ],\n    s5,\n    s3[ x -&gt; lt(s3(x), 0) ],\n    s7 \n)\n</code></pre> <p>By applying the fixed point algorithm to <code>f2</code> we find the following solution</p> <pre><code>s0 = [ x -&gt; top, t -&gt; top, input -&gt; top ]\ns1 = [ x -&gt; top, t -&gt; top, input -&gt; top ]\ns2 = [ x -&gt; top, t -&gt; +0, input -&gt; top ]\ns3 = [ x -&gt; top, t -&gt; +0, input -&gt; top ]\ns4 = [ x -&gt; +0, t -&gt; +0, input -&gt; top ]\ns5 = [ x -&gt; top, t -&gt; +0, input -&gt; top ]\ns6 = [ x -&gt; -, t -&gt; +0, input -&gt; top]\ns7 = [ x -&gt; -, t -&gt; +0, input -&gt; top]\n</code></pre> <p>which detects that the sign of <code>x</code> at instruction 8 is <code>-</code>.</p>"},{"location":"notes/advanced_static_analysis/#information-flow-analysis","title":"Information Flow Analysis","text":"<p>One widely applicable static analysis is information flow analysis.   </p> <p>The information flow in a program describes how data are evaluated and propogated in the program via variables and operations.</p> <p>The goal of information flow analysis is to identify \"incorrect information flow\". There two main kinds.</p> <ol> <li>Low security level data is being written into high security level resources, AKA tainted flow, e.g. SQL injection.</li> <li>High security level data is being sent to low security level observer. i.e, sensitive resource being read by unauthorized users, AKA, information disclosure.</li> </ol>"},{"location":"notes/advanced_static_analysis/#tainted-flow","title":"Tainted Flow","text":"<p>IN this case, we say that the information flow is tainted if some sensitive information is updated / written by unauthorized users, e.g. </p> <pre><code>String id = request.getParameter(\"id\"); // untrusted user input \nString query = \"SELECT account_number, account_balance FROM customer_data WHERE account_owner_id = \" + id;\ntry {\n    Statement statement = dbconnection.createStatement();\n    ResultSet res = statement.executeQuery( query ); // access sensitive resource\n}\n</code></pre> <p>The above example was adapted from some online example showing what SQL injection vulnerable python code looks like. In this case we could argue that it is a tainted control flow as the untrusted user data is being used directly to access the sensitive resources. </p> <p>When the <code>id</code> is <code>\"' OR 'a'='a'; delete from customer_data; --\"</code>, the malicious user gains the login access and deletes all records from the <code>customer_data</code> table.</p> <p>This can be prevented by using a prepared statement. </p> <pre><code>String id = request.getParameter(\"id\"); // untrusted user input \nString query = \"SELECT account_number, account_balance FROM customer_data WHERE account_owner_id = ?\";\ntry {\n    PreparedStatement pstmt = connection.prepareStatement( query );\n    pstmt.setString(1, id); // pstmt is sanitized before being used.\n    ResultSet results = pstmt.executeQuery(); \n}\n</code></pre> <p>One may argue that using manual source code review should help to identify this kind of issues. The situation gets complicated when the program control is not simple</p> <p>Let's recast the above into SIMP, we would have the vulunerable code as </p> <p><pre><code>id = input();\nquery = \"select \" + id;\nexec(query);\nreturn id;\n</code></pre> We assume that we've extended SIMP to support string values and string concatenation.  The <code>input</code> is a function that prompts the user for input. The <code>exec</code> function is a database builtin function.</p> <p>The following version fixed the vulnerability, assume the <code>sanitize</code> function, sanitizes the input.</p> <pre><code>id = input();\nquery = \"select \";\nquery = sanitize(query, id)\nexec(query);\nreturn id;\n</code></pre> <p>To increase the level of compexlity, let's add some control flow to the example.</p> <pre><code>id = input();\nquery = \"select \" + id;\nwhile (id == \"\") {\n    id = input();\n    query = sanitize(\"select \", id)\n}\nexec(query);\nreturn id;\n</code></pre> <p>In the above, it is not directly clear that the <code>exec()</code> is given a sanitized query. The manual check becomes exponentially hard as the code size grows. </p> <p>We can solve it using a forward may analysis. We define the abstract domain as the following complete lattice. </p> <pre><code>graph\n    tainted --- clean --- bot(\"\u22a5\")</code></pre> <p>We rewrite the above SIMP program into the following PA equivalent.</p> <pre><code>1: id &lt;- input()\n2: query &lt;- \"select \" + id\n3: b &lt;- id == \"\"\n4: ifn b goto \n5: id &lt;- input()\n6: query &lt;- sanitize(\"select\", id) \n7: goto 3\n8: _ &lt;- exec(query)\n9: r_ret &lt;- id \n10: ret\n</code></pre> <p>We define the equation generation rules as follows, </p> \\[ join(s_i) = \\bigsqcup pred(s_i) \\] <p>where \\(pred(s_i)\\) returns the set of predecessors of \\(s_i\\) according to the control flow graph.</p> <p>The monotonic functions can be defined by the following cases.</p> <ul> <li>case \\(l == 0\\), \\(s_0 = \\lbrack x \\mapsto \\bot \\mid x \\in V\\rbrack\\)</li> <li>case \\(l: t \\leftarrow src\\), \\(s_l = join(s_l) \\lbrack t \\mapsto join(s_l)(src)\\rbrack\\)</li> <li>case \\(l: t \\leftarrow src_1\\ op\\ src_2\\), \\(s_l = join(s_l) \\lbrack t \\mapsto (join(s_l)(src_1)\\ \\sqcup \\ join(s_l)(src_2))\\rbrack\\)</li> <li>case \\(l: t \\leftarrow input()\\): \\(s_l = join(s_l) \\lbrack t \\mapsto tainted\\rbrack\\)</li> <li>case \\(l: t \\leftarrow sanitize(src_1, src_2)\\): \\(s_l = join(s_l) \\lbrack t \\mapsto clean\\rbrack\\)</li> <li>other cases: \\(s_l = join(s_l)\\)</li> </ul> <p>Let \\(m\\) be a map lattice object, and \\(src\\) be a PA source operand, the lookup operation \\(m(src)\\) for sign analysis is defined as follows </p> \\[ \\begin{array}{rcl} m(c) &amp; = &amp; clean \\\\ m(t) &amp; = &amp; \\left \\{         \\begin{array}{cc}         v &amp; t \\mapsto v \\in m \\\\          error &amp; otherwise         \\end{array}             \\right . \\\\ \\\\  m(r) &amp; = &amp; error  \\end{array} \\] <p>We inline the equations as comments in the PA code.</p> <pre><code>                                    // s0 = [id -&gt; bot, query -&gt; bot, b -&gt; bot]\n1: id &lt;- input()                    // s1 = s0[ id -&gt; tainted ]  \n2: query &lt;- \"select \" + id          // s2 = s1[ query -&gt; lub(clean, s1(id)) ]\n3: b &lt;- id == \"\"                    // s3 = lub(s2,s7)[ b -&gt; lub( lub(s2,s7)(id), clean) ]\n4: ifn b goto 8                     // s4 = s3\n5: id &lt;- input()                    // s5 = s4[ id -&gt; tainted ]\n6: query &lt;- sanitize(\"select\", id)  // s6 = s5[ query -&gt; clean ]\n7: goto 3                           // s7 = s6\n8: exec(query)                      // s8 = s4                  \n9: r_ret &lt;- id                      // s9 = s8\n10: ret                             \n</code></pre> <p>By applying the above we have the followning monotonic function.</p> <pre><code>f((s0,s1,s2,s3,s4,s5,s6,s7,s8,s9)) = (\n    [id -&gt; bot, query -&gt; bot, b -&gt; bot],\n    s0[ id -&gt; tainted], \n    s1[ query -&gt; lub(clean, s1(id)) ],\n    lub(s2,s7)[ b -&gt; lub( lub(s2,s7)(id), clean) ],\n    s3,\n    s4[ id -&gt; tainted ],\n    s5[ query -&gt; clean ],\n    s6,\n    s4,\n    s8    \n)\n</code></pre> <p>By applying the fixed point algorithm, we find the following solution for the monotonic equations.</p> <p><pre><code>s0 = [id -&gt; bot, query -&gt; bot, b -&gt; bot]\ns1 = [id -&gt; tainted, query -&gt; bot, b -&gt; bot] \ns2 = [id -&gt; tainted, query -&gt; tainted, b -&gt; bot]\ns3 = [id -&gt; tainted, query -&gt; tainted, b -&gt; tainted ]\ns4 = [id -&gt; tainted, query -&gt; tainted, b -&gt; tainted ] \ns5 = [id -&gt; tainted, query -&gt; tainted, b -&gt; tainted ]\ns6 = [id -&gt; tainted, query -&gt; clean, b -&gt; tainted ]   \ns7 = [id -&gt; tainted, query -&gt; clean, b -&gt; tainted ]   \ns8 = [id -&gt; tainted, query -&gt; tainted, b -&gt; tainted ]\ns9 = [id -&gt; tainted, query -&gt; tainted, b -&gt; tainted ]   \n</code></pre> which says that the use of variable <code>query</code> at instruction 8 is risky as <code>query</code> may be tainted at this point.</p>"},{"location":"notes/advanced_static_analysis/#sensitive-information-disclosure","title":"Sensitive Information Disclosure","text":"<p>In this case of incorrect information flow, sensitive information may be written / observable by low access level agents, users or system, directly or indirectly. </p> <pre><code>String username = request.getParameter(\"username\"); \nString password = request.getParameter(\"password\"); // sensitive user input \nString cmd = \"INSERT INTO user VALUES (?, ?)\";\ntry {\n    PreparedStatement pstmt = connection.prepareStatement( cmd );\n    pstmt.setString(1, username); \n    pstmt.setString(2, password);  // user password is saved without hashing?\n    ResultSet results = pstmt.execute(); \n}\n</code></pre> <p>In the above code snippet, we retrieve the username and password from a HTTP form request object and create a user record in the database table. The issue with this piece of code is that the user password is inserted into the database without hashing. This violates the security policy of which the user password is confidential where the database table <code>user</code> is having a lower security level, since it is accessed by the database users. </p> <pre><code>String username = request.getParameter(\"username\"); \nString password = request.getParameter(\"password\"); // sensitive user input \nString cmd = \"INSERT INTO user VALUES (?, ?)\";\ntry {\n    PreparedStatement pstmt = connection.prepareStatement( cmd );\n    String hashed_password = hash(password); // the hash serves as a declassification operation.\n    pstmt.setString(1, username); \n    pstmt.setString(2, hashed_password);  // user password is saved without hashing?\n    ResultSet results = pstmt.execute(); \n}\n</code></pre> <p>In the above modified version, we store the hashed password instead, which is safe since it is hard to recover the raw password based on the hashed password.  The <code>hash</code> function here serves as a declassifier that takes a high security level input and returns a lower secirity level output. </p> <p>We can analyses this kind of information flow that will accept the second snippet and reject the first one. The idea is nearly identical to the tainted analysis, except,</p> <ul> <li>We will have a different lattice for security level.  <pre><code>graph\n    secret --- confidential --- bot(\"\u22a5\")</code></pre></li> </ul> <p>where <code>secret</code> has a higher level of security than <code>confidential</code>. </p> <ul> <li>Instead of using <code>sanitize</code> to increase the level of security, we use <code>hash</code> (or <code>declassify</code>) to lower the level of security.</li> </ul>"},{"location":"notes/code_generation/","title":"50.054 - Code Generation","text":""},{"location":"notes/code_generation/#learning-outcomes","title":"Learning Outcomes","text":"<ol> <li>Name the difference among the target code platforms</li> <li>Apply SSA-based register allocation to generate 3-address code from Pseudo Assembly</li> <li>Handle register spilling</li> <li>Implement the target code generation to JVM bytecode given a Pseudo Assembly Program</li> </ol>"},{"location":"notes/code_generation/#recap-compiler-pipeline","title":"Recap Compiler Pipeline","text":"<pre><code>graph LR\nA[Lexing] --&gt;B[Parsing] --&gt; C[Semantic Analysis] --&gt; D[Optimization] --&gt; E[Target Code Generation]\nD --&gt; C</code></pre> <p>For Target Code Generation, we consider some IR as input, the target code (executable) as the output.</p>"},{"location":"notes/code_generation/#instruction-selection","title":"Instruction Selection","text":"<p>Instruction selection is a process of choosing the target platform on which the language to be executed. </p> <p>There are mainly 3 kinds of target platforms.</p> <ul> <li>3-address instruction<ul> <li>RISC (Reduced Instruction Set Computer) architecture. E.g. Apple PowerPC, ARM, Pseudo Assembly</li> </ul> </li> <li>2-address instruction<ul> <li>CISC (Complex Instruction Set Computer) architecture. E.g. Intel x86</li> </ul> </li> <li>1-address instruction<ul> <li>Stack machine. E.g. JVM</li> </ul> </li> </ul>"},{"location":"notes/code_generation/#assembly-code-vs-machine-code","title":"Assembly code vs Machine code","text":"<p>Note that the instruction formats mentioned here are the human-readable representations of the target code. The actual target code (machine code) is in binary format.</p>"},{"location":"notes/code_generation/#3-address-instruction","title":"3-address instruction","text":"<p>In 3-address instruction target platform, each instruction is set to use 3 addresses in maximum. For instance, the Pseudo Assembly we studied earlier is a kind of 3-address instruction without the hardware restriction.</p> <p>For instance in 3 address instruction, we have instructions that look like </p> <pre><code>x &lt;- 1\ny &lt;- 2\nr &lt;- x + y\n</code></pre> <p>where <code>r</code>, <code>x</code> and <code>y</code> are registers . Alternatively, in some other 3 address instruction format, we express the code fragement above in a prefix notation, </p> <pre><code>load x 1\nload y 2\nadd r x y\n</code></pre> <p>The advantage of having more register (addresses) per instruction allows us to huge room of code optimization while keeping a relative simple and small set of instructions (for instance, consider our Pseudo Assembly has a simple set.)</p>"},{"location":"notes/code_generation/#2-address-instruction","title":"2-address instruction","text":"<p>In 2-address instruction target platform, each instruction has maximum 2 addresses. As a result, some of the single line instruction in 3-address instruction has to be encoded as multiple instructions in 2 address platform. For example, to add <code>x</code> and <code>y</code> and store the result in <code>r</code>, we have to write</p> <pre><code>load x 1\nload y 2\nadd x y\n</code></pre> <p>in the 3<sup>rd</sup> instruction we add the values stored in registers <code>x</code> and <code>y</code>. The sum will be stored in <code>x</code>. In the last statement, we move the result from <code>x</code> to <code>r</code>.</p> <p>As the result, we need fewer registers (in minimum) to carry out operations. On the other hands, the set of instructions in 2-address instruction are often more complex.</p>"},{"location":"notes/code_generation/#1-address-instruction","title":"1-address instruction","text":"<p>In the exterem case, we find some target platform has only 1 address instruction. This kind of target is also known as the P-code (P for Pascal) or the stack machine code. </p> <p>For example for the same program, we need t9o encode it in 1-address instruction as follows</p> <p><pre><code>push 1\npush 2\nadd \nstore r\n</code></pre> In the first instruction, we push the constant 1 to the left operand register (or the 1<sup>st</sup> register). In the second instruction, we push the constant 2 to the right oeprand register (the 2<sup>nd</sup> register). In the 3<sup>rd</sup> instruction, we apply the add operation to sum up the two registers and the result is stored in the first register. The 2<sup>nd</sup> register is cleared (or popped). In the last instruction, we pop the result from the first register store it in a temporary variable <code>r</code></p> <p>The benefit of 1 address intruction is having a minimum and uniform requirement for the hardware. It requrest the least amount registers, for example, JVM has only 3 registers. On the other hand, its instruction set is the most complex.</p>"},{"location":"notes/code_generation/#from-pa-to-3-address-target-platform","title":"From PA to 3-address target platform","text":"<p>In this section, we consider generating code for a target platform that using 3-address instruciton.</p>"},{"location":"notes/code_generation/#register-allocation-problem","title":"Register Allocation Problem","text":"<p>Let's consider the register allocation problem. Recall that in Pseudo Assembly, we have unlimited temporary variables and registers. Among all the examples of PA we seen so far, we did not use any register except for the return register <code>rret</code>.</p> <p>Such an assumption is no longer valid in the code generation phase. We face two major constraints.</p> <ol> <li>Most of the operations can be only applied to registers, not to temporary variables. Operands from temporary variables need to be loaded to some registers before the application of the operation.</li> <li>The number of registers is finite and often limited. This implies that we can't possibly load all the temporary variables to registers. At some point, we need to unload the content of some register to the temporary variable to make room for the next operation.</li> </ol> <p>For example, the following PA program </p> <p><pre><code>// PA1\n1: x &lt;- inpput\n2: y &lt;- x + 1\n3: z &lt;- y + 1\n4: w &lt;- y * z\n5: rret &lt;- w\n6: ret\n</code></pre> has to be translated into</p> <pre><code>1: r0 &lt;- input\n2: r1 &lt;- r0 + 1\n3: r2 &lt;- r1 + 1\n4: r3 &lt;- r1 * r2\n5: rret &lt;- r3\n6: ret\n</code></pre> <p>assuming we have 4 other registers <code>r0</code>, <code>r1</code>, <code>r2</code> and <code>r3</code>, besides <code>rret</code>. We can map the PA variables <code>{x : r0, y : r1, z : r2, w : r3}</code></p> <p>When we only have 3 other registers excluding <code>rret</code> we need to offload some result into some temporary variable. The offloading of the result from registers to temporary variables is also known as register spilling.</p> <pre><code>1: r0 &lt;- input\n2: r1 &lt;- r0 + 1\n3: r2 &lt;- r1 + 1\n4: x  &lt;- r0\n5: r0 &lt;- r1 * r2\n6: rret &lt;- r0\n7: ret\n</code></pre> <p>The above program will work within the hardware constraint (3 extra registers besides <code>rret</code>). Now the register allocation, <code>{x : r0, y : r1, z : r2}</code> is only valid for instructions <code>1-4</code> and the alloction for instructions <code>5-7</code> is <code>{w : r0, y : r1, z: r2}</code>.</p> <p>As we can argue, we could avoid the offloading by mapping <code>w</code> to <code>rret</code> since it is the one being retured. </p> <p><pre><code>1: r0 &lt;- input\n2: r1 &lt;- r0 + 1\n3: r2 &lt;- r1 + 1\n4: rret &lt;- r1 * r2\n5: ret\n</code></pre> However this option is not always possible, as the following the <code>w</code> might not be returned variable in some other examples.</p> <p>We could also avoid the offloading by exploiting the liveness analysis, that <code>x</code> is not live from instruction <code>3</code> onwards, hence we should not even save the result of <code>r0</code> to the temporary variable <code>x</code>.</p> <p><pre><code>1: r0 &lt;- input\n2: r1 &lt;- r0 + 1\n3: r2 &lt;- r1 + 1\n4: r0 &lt;- r1 * r2\n5: rret &lt;- r0\n6: ret\n</code></pre> However this option is not always possible, as in some other situation <code>x</code> is needed later.</p> <p>The Register Allocation Problem is then define as follows.</p> <p>Given a program \\(p\\), and \\(k\\) registers, find an optimal register assignment so that the register spilling is minimized.</p>"},{"location":"notes/code_generation/#interference-graph","title":"Interference Graph","text":"<p>To solve the register allocation problem, we define a data structure called the interference graph. </p> <p>Two temporary variables are interferring each other when they are both \"live\" at the same time in a program.  In the following we include the liveness analysis result as the comments in the program <code>PA1</code>.</p> <pre><code>// PA1\n1: x &lt;- inpput // {input}\n2: y &lt;- x + 1  // {x}\n3: z &lt;- y + 1  // {y}\n4: w &lt;- y * z  // {y,z}\n5: rret &lt;- w   // {w}\n6: ret         // {}\n</code></pre> <p>We conclude that <code>y</code> and <code>z</code> are interfering each other. Hence they should not be sharing the same register. </p> <pre><code>graph TD;\n    input\n    x\n    y --- z\n    w </code></pre> <p>From the graph we can tell that \"at peak\" we need two registers concurrently, hence the above program can be translated to the target code using 2 registers excluding the <code>rret</code> register. </p> <p>For example we annotate the graph with the mapped registers <code>r0</code> and <code>r1</code> </p> <pre><code>graph TD;\n    input[\"input(r0)\"]\n    x[\"x(r0)\"]\n    y[\"y(r0)\"] --- z[\"z(r1)\"]\n    w[\"w(r0)\"]</code></pre> <p>And we can generate the following output </p> <pre><code>1: r0 &lt;- inpput   \n2: r0 &lt;- r0 + 1  \n3: r1 &lt;- r0 + 1  \n4: r0 &lt;- r0 * r1  \n5: rret &lt;- r0   \n6: ret         \n</code></pre>"},{"location":"notes/code_generation/#graph-coloring-problem","title":"Graph Coloring Problem","text":"<p>From the above example, we find that we can recast the register allocation problem into a graph coloring problem. </p> <p>The graph coloring problem is defined as follows.</p> <p>Given a undirected graph, and \\(k\\) colors, find a coloring plan in which no adjacent vertices sharing the same color, if possible. </p> <p>Unfortunately, this problem is NP-complete in general. No efficient algorithm is known.</p> <p>Fortunatley, we do know a subset of graphs in which a polynomial time coloring algorithm exists. </p>"},{"location":"notes/code_generation/#chordal-graph","title":"Chordal Graph","text":"<p>A graph \\(G = (V,E)\\) is chordal if, for all cycle \\(v_1,...,v_n\\) in \\(G\\) with \\(n &gt; 3\\) there exists an edge \\((v_i,v_j) \\in E\\) and \\(i, j \\in \\{1,...,n\\}\\) such that \\((v_i, v_j)\\) is not part of the cycle.</p> <p>For example, the following graph</p> <pre><code>graph TD\n    v1 --- v2 --- v3 --- v4 --- v1\n    v2 --- v4</code></pre> <p>is chordal, because of \\((v_2,v_4)\\).</p> <p>The following graph </p> <pre><code>graph TD\n    v1 --- v2 --- v3 --- v4 --- v1</code></pre> <p>is not chordal, or chordless.</p> <p>It is a known result that a the coloring problem of chordal graphs can be solved in polynomial time.</p>"},{"location":"notes/code_generation/#an-example","title":"An Example","text":"<p>Consider the following PA program with the variable liveness result as comments</p> <pre><code>// PA2\n1: a &lt;- 0           // {}\n2: b &lt;- 1           // {a}\n3: c &lt;- a + b       // {a, b}\n4: d &lt;- b + c       // {b, c}\n5: a &lt;- c + d       // {c, d}\n6: e &lt;- 2           // {a}\n7: d &lt;- a + e       // {a, e}\n8: r_ret &lt;- e + d   // {e, d}\n9: ret \n</code></pre> <p>We observe the interference graph </p> <p><pre><code>graph TD\n    a --- b --- c --- d \n    a --- e --- d</code></pre> and find that it is chordless.</p>"},{"location":"notes/code_generation/#ssa-saves-the-day","title":"SSA saves the day!","text":"<p>With some research breakthroughs in 2002-2006, it was proven that programs in SSA forms are always having chordal interference graph.</p> <p>For example, if we apply SSA conversion to <code>PA2</code></p> <p>We have the following</p> <pre><code>// PA_SSA2\n1: a1 &lt;- 0           // {}\n2: b1 &lt;- 1           // {a1}\n3: c1 &lt;- a1 + b1     // {a1, b1}\n4: d1 &lt;- b1 + c1     // {b1, c1}\n5: a2 &lt;- c1 + d1     // {c1, d1}\n6: e1 &lt;- 2           // {a2}\n7: d2 &lt;- a2 + e1     // {a2, e1}\n8: r_ret &lt;- e1 + d2  // {e1, d2}\n9: ret \n</code></pre> <p>The liveness analysis algorithm can be adapted to SSA with the following adjustment.</p> <p>We define the \\(join(s_i)\\) function as follows</p> \\[ join(s_i) = \\bigsqcup_{v_j \\in succ(v_i)} \\Theta_{i,j}(s_j)  \\] <p>where \\(\\Theta_{i,j}\\) is a variable substitution derived from phi assignment of the labeled instruction at \\(j : \\overline{\\phi}\\ instr\\). </p> \\[ \\begin{array}{rcl} \\Theta_{i,j} &amp; = &amp; \\{ (t_i/t_k) \\mid t_k = phi(..., i : t_i, ...) \\in \\overline{\\phi} \\} \\end{array} \\] <p>The monotonic functions can be defined by the following cases.</p> <ul> <li>case \\(l: \\overline{\\phi}\\ ret\\), \\(s_l = \\{\\}\\)</li> <li>case \\(l: \\overline{\\phi}\\ t \\leftarrow src\\), \\(s_l = join(s_l) - \\{ t \\} \\cup var(src)\\)</li> <li>case \\(l: \\overline{\\phi}\\ t \\leftarrow src_1\\ op\\ src_2\\), \\(s_l = join(s_l) - \\{t\\} \\cup var(src_1) \\cup var(src_2)\\)</li> <li>case \\(l: \\overline{\\phi}\\ r \\leftarrow src\\), \\(s_l = join(s_l) \\cup var(src)\\)</li> <li>case \\(l: \\overline{\\phi}\\ r \\leftarrow src_1\\ op\\ src_2\\), \\(s_l = join(s_l) \\cup var(src_1) \\cup var(src_2)\\)</li> <li>case \\(l: \\overline{\\phi}\\ ifn\\ t\\ goto\\ l'\\), \\(s_l = join(s_l) \\cup \\{ t \\}\\)</li> <li>other cases: \\(s_l = join(s_l)\\)</li> </ul> <p>Now the interference graph of the <code>PA_SSA2</code> is as follows</p> <p><pre><code>graph TD;\n    a1 --- b1 --- c1 --- d1\n    a2 --- e1 --- d2</code></pre> which is chordal.</p>"},{"location":"notes/code_generation/#coloring-interference-graph-generated-from-ssa","title":"Coloring Interference Graph generated from SSA","text":"<p>According to the findings of Budimlic's work and Hack's work, coloring the interference graph generated from an SSA program in in-order traversal of dominator tree gives us the optimal coloring. </p> <p>In Hack's paper, it was discussed that the elimination step should be done in the post-order traveral of the dominator tree. From graph coloring problem, we know that the order of coloring is the reverse of the vertex eliminiation order.</p> <p>In the context of PA, the in-order traversal of the dominator tree is always the same order of the instructions being labeled (assuming we generate the PA using the maximal munch algorithm introduced in the earlier lesson.)</p> <p>Therefore we can color the above graph as follows,</p> <pre><code>graph TD;\n    a1(\"a1(r0)\") --- b1(\"b1(r1)\") --- c1(\"c1(r0)\") --- d1(\"d1(r1)\")\n    a2(\"a2(r0)\") --- e1(\"e1(r1)\") --- d2(\"d2(r0)\")</code></pre> <p>From now onwards until the next section (JVM Bytecode generatoin), we assume that program to be register-allocated must be in SSA form.</p> <p>Given that the program interference graph is chordal, the register allocation can be computed in polymomial type.</p> <p>Instead of using building the interference graph, we consider using the live range table of an SSA program, </p> <p>In the following table (of <code>PA_SSA2</code>), the first row contains the program labels and the first column defines the variables and the last column is the allocated register. An <code>*</code> in a cell <code>(x, l)</code> represent variable <code>x</code> is live at program location <code>l</code>.</p> var 1 2 3 4 5 6 7 8 9 reg a1 * * r0 b1 * * r1 c1 * * r0 d1 * r1 a2 * * r0 e1 * * r1 d2 * r0 <p>At any point, (any column), the number of <code>*</code> denotes the number of live variables concurrently. The above tables show that at any point in-time, the peak of the register usage is <code>2</code> (in some literature, it is also known as the chromatic of the interference graph). Therefore, minimumally we need 2 registers to allocate the above program without spilling.</p>"},{"location":"notes/code_generation/#register-spilling","title":"Register Spilling","text":"<p>However register spilling is avoidable due to program complexity and limit of hardware. </p> <p>Let's consider another example </p> <pre><code>// PA3\n1: x &lt;- 1       // {}\n2: y &lt;- x + 1   // {x}\n3: z &lt;- x * x   // {x,y}\n4: w &lt;- y * x   // {x,y,z}\n5: u &lt;- z + w   // {z,w}\n6: r_ret &lt;- u   // {u}  \n7: ret          // {}\n</code></pre> <p>The SSA form is identical to the above, since there is no variable re-assignment. In the comment, we include the result of the liveness analysis.</p> var 1 2 3 4 5 6 7 reg x * * * y * * z * * w * u * <p>From the live range table able, we find that at peak i.e. instruction <code>4</code>, there are 3 live variables currently. We would need three registers for the allocation.</p> <p>What if we only have two registers? Clearly, we need to \"sacrifice\" some live variable at instruction <code>4</code>, by spilling it back to the temporary variable and reloading before it is needed again. But which one shall we \"sacrifice\"? There are a few options here.</p> <ol> <li>Spill the least urgently needed live variable. Recall that the liveness analysis is a may analaysis, its result is an over-approximation. Some live variables might not be needed at this point.</li> <li>Spill the live variable that interfere the most. This option works for the bruteforce searching coloring algorithm, the idea was to reduce the level of interference so that the remaining graph without this variable can be colored. </li> </ol> <p>For now let's take the first option. Suppose we extend the liveness analysis to keep track of the label where a variable is marked live.</p> <pre><code>// PA3\n1: x &lt;- 1       // {}\n2: y &lt;- x + 1   // {x(3)}\n3: z &lt;- x * x   // {x(3),y(4)}\n4: w &lt;- y * x   // {x(4),y(4),z(5)}\n5: u &lt;- z + w   // {z(5),w(5)}\n6: r_ret &lt;- u   // {u(6)}  \n7: ret          // {}\n</code></pre> <p>From the above results, we can conclude that at instruction <code>4</code>, we should sacrifice the live variable <code>z</code>, because <code>z</code> is marked live at label <code>5</code> which is needed in the instruction one-hop away in the CFG, compared to <code>x</code> and <code>y</code> which are marked live at label <code>4</code>. In other words, <code>z</code> is not as urgently needed compared to <code>x</code> and <code>y</code>. </p> var 1 2 3 4 5 6 7 reg x * * * r0 y * * r1 z - * w * u * <p>From the above, we find that the graph is colorable again. However register spilling requires some extra steps. First at label <code>3</code>, variable is <code>z</code> is some register, either <code>r0</code> or <code>r1</code>, assuming in the target code operation <code>*</code> can use the same register for both operands and the result. We encounter another problem. To spill <code>z</code> (from the register) to the temporary variable, we need to figure out which other live variable to be swapped out so that the spilling can be done. Let's illustrate using the same example. </p> <p><pre><code>// PA3_REG\n1: r0 &lt;- 1        // x is r0\n2: r1 &lt;- r0 + 1   // y is r1\n3: ?? &lt;- r0 * r0  // what register should hold the result of x * x, before spilling it to `z`?\n</code></pre> where the comments indicate what happens after the label instruction is excuted.</p> <p>There are two option here</p> <ol> <li><code>??</code> is <code>r1</code>. It implies that we need to spill <code>r1</code> to <code>y</code> first after instruction <code>2</code> and then spill <code>r1</code> to <code>z</code> after instruction <code>3</code>, and load <code>y</code> back to <code>r1</code> after instruction <code>3</code> before instruction <code>4.</code></li> <li><code>??</code> is <code>r0</code>. It implies that we need to spill <code>r0</code> to <code>z</code> first after instruction <code>2</code> and then spill <code>r0</code> to <code>z</code> after instruction <code>3</code>, and load <code>x</code> back to <code>r0</code> after instruction <code>3</code> before instruction <code>4.</code></li> </ol> <p>In this particular example, both options are equally good (or equally bad). In general, we can apply the heuristic of choosing the conflicting variable whose live range ends earlier, hopefully the main subject of spilling (<code>z</code> in this example) is not needed until then. </p> <p>Now let's say we pick the first option, the register allocation continues </p> var 1 2 3 4 5 6 7 reg x * * * r0 y * * r1 z - * r1 w * r0 u * r1 <p>where <code>-</code> indicates taht <code>z</code> is being spilled from <code>r1</code> before label <code>4</code> and it needs to be loaded back to <code>r1</code> before label <code>5</code>.  And the complete code of <code>PA3_REG</code> is as follows</p> <p><pre><code>// PA3_REG\n1: r0 &lt;- 1        // x is r0\n2: r1 &lt;- r0 + 1   // y is r1\n   y  &lt;- r1       // temporarily save y\n3: r1 &lt;- r0 * r0  // z is r1 \n   z  &lt;- r1       // spill to z\n   r1 &lt;- y        // y is r1\n4: r0 &lt;- r1 * r0  // w is r0 (x,y are dead afterwards)\n   r1 &lt;- z        // z is r1\n5: r1 &lt;- r1 + r0  // u is r1 (z,w are dead afterwards)\n6: r_ret &lt;- r1\n7: ret\n</code></pre> In the above, assume that in the target platform, a label can be associated with a sequence of instructions, (which is often the case).</p> <p>As an exercise, work out what if we save <code>x</code> temporarily instead of <code>y</code> at label <code>2</code>.</p>"},{"location":"notes/code_generation/#register-allocation-for-phi-assignments","title":"Register allocation for phi assignments","text":"<p>What remains to address is the treatment of the phi assignments.</p> <p>Let's consider a slightly bigger example. </p> <p><pre><code>// PA4\n1: x &lt;- input   // {input}\n2: s &lt;- 0       // {x}\n3: c &lt;- 0       // {s,x}\n4: b &lt;- c &lt; x   // {c,s,x}\n5: ifn b goto 9 // {b,c,s,x}\n6: s &lt;- c + s   // {c,s,x}\n7: c &lt;- c + 1   // {c,s,x}\n8: goto 4       // {c,s,x}\n9: r_ret &lt;- s   // {s}\n10: ret         // {}\n</code></pre> In the above we find a sum program with liveness analysis results included as comments.</p> <p>Let's convert it into SSA.</p> <p><pre><code>// PA_SSA4\n1: x1 &lt;- input1  // {input1(1)}\n2: s1 &lt;- 0       // {x1(4)}\n3: c1 &lt;- 0       // {s1(4),x1(4)}\n4: c2 &lt;- phi(3:c1, 8:c3)\n   s2 &lt;- phi(3:s1, 8:s3)\n   b1 &lt;- c2 &lt; x1 // {c2(4),s2(6,9),x1(4)}\n5: ifn b1 goto 9 // {b1(5),c2(6),s2(6,9),x1(4)}\n6: s3 &lt;- c2 + s2 // {c2(6),s2(6),x1(4)}\n7: c3 &lt;- c2 + 1  // {c2(7),s3(4),x1(4)}\n8: goto 4        // {c3(4),s3(4),x1(4)}\n9: r_ret &lt;- s2   // {s2(9)}\n10: ret          // {}\n</code></pre> We put the liveness analysis results as comments. </p> <p>There are a few options of handling phi assignments.</p> <ol> <li>Treat them like normal assignment, i.e. translate them back to move instruction (refer to \"SSA back to Pseudo Assembly\" in the name analysis lesson.) This is the most conservative approach definitely work, but not necessary giving us optimized code</li> <li>Ensure the variables in the phi assignments sharing the same registers. </li> </ol> <p>Let's consider the first approach </p>"},{"location":"notes/code_generation/#conservative-approach","title":"Conservative approach","text":"<p>When we translate the SSA back to PA</p> <pre><code>// PA_SSA_PA4\n1: x1 &lt;- input1  // {input1(1)}\n2: s1 &lt;- 0       // {x1(4)}\n3: c1 &lt;- 0       // {s1(3.1),x1(4)}\n3.1: c2 &lt;- c1     \n     s2 &lt;- s1    // {s1(3.1),x1(4),c1(3.1)}\n4: b1 &lt;- c2 &lt; x1 // {c2(4),s2(6,9),x1(4)}\n5: ifn b1 goto 9 // {b1(5),c2(6),s2(6,9),x1(4)}\n6: s3 &lt;- c2 + s2 // {c2(6),s2(6),x1(4)}\n7: c3 &lt;- c2 + 1  // {c2(7),s3(7.1),x1(4)}\n7.1: c2 &lt;- c3\n     s2 &lt;- s3    // {s3(7.1),x1(4),c3(7.1)}\n8: goto 4        // {c2(4),s2(6,9),x1(4)}\n9: r_ret &lt;- s2   // {s2(9)}\n10: ret          // {}\n</code></pre> <p>It is clear that the program is allocatable without spilling with 4 registers. Let's challenge ourselves with just 3 registers.</p> var 1 2 3 3.1 4 5 6 7 7.1 8 9 10 reg input1 * r0 x1 * * * * - - - - - r1 s1 * * r2 c1 * r0 s2 * * * * * r2 c2 * * * * * r0 b1 * r1 s3 * * r2 c3 * r0 <p>At the peak of the live variables, i.e. instruction <code>5</code>, we realize that <code>x1</code> is live but not urgently needed until <code>4</code> which is 5-hop away from the current location. Hence we spill it from register <code>r1</code> to the temporary variable to free up <code>r1</code>.  Registers are allocated by the next available in round-robin manner.</p> <pre><code>// PA4_REG1\n1: r0 &lt;- input1  // input is r0\n   r1 &lt;- r0      // x1 is r1\n2: r2 &lt;- 0       // s1 is r2\n3: r0 &lt;- 0       // c1 is r0\n                 // c2 is r0 \n                 // s2 is r2\n                 // no need to load r1 from x1\n                 // b/c x1 is still active in r1\n                 // from 3 to 4\n4: x1 &lt;- r1      // spill r1 to x1\n   r1 &lt;- r0 &lt; r1 // b1 is r1\n5: ifn r1 goto 9 // \n6: r2 &lt;- r0 + r2 // s3 is r2\n7: r0 &lt;- r0 + 1  // c3 is r0\n                 // c2 is r0\n                 // s2 is r2\n8: r1 &lt;- x1      // restore r1 from x1\n   goto 4        // b/c x1 is inactive but needed in 4\n9: r_ret &lt;- r2   // \n10: ret          // \n</code></pre> <p>What if at instruction <code>7</code>, we allocate <code>r1</code> to <code>s3</code> instead of <code>r2</code>? Thanks to some indeterminism, we could have a slightly different register allocation as follows</p> var 1 2 3 3.1 4 5 6 7 7.1 8 9 10 reg input1 * r0 x1 * * * * - - - - - r1 s1 * * r2 c1 * r0 s2 * * * * * r2 c2 * * * * * r0 b1 * r1 s3 * * r1 c3 * r2 <pre><code>// PA4_REG2\n1: r0 &lt;- input1  // input is r0\n   r1 &lt;- r0      // x1 is r1\n2: r2 &lt;- 0       // s1 is r2\n3: r0 &lt;- 0       // c1 is r0\n                 // c2 is r0 \n                 // s2 is r2\n                 // no need to load r1 from x1\n                 // b/c x1 is still active in r1\n                 // from 3 to 4\n4: x1 &lt;- r1      // spill r1 to x1\n   r1 &lt;- r0 &lt; r1 // b1 is r1\n5: ifn r1 goto 9 // \n6: r1 &lt;- r0 + r2 // s3 is r1\n7: r2 &lt;- r0 + 1  // c3 is r2\n7.1: r0 &lt;- r2    // c2 is r0  \n     r2 &lt;- r1    // s2 is r2\n8: r1 &lt;- x1      // restore r1 from x1 \n   goto 4        // b/c x1 is inactive but needed in 4\n9: r_ret &lt;- s2   \n10: ret          \n</code></pre> <p>In this case we have to introduce some additional register shuffling at <code>7.1</code>. Compared to <code>PA4_REG1</code>, this result is less efficient.</p>"},{"location":"notes/code_generation/#register-coalesced-approach-ensure-the-variables-in-the-phi-assignments-sharing-the-same-registers","title":"Register coalesced approach - Ensure the variables in the phi assignments sharing the same registers","text":"<p>Note that we should not enforce the variable on the LHS of a phi assignment to share the same register as the operands on the RHS. Otherwise, we could lose the chordal graph property of SSA. </p> <p>What we could construct the live range table as follow.</p> var 1 2 3 4 5 6 7 8 9 10 reg input1 * r0 x1 * * * - - - - r1 s1 * r2 c1 r0 s2 * * * * r2 c2 * * * * r0 b1 * r1 s3 * * r2 c3 * r0 <p>Although from the above we find <code>c1</code> seems to be always dead, but it is not, because its value is merged into c2 in label <code>4</code>. This is because in our SSA language, the phi assignment is not an instruction alone while liveness analysis is performed on per instruction level.</p> <p>We also take note we want to <code>c1</code> and <code>c3</code> to share the same register, and <code>s1</code> and <code>s3</code>to share the same register. Hence we can allocate the 3 registers according to the above plan. In this case, we have the same result as the first attempt in the conservative approach <code>PA4_REG1</code>.</p> <p>Note that this approach is not guanranteed to produce more efficient results than the conversvative approach. </p>"},{"location":"notes/code_generation/#summary-so-far","title":"Summary so far","text":"<p>To sum up the code generation process from PA to 3-address target could be carried out as follows,</p> <ol> <li>Convert the PA program into a SSA.</li> <li>Perform Liveness Analysis on the SSA. </li> <li>Generate the live range table based on the liveness analysis results.</li> <li>Allocate registers based on the live range table. Detect potential spilling.</li> <li>Depending on the last approach, either<ol> <li>convert SSA back to PA and generate the target code according to the live range table, or </li> <li>generate the target code from SSA with register coalesced for the phi assignment operands.</li> </ol> </li> </ol>"},{"location":"notes/code_generation/#further-reading-for-ssa-based-register-allocation","title":"Further Reading for SSA-based Register Allocation","text":"<ul> <li>https://compilers.cs.uni-saarland.de/papers/ssara.pdf</li> <li>https://dl.acm.org/doi/10.1145/512529.512534</li> </ul>"},{"location":"notes/code_generation/#jvm-bytecode-reduced-set","title":"JVM bytecode (reduced set)","text":"<p>In this section, we consider the generated JVM codes from PA. </p> \\[ \\begin{array}{rccl} (\\tt JVM\\ Instructions) &amp; jis &amp; ::= &amp; [] \\mid ji\\ jis\\\\  (\\tt JVM\\ Instruction) &amp; ji &amp; ::= &amp; ilabel~l \\mid iload~n \\mid istore~n \\mid iadd \\mid isub \\mid imul \\\\  &amp; &amp; &amp; \\mid if\\_icmpge~l \\mid if\\_icmpne~l \\mid igoto~l \\mid  sipush~c \\mid ireturn\\\\ (\\tt JVM\\ local\\ vars) &amp; n &amp; ::= &amp; 1 \\mid 2 \\mid ... \\\\  (\\tt constant) &amp; c &amp; ::= &amp; -32768 \\mid ... \\mid 0 \\mid ... \\mid 32767  \\end{array} \\] <p>As mentioned, JVM has 3 registers</p> <ol> <li>a register for the first operand and result</li> <li>a register for the second operand</li> <li>a register for controlling the state of the stack operation (we can't used.)</li> </ol> <p>Technically speaking we only have 2 registers.</p> <p>An Example of JVM byte codes is illustrated as follows</p> <p>Supposed we have a PA program as follows, <pre><code>1: x &lt;- input\n2: s &lt;- 0\n3: c &lt;- 0\n4: b &lt;- c &lt; x\n5: ifn b goto 9\n6: s &lt;- c + s\n7: c &lt;- c + 1\n8: goto 4\n9: _ret_r &lt;- s\n10: ret\n</code></pre> For ease of reasoning, we assume that we map PA temporary variables to numerical JVM variables, as <code>input</code> to <code>1</code>, <code>x</code> to <code>2</code>, <code>s</code> to <code>3</code>, <code>c</code> to <code>4</code> (and <code>b</code> to <code>5</code>, though <code>b</code> is not needed in the JVM instruction). We also map the PA label (the useful ones) to JVM label. <code>4</code> to <code>l1</code> and <code>9</code> to <code>l2</code>.</p> <pre><code>iload 1      // push the content of input to register 0\nistore 2     // pop register 0's content to x,  \nsipush 0     // push the value 0 to register 0\nistore 3     // pop register 0 to s\nsipush 0     // push the value 0 to register 0\nistore 4     // pop register 0 to c\nilabel l1    // mark label l1\niload 4      // push the content of c to register 0\niload 2      // push the content of x to register 1\nif_icmpge l2 // if register 0 &gt;= register 1 jump, \n             // regardless of the comparison pop both registers\niload 4      // push the content of c to register 0\niload 3      // push the content of s to register 1\niadd         // sum up the r0 and r1 and result remains in register 0\nistore 3     // pop register 0 to s\niload 4      // push the content of c to register 0\nsipush 1     // push a constant 1 to register 1\niadd        \nistore 4     // pop register 0 to c\nigoto l1\nilabel l2\niload 3      // push the content of s to register 0\nireturn\n</code></pre>"},{"location":"notes/code_generation/#jvm-bytecode-operational-semantics","title":"JVM bytecode operational semantics","text":"<p>To describe the operational semantics of JVM bytecodes, we define the following meta symbols.</p> \\[ \\begin{array}{rccl} (\\tt JVM\\ Program) &amp; J &amp; \\subseteq &amp; jis \\\\ (\\tt JVM\\ Environment) &amp; \\Delta &amp; \\subseteq &amp; n \\times c \\\\  (\\tt JVM\\ Stack) &amp; S &amp; =  &amp; \\_,\\_ \\mid c,\\_ \\mid c,c   \\end{array} \\] <p>An JVM program is a sequence of JVM instructions. \\(\\Delta\\) is local environment maps JVM variables to constants. \\(S\\) is a 2 slots stack where the left slot is the bottom (\\(r_0\\)) and the right slot is the top (\\(r_1\\)). \\(\\_\\) denotes that a slot is vacant.</p> <p>We can decribe the operational semantics of JVM byte codes using the follow rule form</p> \\[  J \\vdash (\\Delta, S, jis) \\longrightarrow (\\Delta', S', jis') \\] <p>\\(J\\) is the entire program, it is required when we process jumps and conditional jump, the rule rewrites a configuration \\((\\Delta, S, jis)\\) to the next configuration \\((\\Delta', S', jis')\\), where \\(\\Delta\\) and \\(\\Delta'\\) are the local environments, \\(S\\) and \\(S'\\) are the stacks, \\(jis\\) and \\(jis'\\) are the currrent and next set of instructions to be processed. </p> \\[ \\begin{array}{rc} (\\tt sjLoad1) &amp; J \\vdash (\\Delta, \\_, \\_, iload\\ n;jis) \\longrightarrow (\\Delta, \\Delta(n), \\_, jis) \\\\ \\\\  (\\tt sjLoad2) &amp; J \\vdash (\\Delta, c, \\_, iload\\ n;jis) \\longrightarrow (\\Delta, c, \\Delta(n), jis) \\\\ \\\\  (\\tt sjPush1) &amp; J \\vdash (\\Delta, \\_, \\_, sipush\\ c;jis) \\longrightarrow (\\Delta, c, \\_, jis) \\\\ \\\\  (\\tt sjPush2) &amp; J \\vdash (\\Delta, c_0, \\_, sipush\\ c_1;jis) \\longrightarrow (\\Delta, c_0, c_1, jis) \\end{array} \\] <p>The rules \\((\\tt sjLoad1)\\) and  \\((\\tt sjLoad2)\\) handles the loading variable's content to the stack registers.  The rules \\((\\tt sjPush1)\\) and  \\((\\tt sjPush2)\\) handles the loading constant to the stack registers. </p> \\[ \\begin{array}{rc} (\\tt sjLabel) &amp; J \\vdash (\\Delta, r_0, r_1, ilabel\\ l;jis) \\longrightarrow (\\Delta, r_0, r_1, jis) \\\\ \\\\  \\end{array} \\] <p>The rule \\((\\tt sjLabel)\\) processes the \\(ilabel\\ l\\) instruction. It is being skipped, because it serves as a syntactical marking (refer to the \\(codeAfterLabel()\\) function below), has no impact to the semantic operation.</p> \\[ \\begin{array}{rc} (\\tt sjStore) &amp; J \\vdash (\\Delta, c, \\_, istore\\ n;jis) \\longrightarrow (\\Delta \\oplus(n,c), \\_, \\_, jis) \\\\ \\\\  \\end{array} \\] <p>The rule \\((\\tt sjStore)\\) processes the \\(istore\\ n\\) instruction by popping the register \\(r_0\\) from the stack and store its content with variable \\(n\\) in \\(\\Delta\\).</p> \\[ \\begin{array}{rc} (\\tt sjAdd) &amp; J \\vdash (\\Delta, c_0, c_1, iadd;jis) \\longrightarrow (\\Delta, c_0+c_1, \\_, jis) \\\\ \\\\  (\\tt sjSub) &amp; J \\vdash (\\Delta, c_0, c_1, isub;jis) \\longrightarrow (\\Delta, c_0-c_1, \\_, jis) \\\\ \\\\  (\\tt sjMul) &amp; J \\vdash (\\Delta, c_0, c_1, imul;jis) \\longrightarrow (\\Delta, c_0*c_1, \\_, jis)   \\end{array} \\] <p>The rules \\((\\tt sjAdd)\\), \\((\\tt sjSub)\\) and \\((\\tt sjMul)\\) process the binary operation assuming both registers in the stack holding some constants. The result of the computation is held by \\(r_0\\) while \\(r_1\\) becomes empty.</p> \\[ \\begin{array}{rc} (\\tt sjGoto) &amp; J \\vdash (\\Delta, r_0, r_1, igoto\\ l';jis) \\longrightarrow (\\Delta, r_0, r_1, codeAfterLabel(J, l')) \\\\ \\\\  (\\tt sjCmpNE1) &amp; \\begin{array}{c}                  c_0 \\neq c_1 \\ \\ \\ \\ jis' = codeAfterLabel(J, l')                 \\\\ \\hline                 J \\vdash  (\\Delta, c_0, c_1, if\\_icmpne\\ l';jis) \\longrightarrow (\\Delta, \\_, \\_, jis')                  \\end{array} \\\\ \\\\ (\\tt sjCmpNE2) &amp; \\begin{array}{c}                  c_0 = c_1                  \\\\ \\hline                 J \\vdash  (\\Delta, c_0, c_1, if\\_icmpne\\ l';jis) \\longrightarrow (\\Delta , \\_, \\_, jis)                  \\end{array} \\\\ \\\\  (\\tt sjCmpGE1) &amp; \\begin{array}{c}                  c_0 \\ge c_1 \\ \\ \\ \\ jis' = codeAfterLabel(J, l')                 \\\\ \\hline                 J \\vdash  (\\Delta, c_0, c_1, if\\_icmpge\\ l';jis) \\longrightarrow (\\Delta, \\_, \\_, jis')                  \\end{array} \\\\ \\\\ (\\tt sjCmpGE2) &amp; \\begin{array}{c}                  c_0 \\lt c_1                  \\\\ \\hline                 J \\vdash  (\\Delta, c_0, c_1, if\\_icmpge\\ l';jis) \\longrightarrow (\\Delta , \\_, \\_, jis)                  \\end{array} \\\\ \\\\  \\end{array} \\] <p>The last set of rules handle the jump and conditional jumps. The rule \\((\\tt sjGoto)\\) processes a goto instruction by replacing the instructions to be processed \\(jis\\) by \\(codeAfterLabel(J, l')\\). Recall that \\(J\\) is storing the entire sequence of JVM instructions, \\(codeAfterLabel(J, l')\\) extracts the suffix of \\(J\\) starting from the point where \\(ilabel\\ l'\\) is found. </p> \\[ \\begin{array}{rcl} codeAfterLabel(ireturn, l) &amp; = &amp; error \\\\  codeAfterLabel(ilabel\\ l;jis, l') &amp; = &amp;              \\left \\{ \\begin{array}{lc}                       jis &amp; l == l'  \\\\                       codeAfterLabel(jis, l') &amp; {\\tt otherwise}                      \\end{array}             \\right . \\\\  codeAfterLabel(ji; jis, l) &amp; = &amp; codeAfterLabel(jis, l) \\end{array} \\] <p>The rule \\((\\tt sjCmpNE1)\\) performs the jump when the values held by the stacks are not equal. The rule \\((\\tt sjCmpNE2)\\) moves onto the next instruction (skpping the jump) when the values held by the stacks are equal. The rule \\((\\tt sjCmpGE1)\\) performs the jump when the values in the stack \\(c_0 \\geq c_1\\). The rule \\((\\tt sjCmpGE2)\\) moves onto the next instruction (skpping the jump) when the \\(c_0 \\lt c_1\\).</p>"},{"location":"notes/code_generation/#conversion-from-pa-to-jvm-bytecodes","title":"Conversion from PA to JVM bytecodes","text":"<p>A simple conversion from PA to JVM bytecodes can be described using the following deduction system.</p> <p>Let \\(M\\) be a mapping from PA temporary variables to JVM local variables. Let \\(L\\) be a mapping from PA labels (which are used as the targets in some jump instructions) to JVM labels.</p> <p>We have three types of rules.</p> <ul> <li>\\(M, L \\vdash lis \\Rightarrow jis\\), convert a sequence of PA labeled isntructions to a sequence of JVM bytecode instructions.</li> <li>\\(M \\vdash s \\Rightarrow jis\\), convert a PA operand into a sequence of JVM bytecode instructions.</li> <li>\\(L \\vdash l \\Rightarrow jis\\), convert a PA label into a JVM bytecode instructions, usually it is either empty or singleton.</li> </ul>"},{"location":"notes/code_generation/#converting-pa-labeled-instructions","title":"Converting PA labeled instructions","text":"\\[ \\begin{array}{rl}      {\\tt (jMove)} &amp; \\begin{array}{c}                     L \\vdash l \\Rightarrow jis_0 \\ \\ \\ M \\vdash s \\Rightarrow jis_1 \\ \\ \\ M,L\\vdash lis \\Rightarrow jis_2 \\\\                     \\hline                     M, L \\vdash l:t \\leftarrow s; lis \\Rightarrow jis_0 + jis_1 + [istore\\ M(t)] + jis_2                 \\end{array} \\\\   \\end{array} \\] <p>The rule \\({\\tt (jMove)}\\) handles the case of a move instruction. In this case we make use of the auxiliary rule \\(L \\vdash l_1 \\Rightarrow jis_0\\) to generate the label, in case the label is used as the target in some jump instructions. The auxiliary rule \\(M \\vdash s \\Rightarrow jis_1\\) converts a PA operand into a loading instruction in JVM bytecodes. Details fo these auxiliary functions can be found in the next subsection.</p> \\[ \\begin{array}{rl}      {\\tt (jEq)} &amp; \\begin{array}{c}                     L \\vdash l_1 \\Rightarrow jis_0 \\ \\ \\ M \\vdash s_1 \\Rightarrow jis_1 \\ \\ \\ M \\vdash s_2 \\Rightarrow jis_2 \\ \\ \\ M,L \\vdash lis \\Rightarrow jis_3 \\\\                     \\hline                     M, L \\vdash l_1:t \\leftarrow s_1 == s_2; l_2:ifn\\ t\\ goto\\ l_3 ; lis \\Rightarrow jis_0 + jis_1 + jis_2 + [if\\_icmpne\\ L(l_3)] + jis_3                 \\end{array} \\\\  \\\\      {\\tt (jLThan)} &amp; \\begin{array}{c}                     L \\vdash l_1 \\Rightarrow jis_0 \\ \\ \\ M \\vdash s_1 \\Rightarrow jis_1 \\ \\ \\ M \\vdash s_2 \\Rightarrow jis_2 \\ \\ \\ M,L \\vdash lis \\Rightarrow jis_3 \\\\                     \\hline                     M, L \\vdash l_1:t \\leftarrow s_1 &lt; s_2; l_2:ifn\\ t\\ goto\\ l_3 ; lis \\Rightarrow jis_0 + jis_1 + jis_2 + [if\\_icmpge\\ L(l_3)] + jis_3                 \\end{array} \\\\   \\end{array} \\] <p>The rules \\((\\tt jEq)\\) and \\((\\tt jLThan)\\) translate the conditional jump instruction from PA to JVM. In these cases, we have to look at the first two instructions in the sequence. This is because in PA the conditional jump is performed in 2 instructions; while in JVM, it is done in a single step with two different instructions.</p> \\[ \\begin{array}{rl}      {\\tt (jAdd)} &amp; \\begin{array}{c}                     L \\vdash l \\Rightarrow jis_0 \\ \\ \\ M \\vdash s_1 \\Rightarrow jis_1 \\ \\ \\ M \\vdash s_2 \\Rightarrow jis_2 \\ \\ \\ M,L \\vdash lis \\Rightarrow jis_3 \\\\                     \\hline                     M, L \\vdash l:t \\leftarrow s_1 + s_2; lis \\Rightarrow jis_0 + jis_1 + jis_2 + [iadd, istore\\ M(t)] + jis_3                 \\end{array} \\\\   \\end{array} \\] \\[ \\begin{array}{rl}      {\\tt (jSub)} &amp; \\begin{array}{c}                     L \\vdash l \\Rightarrow jis_0 \\ \\ \\ M \\vdash s_1 \\Rightarrow jis_1 \\ \\ \\ M \\vdash s_2 \\Rightarrow jis_2 \\ \\ \\ M,L \\vdash lis \\Rightarrow jis_3 \\\\                     \\hline                     M, L \\vdash l:t \\leftarrow s_1 - s_2; lis \\Rightarrow jis_0 + jis_1 + jis_2 + [isub, istore\\ M(t)] + jis_3                 \\end{array} \\\\   \\end{array} \\] \\[ \\begin{array}{rl}      {\\tt (jMul)} &amp; \\begin{array}{c}                     L \\vdash l \\Rightarrow jis_0 \\ \\ \\ M \\vdash s_1 \\Rightarrow jis_1 \\ \\ \\ M \\vdash s_2 \\Rightarrow jis_2 \\ \\ \\ M,L \\vdash lis \\Rightarrow jis_3 \\\\                     \\hline                     M, L \\vdash l:t \\leftarrow s_1 * s_2; lis \\Rightarrow jis_0 + jis_1 + jis_2 + [imul, istore\\ M(t)] + jis_3                 \\end{array} \\\\   \\end{array} \\] <p>The rules \\((\\tt jAdd)\\),  \\((\\tt jSub)\\) and  \\((\\tt jMul)\\) handle the binary operation instruction in PA to JVM.</p> \\[ \\begin{array}{rl}      {\\tt (jGoto)} &amp; \\begin{array}{c}                     L \\vdash l_1 \\Rightarrow jis_0\\ \\ \\  M,L \\vdash lis \\Rightarrow jis_1 \\\\                     \\hline                     M, L \\vdash l_1:goto\\ l_2; lis \\Rightarrow jis_0 + [igoto\\ l_2] + jis_1                 \\end{array} \\\\   \\end{array} \\] \\[ \\begin{array}{rl}      {\\tt (jReturn)} &amp; \\begin{array}{c}                     L \\vdash l_1 \\Rightarrow jis_0\\ \\ \\ M \\vdash s \\Rightarrow jis_1\\ \\ \\ \\\\                     \\hline                     M, L \\vdash l_1:rret \\leftarrow s;  l_2: ret \\Rightarrow jis_0 + jis_1 + [ireturn]                  \\end{array} \\\\   \\end{array} \\] <p>The last two rules \\((\\tt jGoto)\\) and \\((\\tt jReturn)\\) are trivial.</p>"},{"location":"notes/code_generation/#converting-pa-operands","title":"Converting PA Operands","text":"\\[ \\begin{array}{rl} {\\tt (jConst)} &amp; M \\vdash c \\Rightarrow [sipush\\ c] \\\\ \\\\  {\\tt (jVar)} &amp; M \\vdash t \\Rightarrow [iload\\ M(t)] \\\\ \\\\  \\end{array} \\]"},{"location":"notes/code_generation/#converting-pa-labels","title":"Converting PA Labels","text":"\\[ \\begin{array}{rl} {\\tt (jLabel1)} &amp; \\begin{array}{c}                      l \\not \\in L                     \\\\ \\hline                     L \\vdash l \\Rightarrow []                     \\end{array} \\\\ \\\\  {\\tt (jLabel2)} &amp; \\begin{array}{c}                      l  \\in L                     \\\\ \\hline                     L \\vdash l \\Rightarrow [ilabel\\ l]                     \\end{array}   \\end{array} \\]"},{"location":"notes/code_generation/#optimizing-jvm-bytecode","title":"Optimizing JVM bytecode","text":"<p>Though it is limited, there is room to opimize the JVM bytecode. For example, </p> <p>From the following SIMP program </p> <pre><code>r = (1 + 2) * 3\n</code></pre> <p>we generate the following PA code via the Maximal Munch</p> <pre><code>1: t &lt;- 1 + 2\n2: r &lt;- t * 3  \n</code></pre> <p>In turn if we apply the above PA to JVM bytecode conversion</p> <p><pre><code>sipush 1\nsipush 2\niadd\nistore 2 // 2 is t\niload 2\nsipush 3\nimul\nistore 3 // 3 is r\n</code></pre> As observe, the <code>istore 2</code> followed by <code>iload 2</code> are rundandant, because <code>t</code> is not needed later (dead).</p> <pre><code>sipush 1\nsipush 2\niadd\nsipush 3\nimul\nistore 3 // 3 is r\n</code></pre> <p>This can either be done via </p> <ol> <li>Liveness analysis on PA level or </li> <li>Generate JVM byte code directly from SIMP.<ul> <li>This requires the expression of SIMP assignment to be left nested. </li> <li>The conversion is beyond the scope of this module.</li> </ul> </li> </ol>"},{"location":"notes/code_generation/#further-reading-for-jvm-bytecode-generation","title":"Further Reading for JVM bytecode generation","text":"<ul> <li>https://ssw.jku.at/Research/Papers/Wimmer04Master/Wimmer04Master.pdf</li> </ul>"},{"location":"notes/code_generation/#summary-for-jvm-bytecode-generation","title":"Summary for JVM bytecode generation","text":"<ul> <li>To generate JVM bytecode w/o optimization can be done via deduction system</li> <li>To optimize JVM bytecode, we could apply liveness analysis to eliminate redundant store-then-load sequence.</li> </ul>"},{"location":"notes/dynamic_semantics/","title":"50.054 - Dynamic Semantics","text":""},{"location":"notes/dynamic_semantics/#learning-outcomes","title":"Learning Outcomes","text":"<ol> <li>Explain the small step operational semantics of a programming language.</li> <li>Explain the big step operational semantics of a programming language.</li> <li>Formalize the run-time behavior of a programming language using small step operational semantics.</li> <li>Formalize the run-time behavior of a programming language using big step operational semantics.</li> </ol> <p>Recall that by formalizing the dynamic semantics of a program we are keen to find out</p> <ol> <li>How does the program get executed?</li> <li>What does the program compute / return?</li> </ol>"},{"location":"notes/dynamic_semantics/#operational-semantics","title":"Operational Semantics","text":"<p>Operational Semantics specifies how a program get executed.</p> <p>For example, in the earlier unit, when studying lambada expression, we made use of the \\(\\beta\\)-reduction, the substitution and alpha renaming rules to formalize the execution of a simple lambda expression. As the language grows to include let-binding, conditional expression, we extend the set of rules to include \\({\\tt (Let)}\\), \\({\\tt (IfI)}\\), \\({\\tt (IfT)}\\) and \\({\\tt (IfF)}\\). The set of rules in this example defines the operational semantics of the programming language lambda expression. We can apply these rules to \"evaluate\" a lambda expression by rewriting it by picking a matching rule (w.r.t to the LHS) and turn it into the form of the RHS. This style of semantics specification is called the small step operational semantics as we only specify the intermediate result when we apply a rule.  </p> <p>As we are going to design and implement a compiler for the SIMP language, it is essential to find out how a SIMP program gets executed.</p> <p>To formalize the execution of SIMP program, we can define a set of rewriting rules similar to those for lambda calculus. We need to consider different cases.</p>"},{"location":"notes/dynamic_semantics/#small-step-operational-semantics-of-simp","title":"Small-Step Operational Semantics of SIMP","text":"<p>Let's try to formalize the Operational Semantics of SIMP language,</p> <p>$$ \\begin{array}{rccl} (\\tt SIMP Environment) &amp; \\Delta &amp; \\subseteq &amp; (X \\times c) \\end{array} $$ We model the memory environment of a SIMP program as pair of variable and values. We write \\(dom(\\Delta)\\) to denote the domain of \\(\\Delta\\), i.e. \\(\\{ X \\mid (X,c) \\in \\Delta \\}\\). We assume for all \\(X \\in dom(\\Delta)\\), there exists only one entry of \\((X,c) \\in \\Delta\\).</p> <p>Given \\(S\\) is a set of pairs, we write \\(S(x)\\) to denote \\(a\\) if \\((x,a) \\in S\\), an error otherwise. We write \\(S \\oplus (x,a)\\) to denote \\(S - \\{(x, S(x))\\} \\cup \\{(x, a)\\}\\). </p> <p>We define the operational semantics of SIMP with two sets of rules.</p> <p>The first set of rules deal with expression.</p>"},{"location":"notes/dynamic_semantics/#small-step-operational-semantics-of-simp-expression","title":"Small Step Operational Semantics of SIMP Expression","text":"<p>The set of small step operational semantics for expressions is defined in a relation \\(\\Delta \\vdash E \\longrightarrow E'\\).</p> \\[ {\\tt (sVar)} ~~~ \\Delta \\vdash X \\longrightarrow \\Delta(X) \\] <p>The \\({\\tt (sVar)}\\) rule looks up the value of variable \\(X\\) from the memory environment. If the variable is not found, it gets stuck and an error is returned.</p> \\[ \\begin{array}{rc} {\\tt (sOp1)} &amp; \\begin{array}{c}         \\Delta \\vdash E_1 \\longrightarrow E_1'           \\\\ \\hline         \\Delta \\vdash E_1\\ OP\\ E_2 \\longrightarrow E_1'\\ OP\\ E_2         \\end{array} \\end{array} \\] \\[ \\begin{array}{rc} {\\tt (sOp2)} &amp; \\begin{array}{c}         \\Delta \\vdash E_2 \\longrightarrow E_2'           \\\\ \\hline         \\Delta \\vdash C_1 \\ OP\\ E_2 \\longrightarrow C_1\\ OP\\ E_2'         \\end{array} \\end{array} \\] \\[ \\begin{array}{rc} {\\tt (sOp3)} &amp; \\begin{array}{c}         C_3 = C_1 \\ OP\\ C_2         \\\\ \\hline         \\Delta \\vdash C_1 \\ OP\\ C_2 \\longrightarrow C_3         \\end{array} \\end{array} \\] <p>The above three rules handle the binary operation expression.</p> <ol> <li>\\({\\tt (sOp1)}\\) matches with the case where both operands are not constant values. It evalues the first operand by one step.  </li> <li>\\({\\tt (sOp2)}\\) matches with the case where the first operand becomes constant, it evaluates the second operand by one step.</li> <li>\\({\\tt (sOp3)}\\) matches with the case where both operands are constant. It returns the result by applying the binary operation to the two constant values.</li> </ol> \\[ \\begin{array}{rc} {\\tt (sParen1)} &amp; \\begin{array}{c}                  \\Delta \\vdash E \\longrightarrow E'                  \\\\ \\hline                   \\Delta \\vdash (E) \\longrightarrow (E')                  \\end{array} \\\\ \\\\ {\\tt (sParen2)} &amp; \\begin{array}{c}                  \\Delta \\vdash (c) \\longrightarrow c                  \\end{array} \\end{array} \\] <p>The rules \\({\\tt  (sParen1)}\\) and \\({\\tt (sParent2)}\\) evaluate an expression enclosed by parantheses. </p>"},{"location":"notes/dynamic_semantics/#small-step-operational-semantics-of-simp-statement","title":"Small Step Operational Semantics of SIMP statement","text":"<p>The small step operational semantics of statements are defined by the relation \\((\\Delta, S) \\longrightarrow (\\Delta', S')\\). The pair of a environment and a statement is called a program configuration.</p> \\[ \\begin{array}{cc} {\\tt (sAssign1)} &amp; \\begin{array}{c}      \\Delta\\vdash E \\longrightarrow E'      \\\\ \\hline      (\\Delta, X = E;) \\longrightarrow  (\\Delta, X = E';)      \\end{array} \\end{array} \\] \\[ \\begin{array}{cc} {\\tt (sAssign2)} &amp; \\begin{array}{c}       \\Delta' = \\Delta \\oplus (X, C)      \\\\ \\hline      (\\Delta, X = C;) \\longrightarrow (\\Delta', nop)      \\end{array} \\end{array} \\] <p>The rules \\({\\tt (sAssign1)}\\) and \\({\\tt (sAssign2)}\\) handle the assignment statements.</p> <ol> <li>\\({\\tt (sAssign1)}\\) matches with the case that the RHS of the assignment is not a constant, it evaluates the RHS expression by one step.</li> <li>\\({\\tt (sAssign2)}\\) matches with the case that the RHS is a constant, it updates the environment by setting \\(C\\) as the new value of variable \\(X\\). The statement of the resulting configuration a \\(nop\\).</li> </ol> \\[ \\begin{array}{cc} {\\tt (sIf1)} &amp; \\begin{array}{c}     \\Delta \\vdash E \\longrightarrow E'     \\\\ \\hline     (\\Delta, if\\ E\\ \\{\\overline{S_1}\\}\\ else\\ \\{\\overline{S_2}\\})     \\longrightarrow (\\Delta,  if\\ E'\\ \\{\\overline{S_1}\\}\\ else\\ \\{\\overline{S_2}\\})     \\end{array} \\end{array} \\] \\[ \\begin{array}{cc} {\\tt (sIf2)} &amp;     (\\Delta, if\\ true\\ \\{\\overline{S_1}\\}\\ else\\ \\{\\overline{S_2}\\})     \\longrightarrow (\\Delta, \\overline{S_1}) \\end{array} \\] \\[ \\begin{array}{cc} {\\tt (sIf3)} &amp;     (\\Delta, if\\ false\\ \\{\\overline{S_1}\\}\\ else\\ \\{\\overline{S_2}\\})     \\longrightarrow (\\Delta, \\overline{S_2}) \\end{array} \\] <p>The rules \\({\\tt (sIf1)}\\), \\({\\tt (sIf2)}\\) and \\({\\tt (sIf3)}\\) handle the if-else statement.</p> <ol> <li>\\({\\tt (sIf1)}\\) matches with the case where the condition expression \\(E\\) is not a constant value. It evaluates \\(E\\) to \\(E'\\) one step.</li> <li>\\({\\tt (sIf2)}\\) matches with the case where the condition expression is \\(true\\), it proceeds to evaluate the statements in the then clauses.</li> <li>\\({\\tt (sIf3)}\\) matches with the case where the condition expression is \\(false\\), it proceeds to evaluate the statements in the else clauses.</li> </ol> \\[ \\begin{array}{cc} {\\tt (sWhile)} &amp;     (\\Delta, while\\ E\\ \\{\\overline{S}\\} )     \\longrightarrow (\\Delta,  if\\ E\\ \\{\\overline{S}; while\\ E\\ \\{\\overline{S}\\}\\}\\ else\\ \\{ nop \\}) \\end{array} \\] <p>The rule \\({\\tt (sWhile)}\\) evaluates the while statement by reiwrting it into a if-else statement.</p> <ul> <li>In the then branch, we unroll the while loop body once followed by the while loop.</li> <li>In the else branch, we should exit the while loop thus, a \\(nop\\) statement is used.</li> </ul> \\[ {\\tt (sNopSeq)} ~~ (\\Delta, nop; \\overline{S}) \\longrightarrow (\\Delta, \\overline{S}) \\] \\[ \\begin{array}{cc} {\\tt (sSeq)} &amp; \\begin{array}{c}     S \\neq nop\\ \\ \\ (\\Delta, S) \\longrightarrow (\\Delta', S')     \\\\ \\hline    (\\Delta, S \\overline{S}) \\longrightarrow (\\Delta', S' \\overline{S})    \\end{array} \\end{array} \\] <p>The rules \\({\\tt (sNopSeq)}\\) and \\({\\tt (sSeq)}\\) handle a sequence of statements.</p> <ol> <li>\\({\\tt (sNopSeq)}\\) rule handles the special case where the leading statement is a \\(nop\\).</li> <li>\\({\\tt (Seq)}\\) rule handles the case where the leading statement is not a \\(nop\\). It evalues \\(S\\) by one step.</li> </ol> <p>For example,</p> <pre><code>{(input, 1)},\nx = input;\ns = 0;\nc = 0;\nwhile c &lt; x {\n    s = c + s;\n    c = c + 1;\n}\nreturn s;\n---&gt; # using (sSeq)\n   {(input,1)}, x = input \n   ---&gt; # (sAssign1) \n   {(input,1)}, x = 1\n   ---&gt; # (sAssign2) \n   {(input, 1), (x,1)}, nop\n---&gt;\n{(input,1), (x,1)},\nnop;\ns = 0;\nc = 0;\nwhile c &lt; x {\n    s = c + s;\n    c = c + 1;\n}\nreturn s;\n\n---&gt; # (sNopSeq)\n\n\n{(input,1), (x,1)},\ns = 0;\nc = 0;\nwhile c &lt; x {\n    s = c + s;\n    c = c + 1;\n}\nreturn s;\n\n---&gt; # (sSeq), (sAssign2), (sNoSeq)\n\n{(input,1), (x,1), (s,0)},\nc = 0;\nwhile c &lt; x {\n    s = c + s;\n    c = c + 1;\n}\nreturn s;\n\n---&gt; # (sSeq), (sAssign2), (sNoSeq)\n\n{(input,1), (x,1), (s,0), (c,0)},\nwhile c &lt; x {\n    s = c + s;\n    c = c + 1;\n}\nreturn s;\n---&gt; # (sSeq) \n    {(input,1), (x,1), (s,0), (c,0)},\n    while c &lt; x {\n        s = c + s;\n        c = c + 1;\n    }\n    ---&gt; # (sWhile)\n\n    {(input,1), (x,1), (s,0), (c,0)},\n    if (c &lt; x) {\n        s = c + s;\n        c = c + 1;\n        while c &lt; x {\n            s = c + s;\n            c = c + 1;\n        }\n    } else {\n        nop\n    }\n\n    ---&gt; # (sIf1)\n        {(input,1), (x,1), (s,0), (c,0)}, c &lt; x \n        ---&gt; # (sOp1) \n        {(input,1), (x,1), (s,0), (c,0)}, 0 &lt; x \n        ---&gt; # (sOp2) \n        {(input,1), (x,1), (s,0), (c,0)}, 0 &lt; 1 \n        ---&gt; # (sOp3) \n        {(input,1), (x,1), (s,0), (c,0)}, true \n    ---&gt; \n    {(input,1), (x,1), (s,0), (c,0)},\n    if true {\n        s = c + s;\n        c = c + 1;\n        while c &lt; x {\n            s = c + s;\n            c = c + 1;\n        }\n    } else {\n        nop\n    }\n    ---&gt; # (sIf2)\n    {(input,1), (x,1), (s,0), (c,0)},\n    s = c + s;\n    c = c + 1;\n    while c &lt; x {\n        s = c + s;\n        c = c + 1;\n    }\n    ---&gt; # (sSeq)\n        {(input,1), (x,1), (s,0), (c,0)},\n        s = c + s ---&gt; # (sAssign1)\n            {(input,1), (x,1), (s,0), (c,0)},\n            c + s ---&gt; # (sOp1) \n            0 + s ---&gt; # (sOp2) \n            0 + 0 ---&gt; # (sOp3) \n            0\n        {(input,1), (x,1), (s,0), (c,0)},\n        s = 0 ---&gt; # (sAssign2)\n        {(input,1), (x,1), (s,0), (c,0)},\n        nop \n    ---&gt; # (sNopSeq)\n    {(input,1), (x,1), (s,0), (c,0)},\n    c = c + 1;\n    while c &lt; x {\n        s = c + s;\n        c = c + 1;\n    }\n    ---&gt; # (sSeq)\n        {(input,1), (x,1), (s,0), (c,0)},\n        c = c + 1 ---&gt; # (sAssign1)\n            {(input,1), (x,1), (s,0), (c,0)},\n            c + 1 ---&gt; # (sOp1)\n            0 + 1 ---&gt; # (SOp3)\n            1\n        {(input,1), (x,1), (s,0), (c,1)},\n        c = 1 ---&gt; # (sAssign2)\n        {(input,1), (x,1), (s,0), (c,1)},\n        nop\n    ---&gt; # (sNopSeq) \n    {(input,1), (x,1), (s,0), (c,1)},\n    while c &lt; x {\n        s = c + s;\n        c = c + 1;\n    }\n    ---&gt; # (sWhile)\n    {(input,1), (x,1), (s,0), (c,1)},\n    if (c &lt; x) {\n        s = c + s;\n        c = c + 1;\n        while c &lt; x {\n            s = c + s;\n            c = c + 1;\n        }\n    } else {\n        nop\n    }\n    ---&gt; # (sIf1)\n        {(input,1), (x,1), (s,0), (c,1)}, c &lt; x \n        ---&gt; # (sOp1) \n        {(input,1), (x,1), (s,0), (c,1)}, 1 &lt; x \n        ---&gt; # (sOp2) \n        {(input,1), (x,1), (s,0), (c,1)}, 1 &lt; 1 \n        ---&gt; # (sOp3) \n        {(input,1), (x,1), (s,0), (c,1)}, false \n    ---&gt; \n    {(input,1), (x,1), (s,0), (c,1)},\n    if false {\n        s = c + s;\n        c = c + 1;\n        while c &lt; x {\n            s = c + s;\n            c = c + 1;\n        }\n    } else {\n        nop\n    }\n    ---&gt; # (sIf3)\n    {(input,1), (x,1), (s,0), (c,1)},\n    nop\n---&gt; # (sNopSeq)\n{(input,1), (x,1), (s,0), (c,1)}\nreturn s;\n</code></pre> <p>At last the derivation stop at the return statement. We can return the value <code>0</code> as result.</p>"},{"location":"notes/dynamic_semantics/#big-step-operational-semantics","title":"Big Step Operational Semantics","text":"<p>Small step operational semantics defines the run-time behavior of programs step by step (kinda like slow motion.) Some times we want to define the run-time behaviors by \"fast-forwarding\" to the result. This leads us to the big step operatinal semantics. Big step operatinal semantics in some literature is also called the structural operational semantics as it leverages on the syntactic structure of the program.</p>"},{"location":"notes/dynamic_semantics/#big-step-operational-semantics-for-simp-expressions","title":"Big Step Operational Semantics for SIMP expressions","text":"<p>We define the big step oeprational semantics for SIMP expressions via a relation \\(\\Delta \\vdash E \\Downarrow C\\), which reads under the memory environment \\(\\Delta\\) the expression \\(E\\) is evaluated constant \\(C\\).</p> <p>We consider the following three rules</p> \\[ {\\tt (bConst)} ~~~~ \\Delta \\vdash C \\Downarrow C \\] <p>In case that the expression is a constant, we return the constant itself.</p> \\[ {\\tt (bVar)} ~~~~ \\Delta \\vdash X \\Downarrow \\Delta(X) \\] <p>In case that the expression is a variable \\(X\\), we return the value associated with \\(X\\) in \\(\\Delta\\).</p> \\[ \\begin{array}{rc} {\\tt (bOp)} &amp; \\begin{array}{c}             \\Delta \\vdash E_1 \\Downarrow C_1 ~~~ \\Delta \\vdash E_2 \\Downarrow C_2 ~~~~             C_1\\ OP\\ C_2 = C_3             \\\\ \\hline             \\Delta \\vdash E_1\\ OP\\ E_2 \\Downarrow C_3             \\end{array} \\end{array} \\] <p>in case that the expression is a binary operation, we evaluate the two operands to values and apply the binary operation to the constant values.</p> \\[ \\begin{array}{rc} {\\tt (bParen)} &amp; \\begin{array}{c}                 \\Delta \\vdash E \\Downarrow C                 \\\\ \\hline                 \\Delta \\vdash (E) \\Downarrow C                \\end{array} \\end{array} \\] <p>the last rule \\({\\tt (bParen)}\\) evaluates an expression enclosed by parantheses.  </p>"},{"location":"notes/dynamic_semantics/#big-step-operational-semantics-for-simp-statements","title":"Big Step Operational Semantics for SIMP statements","text":"<p>We define the big step operational semantics for SIMP statement using a relation \\((\\Delta, S) \\Downarrow \\Delta'\\), which says the program configuration \\((\\Delta, S)\\) is evaluated to result memory environment \\(\\Delta'\\) assuming \\(S\\) is terminating under \\(\\Delta\\). Note that big step operational semantics for SIMP statement can only defines the behavior of terminating program configurations.</p> <p>We consider the following rules</p> \\[ \\begin{array}{rc} {\\tt (bAssign)} &amp; \\begin{array}{c}     \\Delta \\vdash E \\Downarrow C     \\\\ \\hline     (\\Delta, X = E) \\Downarrow \\Delta \\oplus (X, C)     \\end{array} \\end{array} \\] <p>In case that the statement is an assignment, we evaluate the RHS expression to a constant value \\(c\\) and update the memory environment.</p> \\[ \\begin{array}{rc} {\\tt (bIf1)} &amp; \\begin{array}{c}     \\Delta \\vdash E \\Downarrow true ~~~~~~     (\\Delta, \\overline{S_1}) \\Downarrow \\Delta_1     \\\\ \\hline     (\\Delta, if\\ E\\ \\{ \\overline{S_1} \\}\\ else\\ \\{ \\overline{S_2} \\} ) \\Downarrow \\Delta_1     \\end{array} \\end{array} \\] \\[ \\begin{array}{rc} {\\tt (bIf2)} &amp; \\begin{array}{c}     \\Delta \\vdash E \\Downarrow false ~~~~~~     (\\Delta, \\overline{S_2}) \\Downarrow \\Delta_2     \\\\ \\hline     (\\Delta, if\\ E\\ \\{ \\overline{S_1} \\}\\ else\\ \\{ \\overline{S_2} \\} ) \\Downarrow \\Delta_2     \\end{array} \\end{array} \\] <p>In case that the statement is an if-else statement, we evaluate \\(\\overline{S_1}\\) if the conditional expression is \\(true\\), otherwise evaluate \\(\\overline{S_2}\\).</p> \\[ \\begin{array}{rc} {\\tt (bWhile1)} &amp; \\begin{array}{c}     \\Delta \\vdash E \\Downarrow true ~~~~~~     (\\Delta, \\overline{S}; while\\ E\\ \\{ \\overline{S} \\}) \\Downarrow \\Delta'     \\\\ \\hline     (\\Delta, while\\ E\\ \\{ \\overline{S} \\}) \\Downarrow \\Delta'     \\end{array} \\end{array} \\] \\[ \\begin{array}{rc} {\\tt (bWhile2)} &amp; \\begin{array}{c}     \\Delta \\vdash E \\Downarrow false     \\\\ \\hline     (\\Delta, while\\ E\\ \\{ \\overline{S} \\}) \\Downarrow \\Delta     \\end{array} \\end{array} \\] <p>In case that the statment is a while loop. We evaluate the body followed by the while loop again when the loop condition expression is \\(true\\), otherwise, we exit the while loop and return the existing memory environment.</p> \\[ {\\tt (bNop)} ~~~~ (\\Delta, nop) \\Downarrow \\Delta \\] <p>In case that the statement is a <code>nop</code> statement, there is no change to the memory environment.</p> \\[ \\begin{array}{rc} {\\tt (bSeq)} &amp; \\begin{array}{c}             (\\Delta, S) \\Downarrow \\Delta' ~~~~ (\\Delta', \\overline{S}) \\Downarrow \\Delta''             \\\\ \\hline             (\\Delta, S \\overline{S}) \\Downarrow \\Delta''             \\end{array} \\end{array} \\] <p>In case of a sequence of statement, we evaluate the leading statement to an updated environment and use the updated environment to evaluate the following statements.</p> <p>For example, the following derivation (tree) is the evaluate of our running example using the big step operational semantics. The reason of having a tree derivation as we are evaluating the SIMP program to the final result directly by evaluating its sub components recursively / inductively.</p> <pre><code>                                   {(input,1),(x,1)} |- 0 \u21d3 0 (bConst)\n                                   ---------------------(bAssign)     [sub tree 1]\n                                   {(input,1), (x,1)}, \n                                   s = 0; \n{(input,1)} |- input \u21d3 1 (bVar)    \u21d3 {(input,1), (x,1), (s,0)}      \n---------------- (bAssign)         -------------------------------------------(bSeq)\n{(input,1)},                       {(input,1), (x,1)},  \nx = input;                         s = 0;\n\u21d3 {(input,1), (x,1)}               c = 0;\n                                   while c &lt; x {\n                                   s = c + s;\n                                   c = c + 1;\n                                   }\n                                   return s; \u21d3 {(input,1), (x,1), (s,0), (c,1)}\n---------------------------------------------------------------------------- (bSeq)\n{(input, 1)}, \nx = input;\ns = 0;\nc = 0;\nwhile c &lt; x {\n    s = c + s;\n    c = c + 1;\n}\nreturn s; \u21d3 {(input, 1), (x, 1), (s, 0), (c, 1)}\n</code></pre> <p>where sub derivation<code>[sub tree 1]</code> is as follows</p> <pre><code>{(input,1), (x,1), (s,0)}              [sub tree 2]  -------------------- (bReturn)\n|- 0 \u21d3 0 (bConst)                                    {(input,1), (x,1), (s,0), (c,1)}, \n                                                     return s; \u21d3 \n                                                     {(input,1), (x,1), (s,0), (c,1)}\n--------------------------(bAssign)   -------------------------------------- (bSeq)\n{(input,1), (x,1), (s,0)},            {(input,1), (x,1), (s,0), (c,0)},\nc = 0;                                while c &lt; x {s = c + s; c = c + 1;} \n\u21d3                                     return s; \u21d3 {(input,1), (x,1), (s,0), (c,1)}\n{(input,1),(x,1),(s,0),(c,0)}                    \n---------------------------------------------------------------------------- (bSeq)\n{(input,1), (x,1), (s,0)},\nc = 0;\nwhile c &lt; x {\n    s = c + s;\n    c = c + 1;\n}\nreturn s;\n\u21d3 {(input,1), (x,1), (s,0), (c,1)}\n</code></pre> <p>where <code>[sub tree 2]</code> is</p> <pre><code>{(input,1), (x,1), (s,0), (c,0)} \n|- c \u21d3 0 (bVar)\n\n{(input,1), (x,1), (s,0), (c,0)} \n|- x \u21d3 1 (bVar)\n\n0 &lt; 1  == true                         [sub tree 3]          [sub tree 4]\n-------------------------------- (bOp) ---------------------------------- (bSeq)\n{(input,1), (x,1), (s,0), (c,0)}       {(input,1), (x,1), (s,0), (c,0)},\n|- c &lt; x \u21d3 true                         s = c + s; c = c + 1; \n                                        while c &lt; x {s = c + s; c = c + 1;} \u21d3\n                                        {(input,1), (x,1), (s,0), (c,1)}\n-----------------------------------------------------------------------  (bWhile1)\n{(input,1), (x,1), (s,0), (c,0)},\nwhile c &lt; x {s = c + s; c = c + 1;} \u21d3 {(input,1), (x,1), (s,0), (c,1)}\n</code></pre> <p>where <code>[sub tree 3]</code> is</p> <pre><code>{(input, 1), (x, 1), (s, 0), (c, 0)}\n|- c \u21d3 0 (bVar)\n\n{(input, 1), (x, 1), (s, 0), (c, 0)}\n|- s \u21d3 0 (bVar)\n\nc + s == 0\n------------------------------------ (bOp)\n{(input, 1), (x, 1), (s, 0), (c, 0)} \n|- c + s \u21d3 0  \n------------------------------------- (bAssign)\n{(input, 1), (x, 1), (s, 0), (c, 0)},\ns = c + s; \u21d3\n{(input, 1), (x, 1), (s, 0), (c, 0)}\n</code></pre> <p>where <code>[sub tree 4]</code> is</p> <pre><code>{(input, 1), (x, 1), (s, 0), (c, 0)}\n|- c \u21d3 0 (bVar)\n\n{(input, 1), (x, 1), (s, 0), (c, 0)}\n|- 1 \u21d3 1 (bConst)\n\nc + 1 == 1\n------------------------------------ (bOp)\n{(input, 1), (x, 1), (s, 0), (c, 0)} \n|- c + 1 \u21d3 1  \n-------------------------------------- (bAssign)\n{(input, 1), (x, 1), (s, 0), (c, 0)},\nc = c + 1; \u21d3\n{(input, 1), (x, 1), (s, 0), (c, 1)}                [sub tree 5]\n--------------------------------------------------------------------- (bSeq)\n{(input, 1), (x, 1), (s, 0), (c, 0)},\nc = c + 1; \nwhile c &lt; x {s = c + s; c = c + 1;} \u21d3 {(input, 1), (x, 1), (s, 0), (c, 1)}\n</code></pre> <p>where <code>[sub tree 5]</code> is</p> <pre><code>{(input, 1), (x, 1), (s, 0), (c, 1)} (bVar)\n|- c \u21d3 1 \n\n{(input, 1), (x, 1), (s, 0), (c, 1)} (bVar)\n|- x \u21d3 1 \n\n1 &lt; 1 == false\n------------------------------------ (bOp)\n{(input, 1), (x, 1), (s, 0), (c, 1)} \n|- c &lt; x \u21d3 false\n---------------------------------------------------- (bWhile2)\n{(input, 1), (x, 1), (s, 0), (c, 1)}, \nwhile c &lt; x {s = c + s; c = c + 1;} \u21d3\n{(input, 1), (x, 1), (s, 0), (c, 1)}\n</code></pre>"},{"location":"notes/dynamic_semantics/#quick-summary-small-step-vs-big-step-operational-semantics","title":"Quick Summary: Small step vs Big Step operational semantics","text":"Small step operational semantics Big step operational semantics mode one step of change at a time many steps of changes at a time derivation it is linear it is a tree cons it is slow-paced and lengthy, requires more rules it is a fast-forward version, requirews fewer rules pros it is expressive, supports non-terminiating program it assumes program is terminating"},{"location":"notes/dynamic_semantics/#formal-results","title":"Formal Results","text":"<p>We use \\(\\longrightarrow^*\\) to denote multiple steps of derivation with \\(\\longrightarrow\\).</p>"},{"location":"notes/dynamic_semantics/#lemma-1-agreement-of-small-step-and-big-step-operational-semantics-of-simp","title":"Lemma 1 (Agreement of Small Step and Big Step Operational Semantics of SIMP)","text":"<p>Let \\(\\overline{S}\\) be a SIMP program, \\(\\Delta\\) be a memory environment. Then \\(\\Delta, \\overline{S} \\Downarrow \\Delta'\\) iff \\((\\Delta, \\overline{S}) \\longrightarrow^* (\\Delta', return\\ X)\\) for some \\(X\\).</p> <p>Proof of this lemma requires some knowledge which will be discussed in the upcoming classes.</p>"},{"location":"notes/dynamic_semantics/#operational-semantics-of-pseudo-assembly","title":"Operational Semantics of Pseudo Assembly","text":"<p>Next we consider the operational semantics of pseudo assembly.</p> <p>Let's define the environments required for the rules.</p> \\[ \\begin{array}{rccl} (\\tt PA\\ Program) &amp; P &amp; \\subseteq &amp; (l \\times li)  \\\\ (\\tt PA\\ Environment) &amp; L &amp; \\subseteq &amp; (t \\times c) \\cup (r \\times c) \\end{array} \\] <p>We use \\(P\\) to denote a PA program, which is a mapping from label to labeled instructions. We use \\(L\\) to denote a memory environment which is a mapping from temp variable or register to constant values.</p>"},{"location":"notes/dynamic_semantics/#small-step-operational-semantics-of-pseudo-assembly","title":"Small Step Operational Semantics of Pseudo Assembly","text":"<p>The dynamic semantics of the pseudo assembly program can be defined using a rule of shape \\(P \\vdash (L, li) \\longrightarrow (L', li')\\), which reads, given a PA program \\(P\\), the current program context \\((L,li)\\) is evaluated to \\((L', li')\\). Note that we use a memory environment and program label instruction pair to denote a program context.</p> \\[ {\\tt (pConst)}~~~P \\vdash (L, l: d \\leftarrow c) \\longrightarrow (L \\oplus (d,c), P(l+1)) \\] <p>In the \\({\\tt (pConst)}\\) rule, we evaluate an assignment instruction of which the RHS is a constant. We update the value of the LHS in the memory environment as \\(c\\) and move on to the next instruction.</p> \\[ {\\tt (pRegister)} ~~~P \\vdash (L, l: d \\leftarrow r) \\longrightarrow (L \\oplus (d,L(r)), P(l+1))   \\] \\[{\\tt (pTempVar)} ~~~P \\vdash (L, l: d \\leftarrow t) \\longrightarrow (L \\oplus (d,L(t)), P(l+1)) \\] <p>In the \\({\\tt (pRegister)}\\) and the \\({\\tt (pTempVar)}\\) rules, we evaluate an assignment instruction of which the RHS is a register (or a temp variable). We look up the value of the register (or the temp variable) from the memory environment and use it as the updated value of the LHS in the memory environment. We move on to the next label instruction.</p> \\[ \\begin{array}{rc} {\\tt (pOp)} &amp;  \\begin{array}{c}         c_1 = L(s_1) ~~~ c_2 = L(s_2) ~~~ c_3 = c_1\\ op\\ c_2         \\\\ \\hline         P \\vdash (L, l: d \\leftarrow s_1\\ op\\ s_2) \\longrightarrow (L \\oplus (d,c_3), P(l+1))           \\end{array} \\end{array} \\] <p>The \\({\\tt (pOp)}\\) rule handles the case where the RHS of the assignment is a binary operation. We first look up the values of the operands from the memory environment. We then apply the binary operation to the values. The result will be used to update the value of the LHS in the memory environment.</p> \\[ \\begin{array}{rc} {\\tt (pIfn0)} &amp; \\begin{array}{c}      L(s) = 0      \\\\ \\hline      P \\vdash (L, l: ifn\\ s\\ goto\\ l') \\longrightarrow (L, P(l'))      \\end{array} \\end{array} \\] \\[ \\begin{array}{rc} {\\tt (pIfnNot0)} &amp; \\begin{array}{c}      L(s) \\neq  0      \\\\ \\hline      P \\vdash (L, l: ifn\\ s\\ goto\\ l') \\longrightarrow (L, P(l+1))      \\end{array} \\end{array} \\] <p>The rules \\({\\tt (pIfn0)}\\) and \\({\\tt (pIfnNot0)}\\) deal with the conditional jump instruction. We first look up the conditional operand's value in the memory environment. If it is 0, we ignore the jump and move on to the next instruction, otherwiwse, we perform a jump but changing the program context to the target label instruction.</p> \\[ {\\tt (pGoto)} ~~ P \\vdash (L, l:goto\\ l') \\longrightarrow (L, P(l')) \\] <p>The rule \\({\\tt (pGoto)}\\) jumps to to the target label instruction.</p> <p>Note that there is no rule for \\(ret\\) as the program execution will stop there. Further more, the set of rules does not mention the scenario in which the look up of a register (or a temp variable) in the environment fails. In these cases, the program exit with an error.</p> <p>For example, let \\(P\\) be</p> <pre><code>1: x &lt;- input\n2: s &lt;- 0\n3: c &lt;- 0\n4: t &lt;- c &lt; x \n5: ifn t goto 9\n6: s &lt;- c + s\n7: c &lt;- c + 1\n8: goto 4\n9: rret &lt;- s\n10: ret\n</code></pre> <p>and \\(input = 1\\).</p> <p>We have the following derivation</p> <pre><code>P |- {(input,1)}, 1: x &lt;- input ---&gt; # (pTempVar)\nP |- {(input,1), (x,1)}, 2: s &lt;- 0 ---&gt; # (pConst)\nP |- {(input,1), (x,1), (s,0)}, 3: c &lt;- 0 ---&gt; # (pConst)\nP |- {(input,1), (x,1), (s,0), (c,0)}, 4: t &lt;- c &lt; x ---&gt; # (pOp)\nP |- {(input,1), (x,1), (s,0), (c,0), (t,1)}, 5: ifn t goto 9 ---&gt; # (pIfn0)\nP |- {(input,1), (x,1), (s,0), (c,0), (t,1)}, 6: s &lt;- c + s ---&gt; # (pOp)\nP |- {(input,1), (x,1), (s,0), (c,0), (t,1)}, 7: c &lt;- c + 1 ---&gt; # (pOp)\nP |- {(input,1), (x,1), (s,0), (c,1), (t,1)}, 8: goto 4 ---&gt; # (pGoto)\nP |- {(input,1), (x,1), (s,0), (c,1), (t,1)}, 4: t &lt;- c &lt; x ---&gt; # (pOp)\nP |- {(input,1), (x,1), (s,0), (c,1), (t,0)}, 5: ifn t goto 9 ---&gt; # (pIfnNot0)\nP |- {(input,1), (x,1), (s,0), (c,1), (t,0)}, 9: rret &lt;- s ---&gt; # (pTempVar)\nP |- {(input,1), (x,1), (s,0), (c,1), (t,0), (rret, 0)}, 10: ret\n</code></pre>"},{"location":"notes/dynamic_semantics/#formal-results_1","title":"Formal Results","text":""},{"location":"notes/dynamic_semantics/#definition-consistency-of-the-memory-environments","title":"Definition: Consistency of the memory environments","text":"<p>Let \\(\\Delta\\) be a SIMP memory environment and \\(L\\) be a pseudo assembly memory environment. We say \\(\\Delta\\) is consistent with \\(L\\) (written \\(\\Delta \\Vdash L\\)), iff</p> <ol> <li>\\(\\forall (x,v) \\in \\Delta\\), \\((x,conv(v)) \\in L\\), and</li> <li>\\(\\forall (y,u) \\in L\\), \\((y, v) \\in \\Delta\\) where \\(u=conv(v)\\).</li> </ol>"},{"location":"notes/dynamic_semantics/#lemma-correctness-of-the-maximal-munch-algorithm","title":"Lemma: Correctness of the Maximal Munch Algorithm","text":"<p>Let \\(S\\) and \\(S'\\) be SIMP program statements. Let \\(\\Delta\\) and \\(\\Delta'\\) be SIMP memory environments such that \\((\\Delta, S) \\longrightarrow (\\Delta', S')\\). Let \\(P\\) be a pseudo assembly program such that \\(G_s(S) = P\\). Let \\(L\\) and \\(L'\\) be pseudo assembly memory enviornments. Let \\(\\Delta \\Vdash L\\). Then we have \\(P \\vdash (L, l:i) \\longrightarrow (L', l':i')\\) and \\(\\Delta' \\Vdash L'\\)</p>"},{"location":"notes/dynamic_semantics/#proof","title":"Proof","text":"<p>Since the \\(S\\) could be a non-terminating program, the derivation via small step operational semantics could be infinite. We need a co-inductive proof, which is beyond the scope of this module. We will only discuss about this when we have time.</p>"},{"location":"notes/dynamic_semantics/#what-about-big-step-operational-semantics-of-pseudo-assembly","title":"What about big step operational semantics of Pseudo Assembly?","text":"<p>As Pseudo Assembly is a flatten language with goto statement, there is no nesting of statement or expression. There is no much value in defining the big step operatnal semantics, i.e. there is no way to \"fast-forward\" a sub statement / a sub expression per se.</p> <p>If you are interested in details of big step operational semantics, you may refer to this paper, which presented the operational and denotational semantics with a language with GOTO (more structures than our Pseudo Assembly.)</p> <pre><code>https://link.springer.com/article/10.1007/BF00264536\n</code></pre>"},{"location":"notes/dynamic_semantics/#denotational-semantics-optional-materials","title":"Denotational Semantics (Optional Materials)","text":"<p>Next we briefly discuss another form of dynamic semantics specification. Denotational Semantics aims to provide a meaning to a program. The \"meaning\" here is to find the result returned by the program. Now we may argue that is it the same as the big step operational semantics? There is some difference between the denotational semantics and big step operational semantics. We will defer the discussion and comparison towards the end of this unit.</p> <p>In denotational semantics, the \"meaning\" of a program is given by a set of semantic functions. These functions are mapping program objects from the syntactic domain to math objects in the semantic domain.</p>"},{"location":"notes/dynamic_semantics/#syntactic-domains","title":"Syntactic Domains","text":"<p>In many cases, the syntactic domains are defined by the grammar rules.</p> <p>For SIMP program, we have the following syntactic domains.</p> <ol> <li>\\(S\\) denotes the domain of all valid single statement</li> <li>\\(E\\) denotes the domain of all valid expressions</li> <li>\\(\\overline{S}\\) denotes the domain of all valid sequence statements</li> <li>\\(OP\\) denotes the domain of all valid operators.</li> <li>\\(C\\) denotes the domain of all constant values.</li> <li>\\(X\\) denotes the domain of all variables.</li> </ol>"},{"location":"notes/dynamic_semantics/#semantic-domains","title":"Semantic Domains","text":"<ol> <li>\\(Int\\) denotes the set of all integers values</li> <li>\\(Bool\\) denotes the set of \\(\\{true, false\\}\\)</li> <li>Given that \\(D_1\\) and \\(D_2\\) are domains, \\(D_1 \\times D_2\\) denotes the cartesian product of the two.</li> <li>Given that \\(D_1\\) and \\(D_2\\) are domains, \\(D_1 \\cup D_2\\) denotes the union and \\(D_1 \\cap D_2\\) denotes the intersection.</li> <li>Given that \\(D_1\\) and \\(D_2\\) are domains, \\(D_1 \\rightarrow D_2\\) denotes a functional mapping from domain \\(D_1\\) to domain \\(D_2\\).<ul> <li>Note that \\(D_1 \\rightarrow D_2 \\rightarrow D_3\\) is intepreted as \\(D_1 \\rightarrow (D_2 \\rightarrow D_3)\\).</li> </ul> </li> <li>Given that \\(D\\) is a domain, \\({\\cal P}(D)\\) denots the power set of \\(D\\).</li> </ol>"},{"location":"notes/dynamic_semantics/#denotational-semantics-for-simp-expressions","title":"Denotational Semantics for SIMP expressions","text":"<p>The denotational semantics for the SIMP expression is defined by the following semantic functions.</p> <p>Let \\(\\Sigma = {\\cal P} (X \\times (Int\\cup Bool))\\)</p> \\[ \\begin{array}{lll} {\\mathbb E} [\\![ \\cdot ]\\!]\\  :\\  E  &amp;\\rightarrow&amp;\\  \\Sigma \\rightarrow (Int \\cup Bool) \\\\ {\\mathbb E}[\\![ X ]\\!] &amp; = &amp; \\lambda\\sigma.\\sigma(X) \\\\ {\\mathbb E}[\\![ c ]\\!] &amp; = &amp; \\lambda\\sigma. c \\\\ {\\mathbb E}[\\![ E_1\\ OP\\ E_2 ]\\!] &amp; = &amp;\\lambda\\sigma.  {\\mathbb E}[\\![ E_1]\\!]\\sigma\\ [\\![ OP ]\\!]\\  {\\mathbb E}[\\![ E_2]\\!]\\sigma\\\\\\ \\end{array} \\] <p>The signature of the semantic function indicates that we map a SIMP expression into a function that takes a memory environment and returns a contant value.</p> <p>Implicitly, we assume that there exists a builtin semantic function that maps operator symbols to the (actual) semantic operators, i.e., \\([\\![ + ]\\!]\\) gives us the sum operation among two integers.  Sometimes we omit the parenthesis for function application when there is no ambiguity, e.g. \\({\\mathbb E}[\\![ E]\\!]\\sigma\\) is the short hand for \\(({\\mathbb E}[\\![ E]\\!])(\\sigma)\\)</p> <p>As we observe, \\({\\mathbb E}[\\![ \\cdot ]\\!]\\) takes an object from the expression syntactic domain and a memory store object from the domain of \\(\\Sigma\\), returns a value frmo the union of \\(Int\\) and \\(Bool\\) semantic domains.</p>"},{"location":"notes/dynamic_semantics/#denotational-semantics-for-simp-statements","title":"Denotational Semantics for SIMP statements","text":"<p>To define the denotational semantics, we need some extra preparation, in order to support non-terminating programs.</p> <p>Let \\(\\bot\\) be a special element, called undefined, that denotes failure or divergence. Let \\(f\\) and \\(g\\) be functions, we define</p> \\[ \\begin{array}{rcl} f \\circ_\\bot g &amp; = &amp; \\lambda \\sigma. \\left [ \\begin{array}{cc}                   \\bot &amp; g(\\sigma) = \\bot \\\\                   f(g(\\sigma)) &amp; otherwise                  \\end{array} \\right . \\end{array} \\] <p>which is a function composition that propogates \\(\\bot\\) if present. Now we define the semantic function for SIMP statements.</p> \\[ \\begin{array}{lll} {\\mathbb S}[\\![ \\cdot ]\\!] :   \\overline{S}  &amp; \\rightarrow\\ &amp; \\Sigma \\ \\rightarrow \\ \\Sigma \\cup \\{ \\bot \\} \\\\ {\\mathbb S} [\\![  nop ]\\!]&amp; = &amp; \\lambda\\sigma. \\sigma \\\\ {\\mathbb S} [\\![ return\\ X ]\\!]&amp; = &amp; \\lambda\\sigma. \\sigma \\\\ {\\mathbb S} [\\![  X = E ]\\!]&amp; = &amp; \\lambda\\sigma. \\sigma \\oplus (X, {\\mathbb E}[\\![ E ]\\!]\\sigma) \\\\ {\\mathbb S} [\\![ S \\overline{S} ]\\!]&amp; = &amp; {\\mathbb S} [\\![ \\overline{S} ]\\!] \\circ_\\bot {\\mathbb S} [\\![ S ]\\!]\\\\ {\\mathbb S} [\\![ if \\ E\\ \\{\\overline{S_1}\\} \\ else\\ \\{\\overline{S_2} \\} ]\\!]&amp; = &amp; \\lambda\\sigma. \\left [ \\begin{array}{cc}                     {\\mathbb S} [\\![ \\overline{S_1} ]\\!]\\sigma  &amp; {\\mathbb E}[\\![ E ]\\!]\\sigma = true \\\\                     {\\mathbb S} [\\![ \\overline{S_2} ]\\!]\\sigma &amp; {\\mathbb E}[\\![ E ]\\!]\\sigma = false \\\\                 \\end{array} \\right . \\\\ {\\mathbb S} [\\![ while \\ E\\ \\{\\overline{S}\\} ]\\!]&amp; = &amp; fix(F) \\\\  {\\tt where}\\ &amp;  &amp; F= \\lambda g.\\lambda\\sigma. \\left [ \\begin{array}{cc}                     (g \\circ_\\bot {\\mathbb S} [\\![ \\overline{S} ]\\!])(\\sigma)  &amp; {\\mathbb E}[\\![ E ]\\!]\\sigma = true \\\\                     \\sigma &amp; {\\mathbb E}[\\![ E ]\\!]\\sigma = false \\\\                 \\end{array} \\right . \\\\ \\end{array} \\] <p>The signature of the semantic function indicates that we map a SIMP statement into a function that takes a memory environment and returns another memory environment or divergence.</p> <p>In case of \\(nop\\) and return statement, the semantic function returns an identiy function. In case of an assignment, the semantic function takes an memory environment object and update the binding of \\(X\\) to the meaning of \\(E\\). In case of sequence statements, the semantic function returns a \\(\\bot\\)-function composition of the semantic function of the leading statement and the semantic function of the the trailing statements. In case of if-else statement, the semantic function returns the semantics of the then or the else branch statement depending on the meaning of the condition expression. In case of while statement, the semantic function returns a fixed point function. This is due to the fact that the underlying domain theory framework we are using does not support recursion. Hence a fixed point operator \\(fix\\) is used, which is kind of like recursion, (as we learnd in lambda caluclus), and it is more expresive as it gives a fixed term notiation for a sequence of infinitely many function objects applications. To help our understanding, we give a cheating version as if recursive function is supported in the underlying domain theory framework and we are allow to refer to a function application as a name function, we would have</p> \\[ \\begin{array}{lll} {\\mathbb S} [\\![ while \\ E\\ \\{\\overline{S}\\} ]\\!]&amp; = &amp; \\lambda\\sigma. \\left \\{ \\begin{array}{cc}                     ({\\mathbb S} [\\![ while \\ E\\ \\{\\overline{S}\\}]\\!]  \\circ_\\bot {\\mathbb S} [\\![ \\overline{S} ]\\!])(\\sigma)  &amp; {\\mathbb E}[\\![ E ]\\!]\\sigma = true \\\\                     \\sigma &amp; {\\mathbb E}[\\![ E ]\\!]\\sigma = false \\\\                 \\end{array} \\right . \\\\ \\end{array} \\] <p>which means the function \\(g\\) in the earlier version is a recursive reference to \\({\\mathbb S} [\\![ while \\ E\\ \\{\\overline{S}\\} ]\\!]\\)</p> <p>For example, let \\(\\sigma = \\{ (input, 1)\\}\\)</p> \\[ \\begin{array}{ll} &amp; {\\mathbb S} [\\![ x=input; s = 0; c=0; while\\ c &lt; x \\{s = c + s; c = c + 1;\\}return\\ s; ]\\!] \\sigma \\\\ = &amp; ({\\mathbb S} [\\![ s = 0; c=0; while\\ c &lt; x \\{s = c + s; c = c + 1;\\}return\\ s; ]\\!] \\circ_\\bot {\\mathbb S} [\\![ x=input ]\\!]) (\\sigma) \\\\ = &amp; {\\mathbb S} [\\![ s = 0; c=0; while\\ c &lt; x \\{s = c + s; c = c + 1;\\}return\\ s; ]\\!] \\sigma_1 \\\\ = &amp; ({\\mathbb S} [\\![ c=0; while\\ c &lt; x \\{s = c + s; c = c + 1;\\}return\\ s; ]\\!] \\circ_\\bot {\\mathbb S} [\\![ s=0 ]\\!]) (\\sigma_1) \\\\ = &amp; {\\mathbb S} [\\![ c=0; while\\ c &lt; x \\{s = c + s; c = c + 1;\\}return\\ s; ]\\!] \\sigma_2 \\\\ = &amp; ({\\mathbb S} [\\![ while\\ c &lt; x \\{s = c + s; c = c + 1;\\}return\\ s; ]\\!] \\circ_\\bot {\\mathbb S} [\\![ c=0 ]\\!]) (\\sigma_2) \\\\ = &amp; {\\mathbb S} [\\![ while\\ c &lt; x \\{s = c + s; c = c + 1;\\}return\\ s; ]\\!] \\sigma_3 \\\\ = &amp; ({\\mathbb S} [\\![ return\\ s; ]\\!] \\circ_\\bot {\\mathbb S} [\\![ while\\ c &lt; x \\{s = c + s; c = c + 1;\\} ]\\!]) (\\sigma_3) \\\\ = &amp; ({\\mathbb S} [\\![ return\\ s; ]\\!] \\circ_\\bot {\\mathbb S} [\\![ while\\ c &lt; x \\{s = c + s; c = c + 1;\\} ]\\!] \\circ_\\bot{\\mathbb S} [\\![ s = c + s; c = c + 1; ]\\!]) (\\sigma_3) \\\\ = &amp; ({\\mathbb S} [\\![ return\\ s; ]\\!] \\circ_\\bot {\\mathbb S} [\\![ while\\ c &lt; x \\{s = c + s; c = c + 1;\\} ]\\!])(\\sigma_4) \\\\ = &amp; {\\mathbb S} [\\![ return\\ s; ]\\!]\\sigma_4 \\\\ = &amp; \\sigma_4 \\end{array} \\] <p>where</p> \\[ \\begin{array}{l} \\sigma_1 = \\sigma \\oplus (x,1) = \\{ (input,1), (x,1) \\} \\\\ \\sigma_2 = \\sigma_1 \\oplus (s,0)  = \\{ (input,1), (x,1), (s,0) \\} \\\\ \\sigma_3 = \\sigma_2 \\oplus (c,0)  = \\{ (input,1), (x,1), (s,0), (c,0) \\}\\\\ \\sigma_4 = \\sigma_3 \\oplus (s,0) \\oplus (c,1)  = \\{ (input,1), (x,1), (s,0), (c,1) \\}\\\\ \\end{array} \\] <p>Let's consider another example of a non-terminating program, we can't use the cheating version here as it would gives the infinite sequence of function compositions. Let \\(\\sigma = \\{(input, true)\\}\\)</p> \\[ \\begin{array}{ll} &amp; {\\mathbb S} [\\![ while\\ input \\{nop;\\}return\\ input; ]\\!] \\sigma \\\\ = &amp; fix(F) \\sigma \\\\ = &amp; \\bot \\end{array} \\] <p>where</p> \\[ \\begin{array}{l} F = \\lambda g.\\lambda\\sigma. \\left [ \\begin{array}{cc}                     (g \\circ_\\bot {\\mathbb S} [\\![ nop ]\\!])(\\sigma)  &amp; {\\mathbb E}[\\![ input ]\\!]\\sigma = true \\\\                     \\sigma &amp; {\\mathbb E}[\\![ input ]\\!]\\sigma = false \\\\                 \\end{array} \\right . \\\\ \\end{array} \\] <p>Since \\({\\mathbb E}[\\![ input ]\\!]\\sigma\\) is always \\(true\\),</p> \\[ F = \\lambda g.\\lambda\\sigma.(g \\circ_\\bot {\\mathbb S} [\\![ nop ]\\!])(\\sigma)   \\] <p>With some math proof, we find that \\(fix(F)\\) is function of type \\(\\Sigma \\rightarrow \\bot\\). We won't be able to discuss the proof until we look into lattice theory in the upcoming classes.</p> <p>In simple term, using the \\(fix\\) operator to define the while statement denotational semantics allows us to \"collapse\" the infinite sequence of function composition/application into a fixed point, which is a non-terminating function.</p>"},{"location":"notes/dynamic_semantics/#denotational-semantics-vs-big-step-operational-semantics-vs-small-step-semantics","title":"Denotational Semantics vs Big Step operational Semantics vs Small Step Semantics","text":"support non-terminating programs don't support non-terminating programs focused on the step by step derivation Small Step Operational Semantics focused on the returned results Denotational Semantics Big Step Operational Semantics <p>Denotational Semantics is often used characterizing programming language model in a compositional way. It allows us to relates syntax objects to semantic objects. For example, if we want to argue that two languages are equivalent, we can map their syntax objects into the same semantic objects. We could also use denotational semantics to reason about concurrency.</p>"},{"location":"notes/dynamic_semantics/#extra-readings-for-denotational-semantics","title":"Extra readings for denotational semantics","text":"<pre><code>https://web.eecs.umich.edu/~weimerw/2008-615/lectures/weimer-615-07.pdf\nhttps://homepage.divms.uiowa.edu/~slonnegr/plf/Book/Chapter9.pdf\n</code></pre>"},{"location":"notes/fp_applicative_monad/","title":"50.054 - Applicative and Monad","text":""},{"location":"notes/fp_applicative_monad/#learning-outcomes","title":"Learning Outcomes","text":"<ol> <li>Describe and define derived type class</li> <li>Describe and define Applicative Functors</li> <li>Describe and define Monads</li> <li>Apply Monad to in design and develop highly modular and resusable software.</li> </ol>"},{"location":"notes/fp_applicative_monad/#derived-type-class","title":"Derived Type Class","text":"<p>Recall that in our previous lesson, we encountered the <code>Eq</code> and <code>Ord</code> type classes from the Haskell prelude.</p> <p><pre><code>-- prelude definitions, please don't execute it.\nclass Eq a where \n    (==) :: a -&gt; a -&gt; Bool \n\ndata Ordering = LT | EQ | GT\n\nclass Eq a =&gt; Ord a where \n    compare              :: a -&gt; a -&gt; Ordering\n    (&lt;), (&lt;=), (&gt;), (&gt;=) :: a -&gt; a -&gt; Bool\n    max, min             :: a -&gt; a -&gt; a\n\n    compare x y = if x == y then EQ\n                  else if x &lt;= y then LT\n                  else GT\n\n    x &lt;= y = case compare x y of { GT -&gt; False; _ -&gt; True }\n    x &gt;= y = y &lt;= x\n    x &gt; y = not (x &lt;= y)\n    x &lt; y = not (y &lt;= x)\n\n    max x y = if x &lt;= y then y else x\n    min x y = if x &lt;= y then x else y\n    {-# MINIMAL compare | (&lt;=) #-}    \n</code></pre> In the above,  the <code>Eq</code> type class is a super class of the <code>Ord</code> type class, because any instance of <code>Ord</code> type class should also be an instance of the <code>Ord</code> (by some type class instance declaration),</p> <p>We also say <code>Ord</code> is a derived type class of <code>Eq</code>.</p> <p>In addition, we find some default implementations of the member functions of <code>Ord</code> in the type class body. Minimally, we only need provide the implementation for either <code>compare</code> or <code>(&lt;=)</code> in an instance of the <code>Ord</code> type class.</p> <p>Let's consider some instances</p> <pre><code>module DerivedTypeClass where\n\ndata BTree a = Empty | \n    Node a (BTree a) (BTree a) -- ^ a node with a value and the left and right sub trees.\n\n\ninstance Eq a =&gt; Eq (BTree a) where \n    (==) Empty Empty = True \n    (==) (Node v1 l1 r1) (Node v2 l2 r2) = v1 == v2 &amp;&amp; l1 == l2 &amp;&amp; r1 == r2\n    (==) _ _ = False \n\ninstance Ord a =&gt; Ord (BTree a) where\n    compare Empty Empty = EQ \n    compare (Node v1 l1 r1) (Node v2 l2 r2) = \n        case compare v1 v2 of \n            EQ -&gt; case compare l1 l2 of \n                EQ -&gt; compare r1 r2\n                o  -&gt; o\n            o  -&gt; o\n    compare Empty (Node _ _ _) = LT \n    compare (Node _ _ _) Empty = GT\n\nNode 1 Empty (Node 2 Empty Empty) &lt;= Node 1 (Node 2 Empty Empty) Empty -- True\n</code></pre>"},{"location":"notes/fp_applicative_monad/#functor-recap","title":"Functor (Recap)","text":"<p>Recall from the last lesson, we make use of the <code>Functor</code> type class to define generic programming style of data processing.</p> <pre><code>-- prelude definitions, please don't execute it.\nclass Functor t where \n    fmap :: (a -&gt; b) -&gt; t a -&gt; t b\n\ninstance Functor List where \n    fmap f l = map f l\n</code></pre> <pre><code>-- our user defined instance Functor BTree\ninstance Functor BTree where \n    fmap f Empty = Empty\n    fmap f (Node v lft rgt) = \n        Node (f v) (fmap f lft) (fmap f rgt)\n</code></pre>"},{"location":"notes/fp_applicative_monad/#applicative-functor","title":"Applicative Functor","text":"<p>The <code>Applicative</code> Functor is a derived type class of <code>Functor</code>, which is defined as follows</p> <pre><code>-- prelude definitions, please don't execute it.\nclass Functor t =&gt; Applicative t where \n    pure :: a -&gt; t a\n    (&lt;*&gt;) :: t (a -&gt; b) -&gt; t a -&gt; t b\n    -- some optional member functions omitted\n</code></pre> <p>We will come to the member functions <code>pure</code> and <code>(&lt;*&gt;)</code> shortly. Since <code>Applicative</code> is a derived type class of  <code>Eq</code>, type instance of <code>Applicative a</code> must be also an instance of <code>Ord a</code>. </p> <p>For example, we consider the predefined instance of <code>Applicative List</code> instance from the prelude. </p> <pre><code>-- prelude definitions, please don't execute it.\ninstance Applicative List where \n    -- pure :: a -&gt; [a]\n    pure x = [x]\n    -- (&lt;*&gt;) :: List (a -&gt; b) -&gt; [a] -&gt; [b]\n    (&lt;*&gt;) fs as = [ f a | f &lt;- fs, a &lt;- as ]\n</code></pre> <p>In the <code>pure</code> function, we take the input argument <code>a</code> and enclose it in a list. In the <code>(&lt;*&gt;)</code> function, (it read as \"app\"), we encounter a list of functions of type <code>a -&gt; b</code> and  a list of values of type <code>a</code>. We apply list comprehension to extract every function elements in <code>fs</code> and  apply it to every value element in <code>as</code>.  If we were to consider the alternative implementation of <code>&lt;*&gt;</code> for list, we could use <code>concatMap</code> and <code>map</code>. </p> <p>Can you try to translate the above list comprehension into an equivalent Haskell expression using <code>concatMap</code> and <code>map</code>?</p> <p>Note that since we have defined <code>Functor List</code> in the earlier section, we don't need to repeat.</p> <p>Let's consider some example that uses <code>Applicative List</code>. Imagine we have a set of different operations and a set of data. The operation in the set should operate independently. We want to apply all the operations to all the data. We can use the <code>&lt;*&gt;</code> operation.</p> <pre><code>intOps = [\\x -&gt; x + 1, \\y -&gt; y * 2]\nints   = [1, 2, 3]\nintOps &lt;*&gt; ints -- ^ yields [2,3,4,2,4,6]\n</code></pre> <p>Let's consider another example. Recall that <code>Maybe a</code> algebraic datatype which captures a value of type <code>a</code> could be potentially empty.</p> <p>We find that <code>Functor Maybe</code> <code>Applicative Maybe</code> instances are in the prelude  as follows</p> <pre><code>-- prelude definitions, please don't execute it.\ninstance Functor Maybe where \n    fmap f Nothing  = Nothing \n    fmap f (Just x) = Just (f x)\n\n\ninstance Applicative Maybe where \n    pure x = Just x \n    (&lt;*&gt;) Nothing _ = Nothing\n    (&lt;*&gt;) _ Nothing = Nothing \n    (&lt;*&gt;) (Just f) (Just x) = Just (f x)\n</code></pre> <p>In the above Applicative instance, the <code>&lt;*&gt;</code> function takes a optional operation and optional value as inputs, tries to apply the operation to the value when both of them are present, otherwise, signal an error by returning <code>Nothing</code>. This allows us to focus on the high-level function-value-input-output relation and abstract away the details of handling potential absence of function or value.</p>"},{"location":"notes/fp_applicative_monad/#applicative-laws","title":"Applicative Laws","text":"<p>Like Functor laws, every Applicative instance must follow the Applicative laws to remain computationally predictable.</p> <ol> <li>Identity: <code>(&lt;*&gt;) (pure \\x-&gt;x)</code> \\(\\equiv\\) <code>\\x-&gt;x</code></li> <li>Homomorphism: <code>(pure f) &lt;*&gt; (pure x)</code> \\(\\equiv\\) <code>pure (f x)</code></li> <li>Interchange: <code>u &lt;*&gt; (pure y)</code> \\(\\equiv\\) <code>(pure (\\f-&gt;f y)) &lt;*&gt; u</code></li> <li> <p>Composition: <code>(((pure (.))) &lt;*&gt; u) &lt;*&gt; v) &lt;*&gt; w</code> \\(\\equiv\\) <code>u &lt;*&gt; (v &lt;*&gt; w)</code></p> </li> <li> <p>Identity law states that applying a lifted identity function of type <code>a-&gt;a</code> is same as an identity function of type <code>t a -&gt; t a</code> where <code>t</code> is an applicative functor.</p> </li> <li>Homomorphism says that applying a lifted function (which has type <code>a-&gt;a</code> before being lifted) to a lifted value, is equivalent to applying the unlifted function to the unlifted value directly and then lift the result.</li> <li> <p>To understand Interchange law let's consider the following equation $$ u y \\equiv (\\lambda f.(f y)) u $$</p> <ul> <li>Interchange law says that the above equation remains valid when \\(u\\) is already lifted, as long as we also lift \\(y\\). </li> </ul> </li> <li> <p>To understand the Composition law, we consider the following equation in lambda calculus</p> </li> </ol> \\[ (((\\lambda f.(\\lambda g.(f \\circ g)))\\ u)\\ v)\\ w \\equiv u\\ (v\\ w) \\] \\[ \\begin{array}{rl} (\\underline{((\\lambda f.(\\lambda g.(f \\circ g)))\\ u)}\\ v)\\ w &amp; \\longrightarrow_{\\beta} \\\\  (\\underline{(\\lambda g.(u \\circ g))\\ v})\\ w &amp; \\longrightarrow_{\\beta} \\\\  (u\\circ v)\\ w &amp; \\longrightarrow_{\\tt composition} \\\\  u\\ (v\\ w) \\end{array} \\] <p>The Composition Law says that the above equation remains valid when \\(u\\), \\(v\\) and \\(w\\) are lifted, as long as we also lift \\(\\lambda f.(\\lambda g.(f \\circ g))\\).</p>"},{"location":"notes/fp_applicative_monad/#cohort-exercise","title":"Cohort Exercise","text":"<p>show that any applicative functor satisfying the above laws also satisfies the Functor Laws</p>"},{"location":"notes/fp_applicative_monad/#monad","title":"Monad","text":"<p>Monad is one of the essential coding/design pattern for many functional programming languages. It enables us to develop high-level resusable code and decouple code dependencies and generate codes by (semi-) automatic code-synthesis. FYI, Monad is a derived type class of Applicative thus Functor.</p> <p>Let's consider a motivating example.  Recall that in the earlier lesson, we came across the following example.</p> <pre><code>data MathExp = \n    Plus  MathExp MathExp | \n    Minus MathExp MathExp |\n    Mult  MathExp MathExp |\n    Div   MathExp MathExp | \n    Const Int\n\n\neval :: MathExp -&gt; Maybe Int \neval (Plus e1 e2) = case eval e1 of \n    Nothing -&gt; Nothing \n    Just v1 -&gt; case eval e2 of \n        Nothing -&gt; Nothing \n        Just v2 -&gt; Just (v1 + v2)\neval (Minus e1 e2) = case eval e1 of \n    Nothing -&gt; Nothing \n    Just v1 -&gt; case eval e2 of \n        Nothing -&gt; Nothing \n        Just v2 -&gt; Just (v1 - v2)\neval (Mult e1 e2) = case eval e1 of \n    Nothing -&gt; Nothing \n    Just v1 -&gt; case eval e2 of \n        Nothing -&gt; Nothing \n        Just v2 -&gt; Just (v1 * v2)\neval (Div e1 e2) = case eval e1 of \n    Nothing -&gt; Nothing \n    Just v1 -&gt; case eval e2 of \n        Nothing -&gt; Nothing\n        Just 0  -&gt; Nothing \n        Just v2 -&gt; Just (v1 `div` v2)\neval (Const v) = Just v \n</code></pre> <p>In which we use <code>Maybe</code> to capture the potential div-by-zero error. One issue with the above is that it is very verbose, we lose some readability of the code thus, it takes us a while to migrate to <code>Either a b</code> if we want to have better error messages. Monad is a good application here.</p> <p>Let's consider the type class definition of <code>Monad m</code>.</p> <p><pre><code>-- prelude definitions, please don't execute it.\nclass Applicative m =&gt; Monad m where \n    (&gt;&gt;=) :: m a -&gt; (a -&gt; m b) -&gt; m b\n    -- optional\n    return :: a -&gt; a\n    return = pure\n    (&gt;&gt;) :: m a -&gt; m b -&gt; m b\n    (&gt;&gt;) m k = m &gt;&gt;= \\_ -&gt; k \n</code></pre> As suggested by the above definition, <code>Monad</code> is a derived type class of <code>Applicative</code>. The minimal requirement of a Monad instance is to implement the <code>(&gt;&gt;=)</code> (pronounced as \"bind\") function besides the obligation from <code>Applicative</code> and <code>Functor</code>.</p> <p>In the history of Haskell, <code>Monad</code> was not defined not as a derived type class of <code>Applicative</code> and <code>Functor</code>. It was reported and resolved since GHC version 7.10 onwards. Such approach was adopted by other language and systems.</p> <p>Let's take a look at the <code>Monad Maybe</code> instance provided by the Haskell prelude. <pre><code>instance Monad Maybe where\n    (&gt;&gt;=) Nothing _ = Nothing \n    (&gt;&gt;=) (Just a) f = f a \n</code></pre></p> <p>The <code>eval</code> function can be re-expressed using <code>Monad Maybe</code>.</p> <pre><code>eval :: MathExp -&gt; Maybe Int \neval (Plus e1 e2) = \n    (eval e1) &gt;&gt;= (\\v1 -&gt; (eval e2) &gt;&gt;= (\\v2 -&gt; return (v1 + v2)))\neval (Minus e1 e2) = \n    (eval e1) &gt;&gt;= (\\v1 -&gt; (eval e2) &gt;&gt;= (\\v2 -&gt; return (v1 - v2)))\neval (Mult e1 e2) = \n    (eval e1) &gt;&gt;= (\\v1 -&gt; (eval e2) &gt;&gt;= (\\v2 -&gt; return (v1 - v2)))\neval (Div e1 e2) = \n    (eval e1) &gt;&gt;= (\\v1 -&gt; (eval e2) &gt;&gt;= (\\v2 -&gt; \n        if v2 == 0\n        then Nothing\n        else return (v1 `div` v2)))\neval (Const i) = return i\n</code></pre> <p>It certainly reduces the level of verbosity, but the readability is worsened. Thankfully, we can make use of a <code>do</code> syntactic sugar provided by Haskell.</p> <p>In Haskell expression  <pre><code>do \n{ v1 &lt;- e1\n; v2 &lt;- e2\n...\n; vn &lt;- en \n; return e\n}\n</code></pre> is automatically desugared into </p> <pre><code>e1 &gt;&gt;= (\\v1 -&gt; e2 &gt;&gt;= (\\v2 -&gt; ... en &gt;&gt;= (\\vn -&gt; return e)))\n</code></pre> <p>Hence we can rewrite the above <code>eval</code> function as </p> <pre><code>eval :: MathExp -&gt; Maybe Int \neval (Plus e1 e2) = do \n    v1 &lt;- eval e1 \n    v2 &lt;- eval e2 \n    return (v1 + v2)\neval (Minus e1 e2) = do \n    v1 &lt;- eval e1\n    v2 &lt;- eval e2 \n    return (v1 - v2)\neval (Mult e1 e2) = do \n    v1 &lt;- eval e1\n    v2 &lt;- eval e2 \n    return (v1 * v2)\neval (Div e1 e2) = do \n    v1 &lt;- eval e1\n    v2 &lt;- eval e2 \n    if v2 == 0 \n    then Nothing \n    else return (v1 `div` v2)\neval (Const i) = return i\n</code></pre> <p>Now the readability is restored.</p> <p>Another advantage of coding with <code>Monad</code> is that its abstraction allows us to switch underlying data structure without major code change.</p> <p>Suppose we would like to use <code>Either String a</code> or some other equivalent as return type of <code>eval</code> function to support better error message. But before that, let's consider some subclasses of the <code>Monad</code> type classes provided in the Haskell standard library <code>mtl</code>.</p> <pre><code>-- mtl definition, please don't execute it.\nclass Monad m =&gt; MonadError e m | m -&gt; e where\n    throwError :: e -&gt; m a \n    catchError :: m a -&gt; (e -&gt; m a) -&gt; m a\n</code></pre> <p>In the above, we define a derived type class of <code>Monad</code>, called <code>MonadError e m</code> where <code>m</code> is the Monadic functor and <code>e</code> is the error type. The additional declaration <code>| m -&gt; e</code> denotes a functional depenedency between the instances of <code>m</code> and <code>e</code>. (You can think of it in terms of database FDs.) It says that whenever we fix a concrete instance of <code>m</code>, we can uniquely identify the corresponding instance of <code>e</code>.  The member function <code>throwErrow</code> takes an error message and injects into the Monad result. Function <code>catchError</code> runs an monad computation <code>m a</code>. In case of error, it applies the 2<sup>nd</sup> argument, a function of type <code>e -&gt; m a</code> to handle it. You can think of  <code>catchError</code> is the <code>try ... catch</code> equivalent in <code>MonadError</code>. </p> <p>Similarly, we extend <code>Monad</code> type class with <code>MonadError</code> type class. Next we examine type class instance <code>MonadError () Maybe</code>. We use <code>()</code> (pronounced as \"unit\") as the error type as we can't really propogate error message in <code>Maybe</code> other than <code>Nothing</code>.</p> <pre><code>-- mtl definition, please don't execute it.\ninstance MonadError () Maybe where \n    throwError _ = Nothing \n    catchError ma handle = case ma of \n        Nothing -&gt; handle () \n        Just v  -&gt; Just v\n</code></pre> <p>Next, we adjust the <code>eval</code> function to takes in a <code>MonadError</code> context instead of a <code>Monad</code> context. In addition, we make the error signal more explicit by calling the <code>throwError</code> function from the <code>MonadError</code> type class.</p> <pre><code>eval :: MathExp -&gt; Maybe Int \neval (Plus e1 e2) = do \n    v1 &lt;- eval e1 \n    v2 &lt;- eval e2 \n    return (v1 + v2)\neval (Minus e1 e2) = do \n    v1 &lt;- eval e1\n    v2 &lt;- eval e2 \n    return (v1 - v2)\neval (Mult e1 e2) = do \n    v1 &lt;- eval e1\n    v2 &lt;- eval e2 \n    return (v1 * v2)\neval (Div e1 e2) = do \n    v1 &lt;- eval e1\n    v2 &lt;- eval e2 \n    if v2 == 0 \n    then throwError ()\n    else return (v1 `div` v2)\neval (Const i) = return i\n</code></pre> <p>Now let's try to refactor the code to make use of <code>Either String Int</code> as the functor instead of <code>Maybe Int</code>.</p> <p><pre><code>-- mtl definition, please don't execute it.\ninstance MonadError String (Either String) where \n    throwError msg = Left msg \n    catchError ma handle = case ma of \n        Left msg -&gt; handle msg\n        Right v  -&gt; Right v\n</code></pre> In the above, we define <code>MonadError String (Either String)</code> instance, which satisfies the functional dependency set by the type class as <code>Either String</code> functionally determines <code>String</code>. </p> <p>Note that the concept of currying is applicable to the type constructors. <code>Either</code> has kind <code>* -&gt; * -&gt; *</code> therefore <code>Either String</code> has kind <code>* -&gt; *</code>.</p> <p>Now we can refactor the <code>eval</code> function by changing its type signature. And its body remains unchanged (almost).</p> <pre><code>eval :: MathExp -&gt; Either String Int \neval (Plus e1 e2) = do \n    v1 &lt;- eval e1 \n    v2 &lt;- eval e2 \n    return (v1 + v2)\neval (Minus e1 e2) = do \n    v1 &lt;- eval e1\n    v2 &lt;- eval e2 \n    return (v1 - v2)\neval (Mult e1 e2) = do \n    v1 &lt;- eval e1\n    v2 &lt;- eval e2 \n    return (v1 * v2)\neval (Div e1 e2) = do \n    v1 &lt;- eval e1\n    v2 &lt;- eval e2 \n    if v2 == 0 \n    then throwError \"division by zero error.\"\n    else return (v1 `div` v2)\neval (Const i) = return i\n</code></pre>"},{"location":"notes/fp_applicative_monad/#commonly-used-monads","title":"Commonly used Monads","text":"<p>We have seen the option Monad and the either Monad. Let's consider a few commonly used Monads.</p>"},{"location":"notes/fp_applicative_monad/#list-monad","title":"List Monad","text":"<p>We know that <code>List</code> is a Functor and an Applicative. It is not surprising that <code>List</code> is also a Monad.</p> <pre><code>-- prelude definitions, please don't execute it.\n\nflip :: (a -&gt; b -&gt; c) -&gt; b -&gt; a -&gt; c\nflip f b a = f a b\n\ninstance Monad List where \n    -- (&gt;&gt;=) :: [a] -&gt; (a -&gt; [b]) -&gt; [b]\n    (&gt;&gt;=) as f = flip concatMap as f\n</code></pre> <p>As we can observe from above, the bind function for List monad is a variant of <code>concatMap</code>.</p> <p>With the above instance, we can write list processing method in for comprehension which is similar to query languages (as an alternative to list comprehension).</p> <p>We define a scheme of a staff record using the following datatype. <pre><code>data Staff = Staff {sid::Int, dept::String, salary::Int}\n</code></pre></p> <p>Note that using <code>{}</code> on the right hand side of a data type definition denotes a record style datatype.  The components of the record data type are associated with field names. </p> <p>The names of the record fields can be used as getter fnuctions. The above is almost the same as an algebraic data type </p> <pre><code>data Staff = Staff Int String Int\n\nsid :: Staff -&gt; Int\nsid (Staff id _ _) = id \n\ndept :: Staff -&gt; String\ndept (Staff _ d _) = d \n\nsalary :: Staff -&gt; Int \nsalary (Staff _ _ s) = s\n</code></pre> <p>One difference is that we can use the record name in the record data type as an pseudo update function.</p> <pre><code>tom = Staff 1 \"hr\" 500000\nhappierTom = tom{salary=(salary tom)*2} -- ^ a new staff with same id and dept and doubled salary.\n</code></pre> <p>Now we are ready to write some query in Haskell list monad. Given a table of data,  <pre><code>staffData = [\n    Staff 1 \"HR\" 50000,\n    Staff 2 \"IT\" 40000,\n    Staff 3 \"SALES\" 100000,\n    Staff 4 \"IT\" 60000\n    ]\n</code></pre></p> <p>We can use the follow query to retrieve all the staff ids whose salary is more than 50000.</p> <pre><code>query :: [Staff] -&gt; [Int]\nquery table = do\n    staff &lt;- table           -- from  staff\n    if salary staff &gt; 50000  -- where salary &gt; 50000\n    then return (sid staff)  -- select sid\n    else []\n</code></pre>"},{"location":"notes/fp_applicative_monad/#reader-monad","title":"Reader Monad","text":"<p>Next we consider the <code>Reader</code> Monad.  <code>Reader</code> Monad denotes a shared input environment used by multiple computations. Once shared, this environment stays immutable.</p> <p>For example, suppose we would like to implement some test with a sequence of API calls. Most of these API calls are having the same host IP. We can set the host IP as part of the reader's environment.</p> <p>First let's consider the reader data type <pre><code>-- mtl definition with monomorphication, please don't execute it.\ndata Reader r a = Reader {run :: r -&gt; a}\n</code></pre></p> <p>It denotes a computation that takes the reference (shared) environment <code>r</code> and produces some result <code>a</code>.</p> <p>Then we provide the necessarily implementation to qualify <code>Reader r</code> as a <code>Monad</code> instance.</p> <pre><code>-- mtl definition with monomorphication, please don't execute it.\ndata Reader r a = Reader {run :: r -&gt; a}\ninstance Functor (Reader r) where \n    -- fmap :: (a -&gt; b) -&gt; Reader r a -&gt; Reader r b\n    fmap f ra = Reader ( \\r -&gt;\n        let a = run ra r\n        in f a ) \n\ninstance Applicative (Reader r) where \n    -- pure :: a -&gt; Reader r a \n    pure a = Reader (\\_ -&gt; a)\n    -- (&lt;*&gt;) :: Reader r (a -&gt; b) -&gt; Reader r a -&gt; Reader r b \n    rf &lt;*&gt; ra = Reader (\\r -&gt;\n        let f = run rf r\n            a = run ra r\n        in f a)\n\ninstance Monad (Reader r) where\n    -- (&gt;&gt;=) :: Reader r a -&gt; (a -&gt; Reader r b) -&gt; Reader r b\n    ra &gt;&gt;= g = Reader (\\r -&gt;\n        let a = run ra r\n            rb = g a\n        in run rb r)\n</code></pre> <ul> <li><code>fmap</code> function takes a function <code>f</code> and a reader <code>ra</code> and returns a reader whose computation function takes an input referenced object <code>r</code> and runs <code>ra</code> with r to obtain <code>a</code>, then applly <code>f</code> to <code>a</code>.</li> <li><code>pure</code> function takes a value of type <code>a</code> and wraps it into a reader object whose computation function is returning the input value ignoring the referenced object <code>r</code>. </li> <li>The app function (<code>&lt;*&gt;</code>) takes a reader <code>rf</code> that produces function(s), and a reader that produces value <code>a</code>. It returns a reader whose computation function takes a referenced object <code>r</code> and run <code>rf</code> with <code>ra</code> with it to produce the function <code>f</code> and the value <code>a</code>. Finally it applies <code>f</code> to <code>a</code>.</li> <li>The bind function (<code>&gt;&gt;=</code>) takes a reader <code>ra</code> and a function <code>g</code>, it returns a reader whose computation function takes a referenced object <code>r</code> and run <code>ra</code> with <code>r</code> to obtained <code>a</code>, next it applies <code>g</code> to <code>a</code> to generate the reader <code>rb</code>, finally, it runs <code>rb</code> with <code>r</code>.</li> </ul> <p>We consider the <code>MonadReader</code> type class definition as follows,</p> <pre><code>-- mtl definition, please don't execute it.\nclass Monad m =&gt; MonadReader r m | m -&gt; r where\n    -- | Retrieves the monad environment.\n    ask   :: m r\n    ask = reader id\n\n    -- | Executes a computation in a modified environment.\n    local :: (r -&gt; r) -- ^ The function to modify the environment.\n          -&gt; m a      -- ^ Reader to run in the modified environment.\n          -&gt; m a\n</code></pre> <p>We provide the implementation for <code>MonadReader r (Reader r)</code>.</p> <pre><code>-- mtl definition with monomorphication, please don't execute it.\ninstance MonadReader r (Reader r) where\n    -- ask :: Reader r r\n    ask = Reader (\\r -&gt; r)\n    -- local :: (r -&gt; r) -&gt; Reader r a -&gt; Reader r a\n    local f ra = Reader (\\r -&gt; \n        let t = f r\n        in run ra t)\n</code></pre> <p>The following example shows how Reader Monad can be used in making several API calls (computation) to the same API server (shared input <code>https://127.0.0.1/</code>).  For authentication we need to call the authentication server <code>https://127.0.0.10/</code> temporarily. </p> <pre><code>import Debug.Trace (trace)\ndata API = API {url::String} \n\ncall :: String -&gt; Reader API () \ncall path = do \n    api &lt;- ask\n    -- we use trace to simulate the api call\n    -- the actual one requires IO monad to be wrapped\n    io &lt;- trace (\"calling \" ++ url api ++ path) (return ())\n    return () \n\n\nauthServer = API \"https://127.0.0.10/\"\napiServer = API \"https://127.0.0.1/\"\n\ntest1 :: Reader API () \ntest1 = do \n    a &lt;- local (\\_ -&gt; authServer) (call \"auth\")\n    t &lt;- call \"time\"\n    j &lt;- call \"job\"\n    a `seq` t `seq` j `seq` return ()\n</code></pre> <p>Calling <code>run test1 apiServer</code> yields the following debugging messages.</p> <pre><code>calling https://127.0.0.10/auth\ncalling https://127.0.0.1/time\ncalling https://127.0.0.1/job\n</code></pre>"},{"location":"notes/fp_applicative_monad/#state-monad","title":"State Monad","text":"<p>Next we consider the <code>State</code> Monad.  A <code>State</code> Monad allows programmers capture and manipulate stateful computation without using assignment and mutable variable. One advantage of doing so is that program has full control of the state without having direct access to the computer memory. In a typeful language like haskell, the type system segregates the pure computation from the stateful computation. This greatly simplify software verification and debugging.</p> <p>We consider the following state data type </p> <pre><code>-- mtl definition with monomorphication, please don't execute it.\ndata State s a = State {run :: s -&gt; (a, s)}\n</code></pre> <p>It denotes a computation that takes the state environment <code>s</code> and produces some result <code>a</code> and the updated state envrionment.</p> <p>Then we provide the necessarily implementation to qualify <code>State s</code> as a <code>Monad</code> instance.</p> <pre><code>-- mtl definition with monomorphication, please don't execute it.\ninstance Functor (State s) where \n    -- fmap :: (a -&gt; b) -&gt; State s a -&gt; State s b\n    fmap f sa = State (\\s -&gt; \n        case run sa s of \n            (a, s1) -&gt; (f a, s1))\n\ninstance Applicative (State s) where \n    -- pure :: a -&gt; State s a\n    pure a = State (\\s -&gt; (a, s))\n    -- (&lt;*&gt;) :: State s (a -&gt; b) -&gt; State s a -&gt; State s b\n    sf &lt;*&gt; sa = State (\\s -&gt; \n        case run sf s of \n            (f, s1) -&gt; case run sa s1 of \n                (a, s2) -&gt; (f a, s2))\n\ninstance Monad (State s) where \n    -- (&gt;&gt;=) :: State s a -&gt; (a -&gt; State s b) -&gt; State s b\n    sa &gt;&gt;= f = State (\\s -&gt; \n        case run sa s of \n            (a, s1) -&gt;  run (f a) s1)\n</code></pre> <ul> <li>The <code>fmap</code> function takes a function <code>f</code> and a state object <code>sa</code> and returns a state object whose computation takes a state <code>s</code> and runs <code>sa</code> with <code>s</code> to copmute the result value <code>a</code> and the updated state <code>s1</code>. Finally, it returns the result of applying <code>f</code> to <code>a</code> and <code>s1</code>.</li> <li>The <code>pure</code> function takes a value of type <code>a</code> and return a <code>State</code> object by wrapping a lambda which takes a state <code>s</code> and returns back the same state <code>s</code> with the input value.</li> <li>The app function (<code>&lt;*&gt;</code>) takes a state object <code>sf</code> and a state object <code>sa</code>, its returned value is a state object that expects an input state <code>s</code> and run <code>sf</code> with <code>s</code> to extract the underlying function <code>f</code> with the updated state <code>s1</code>, then it runs <code>sa</code> with <code>s1</code> to extract the result <code>a</code> and the updated state <code>s2</code>. Finally, it returns the result of applying <code>f</code> to <code>a</code> and the updated state <code>s2</code>.</li> <li>In the bind function (<code>&gt;&gt;=</code>), we take a computation <code>sa</code> of type <code>State s a</code>, i.e. a stateful computation over state type <code>s</code> and return a result of type <code>a</code>. In addition, we take a function that expects input of type <code>a</code> and returns a stateful computation <code>State s b</code>. We return a <code>State</code> object contains a function that takes the input state <code>s</code>, and running with <code>sa</code> to retrieve the result <code>a</code> and the updated state <code>s1</code>, finally, the function run the state monad <code>f a</code> with <code>s1</code> to compute <code>b</code> and the output state.</li> </ul> <p>We consider the <code>MonadState</code> type class definition as follows,</p> <p><pre><code>-- mtl definition, please don't execute it.\nclass Monad m =&gt; MonadState s m | m -&gt; s where\n    -- | Return the state from the internals of the monad.\n    get :: m s\n    get = state (\\s -&gt; (s, s))\n    -- | Replace the state inside the monad.\n    put :: s -&gt; m ()\n    put s = state (\\_ -&gt; ((), s))\n</code></pre> where <code>get</code> is the query function that accesses the current state,  <code>put</code> is the setter function which \"updates\" the state by returning a (potentially new) state.</p> <p>We provide the implementation for <code>MonadState s (State s)</code>.</p> <pre><code>instance MonadState s (State s) where\n    -- get :: State s s\n    get = State (\\s -&gt; (s,s)) \n    -- put :: s -&gt; State s ()\n    put s = State (\\_ -&gt; ((), s))\n</code></pre> <p>Let's consider the following example </p> <pre><code>data Counter = Counter {c::Int} deriving Show\n\nincr :: State Counter () \nincr = do \n    Counter c &lt;- get\n    put (Counter (c+1))\n\n\napp :: State Counter Int\napp = do \n    incr\n    incr \n    Counter c &lt;- get \n    return c\n</code></pre> <p>In the above we define the state environment as an integer counter. Monadic function <code>incr</code> increase the counter in the state. The <code>deriving</code> keyword generate the type class instance <code>Show Counter</code> automatically. Running <code>run app (Counter 0)</code> yields <code>(2, Counter {c = 2})</code>.</p>"},{"location":"notes/fp_applicative_monad/#monad-laws","title":"Monad Laws","text":"<p>Similar to Functor and Applicative, all instances of Monad must satisfy the following three Monad Laws.</p> <ol> <li>Left Identity: <code>(return a) &gt;&gt;= f</code> \\(\\equiv\\) <code>f a</code></li> <li>Right Identity: <code>m &gt;&gt;= return</code> \\(\\equiv\\) <code>m</code></li> <li> <p>Associativity: <code>(m &gt;&gt;= f) &gt;&gt;= g</code> \\(\\equiv\\) <code>m &gt;&gt;= (\\x -&gt; ((f x) &gt;&gt;= g))</code></p> </li> <li> <p>Intutively speaking, a bind operation is to extract results of type <code>a</code> from its first argument with type <code>m a</code> and apply <code>f</code> to the extracted results.</p> </li> <li>Left identity law enforces that binding a lifted value to <code>f</code>, is the same as applying <code>f</code> to the unlifted value directly, because the lifting and the extraction of the bind cancel each other.</li> <li>Right identity law enforces that binding a lifted value to <code>return</code>,  is the same as the lifted value, because extracting results from <code>m</code> and <code>return</code> cancel each other.</li> <li>The Associativity law enforces that binding a lifted value <code>m</code> to <code>f</code> then to <code>g</code> is the same as binding <code>m</code> to a monadic bind composition <code>\\x -&gt; ((f x) &gt;&gt;= g)</code></li> </ol>"},{"location":"notes/fp_applicative_monad/#summary","title":"Summary","text":"<p>In this lesson we have discussed the following</p> <ol> <li>A derived type class is a type class that extends from another one.</li> <li>An Applicative Functor is a sub-class of Functor, with the methods <code>pure</code> and <code>&lt;*&gt;</code>.</li> <li>The four laws for Applicative Functor.</li> <li>A Monad Functor is a sub-class of Applicative Functor, with the method <code>&gt;&gt;=</code>.</li> <li>The three laws of Monad Functor.</li> <li>A few commonly used Monad such as, List Monad, Option Monad, Reader Monad and State Monad.</li> </ol>"},{"location":"notes/fp_applicative_monad/#extra-materials-you-dont-need-to-know-these-to-finish-the-project-nor-to-score-well-in-the-exams","title":"Extra Materials (You don't need to know these to finish the project nor to score well in the exams)","text":""},{"location":"notes/fp_applicative_monad/#writer-monad","title":"Writer Monad","text":"<p>The dual of the <code>Reader</code> Monad is the <code>Writer</code> Monad, which has the following definition.</p> <pre><code>-- mtl definition, please don't execute it.\ndata Writer w a = Writer { run :: (a,w) }\n\ninstance Functor (Writer w) where \n    -- fmap :: (a -&gt; b) -&gt; Write w a -&gt; Writer w b\n    fmap f (Writer (a,w)) = Writer (f a, w)\n\ninstance Monoid w =&gt; Applicative (Writer w) where \n    -- pure :: a -&gt; Writer w a\n    pure a = Writer (a, mempty)\n    -- (&lt;*&gt;) :: Writer w (a -&gt; b) -&gt; Writer w a -&gt; Writer w b\n    (Writer (f,w1)) &lt;*&gt; (Writer (a,w2)) = Writer (f a, w1 &lt;&gt; w2)\n\ninstance Monoid w =&gt; Monad (Writer w) where \n    -- (&gt;&gt;=) :: Writer w a -&gt; (a -&gt; Writer w b) -&gt; Writer w b\n    (Writer (a,w)) &gt;&gt;= f = case f a of \n        (Writer (b, w')) -&gt; (Writer (b, w &lt;&gt; w'))\n</code></pre> <p>A writer object stores the result and writer output state object which can be empty (via <code>mempty</code>)  and extended (via <code>&lt;&gt;</code>). For simplicity, we can think of <code>mempty</code> is the default empty state, for example, empty list <code>[]</code>, and <code>&lt;&gt;</code> is the append operation like <code>++</code>. For details about the <code>Monoid</code> type class refer to  <pre><code>https://hackage.haskell.org/package/base-4.16.3.0/docs/Prelude.html#g:9\n</code></pre></p> <p>The type class definition of <code>MonadWriter</code> is given in the <code>mtl</code> package as follows</p> <pre><code>-- mtl definition, please don't execute it.\nclass (Monoid w, Monad m) =&gt; MonadWriter w m | m -&gt; w where\n    -- | 'tell' w is an action that produces the output w\n    tell   :: w -&gt; m ()\n    -- | 'listen' m is an action that executes the action m and adds\n    -- its output to the value of the computation.\n    listen :: m a -&gt; m (a, w)\n    -- | 'pass' m is an action that executes the action m, which\n    -- returns a value and a function, and returns the value, applying\n    -- the function to the output.\n    pass   :: m (a, w -&gt; w) -&gt; m a\n\ninstance Monoid w =&gt; MonadWriter w (Writer w) where \n    -- tell :: w -&gt; Writer w ()\n    tell w = Writer ((),w)\n    -- listen :: Writer w a -&gt; Writer w (a, w)\n    listen (Writer (a,w)) = Writer ((a,w),w)\n    -- pass :: Writer w (a, w -&gt; w) -&gt; Writer w a\n    pass (Writer ((a, f), w)) = Writer (a, f w)\n</code></pre> <p>With these we are above to define a simple logger app as follows,</p> <pre><code>data LogEntry = LogEntry {msg::String} deriving Show \n\nlogger :: String -&gt; Writer [LogEntry] () \nlogger s = tell [LogEntry s]\n\napp :: Writer [LogEntry] Int\napp = do \n    logger \"start\"\n    x &lt;- return (1 + 1)\n    logger (\"the result is \" ++ show x)\n    logger (\"done\")\n    return x\n</code></pre> <p>Running <code>run app</code> yields </p> <pre><code>(2,[LogEntry {msg = \"start\"},LogEntry {msg = \"the result is 2\"},LogEntry {msg = \"done\"}])\n</code></pre>"},{"location":"notes/fp_applicative_monad/#monad-transformer","title":"Monad Transformer","text":"<p>In the earlier exection, we encounter our <code>State</code> datatype to record the computation in a state monad.  What about the following, can it be use as a state datatype for a state monad? </p> <pre><code>data MyState s a = MyState {run' :: s -&gt; Maybe (a,s)}\n</code></pre> <p>The difference between this class and the <code>State</code> class we've seen earlier is that the execution method <code>run'</code> yields result of type <code>Maybe (s,a)</code> instead of <code>(s,a)</code> which means that it can potentially fail.</p> <p>It is ascertained that <code>MyState</code> is also a Monad, and it is a kind of special State Monad.</p> <pre><code>instance Functor (MyState s) where \n    -- fmap :: (a -&gt; b) -&gt; MyState s a -&gt; MyState s b\n    fmap f sa = MyState (\\s -&gt; \n        case run' sa s of \n            Nothing -&gt; Nothing\n            Just (a, s1) -&gt; Just (f a, s1))\n\ninstance Applicative (MyState s) where \n    -- pure :: a -&gt; MyState s a\n    pure a = MyState (\\s -&gt; Just (a, s))\n    -- (&lt;*&gt;) :: MyState s (a -&gt; b) -&gt; MyState s a -&gt; MyState s b\n    sf &lt;*&gt; sa = MyState (\\s -&gt; \n        case run' sf s of \n            Nothing -&gt; Nothing \n            Just (f, s1) -&gt; case run' sa s1 of \n                Nothing -&gt; Nothing \n                Just (a, s2) -&gt; Just (f a, s2))\n\ninstance Monad (MyState s) where \n    -- (&gt;&gt;=) :: MyState s a -&gt; (a -&gt; MyState s b) -&gt; MyState s b\n    sa &gt;&gt;= f = MyState (\\s -&gt; \n        case run' sa s of \n            Nothing -&gt; Nothing \n            Just (a, s1) -&gt;  run' (f a) s1)\n\ninstance MonadState s (MyState s) where\n    -- get :: MyState s s\n    get = MyState (\\s -&gt; Just (s,s)) \n    -- put :: s -&gt; MyState s ()\n    put s = MyState (\\_ -&gt; Just ((), s))\n</code></pre> <p>Besides \"stuffing-in\" an <code>Maybe</code> type, one could use an <code>Either</code> type and etc. Is there a way to generalize this by parameterizing? Seeking the answer to this question leads us to Monad Transformer.</p> <p>We begin by parameterizing the <code>Option</code> functor in <code>MyState</code></p> <pre><code>-- mtl definition, please don't execute it.\ndata StateT s m a = StateT {run :: s -&gt; m (a, s)}\n\ninstance Monad m =&gt; Functor (StateT s m) where \n    -- fmap :: (a -&gt; b) -&gt; StateT s m a -&gt; StateT s m b\n    fmap f sma = StateT (\\s -&gt; do \n        (a,s1) &lt;- run sma s\n        return (f a, s1))\n\ninstance Monad m =&gt; Applicative (StateT s m) where  \n    -- pure :: a -&gt; StateT s m a\n    pure a = StateT (\\s -&gt; return (a, s))\n    -- (&lt;*&gt;) :: StateT s m (a -&gt; b) -&gt; StateT s m a -&gt; StateT s m b\n    smf &lt;*&gt; sma = StateT (\\s -&gt; do \n        (f, s1) &lt;- run smf s\n        (a, s2) &lt;- run sma s1  \n        return (f a, s2))\n\ninstance Monad m =&gt; Monad (StateT s m) where \n    -- (&gt;&gt;=) :: StateT s m a -&gt; (a -&gt; StateT s m b) -&gt; StateT s m b\n    sma &gt;&gt;= f = StateT (\\s -&gt; do \n        (a, s1) &lt;- run sma s \n        run (f a) s1)\n\ninstance Monad m =&gt; MonadState s (StateT s m) where\n    -- get :: StateT s m s\n    get = StateT (\\s -&gt; return (s,s)) \n    -- put :: s -&gt; StateT s m ()\n    put s = StateT (\\_ -&gt; return ((), s))\n</code></pre> <p>In the above it is largely similar to <code>MyState</code> datatype, except that we parameterize <code>Maybe</code> by a type parameter <code>m</code>. As we observe from the type class instances that follow, <code>m</code> must be an instance of <code>Monad</code>, (which means <code>m</code> could be <code>Maybe</code>, <code>Either String</code>, and etc.)</p> <p>Let <code>m</code> be <code>Maybe</code>, which is a Monad instance, we can replace <code>MonadState s (MyState s)</code> in terms of <code>MonadState s (StateT s m)</code> and <code>Monad Maybe</code>.</p> <pre><code>type MyState s a = StateT s Maybe a\n</code></pre> <p>If we want to have a version with <code>Either String</code>, we could define</p> <pre><code>type MyState2 s a = StateT s (Either String) a\n</code></pre> <p>What about the original vanilla <code>State</code> Monad? Can we redefine it interms of <code>StateT</code>? </p> <p>We could introduce the <code>Identity</code> Monad.</p> <pre><code>-- mtl definition, please don't execute it.\ndata Identity a = Identity a \n\ninstance Functor Identity where \n    -- fmap :: (a -&gt; b) -&gt; Identity a -&gt; Identity b \n    fmap f (Identity a) = Identity (f a) \n\ninstance Applicative Identity where \n    -- pure :: a -&gt; Identity a\n    pure a = Identity a\n    -- (&lt;*&gt;) :: Identity (a -&gt; b) -&gt; Identity a -&gt; Identity b\n    (Identity f) &lt;*&gt; (Identity a) = Identity (f a)\n\ninstance Monad Identity where \n    -- (&gt;&gt;=) :: Identity a -&gt; (a -&gt; Identity b) -&gt; Identity b\n    (Identity a) &gt;&gt;= f = f a\n\ntype State s a = StateT s Identity a\n</code></pre> <p>One advantage of having Monad Transformer is that now we can create new Monad by composition of existing Monad Transformers. We are able to segregate and interweave methods from different Monad serving different purposes.</p> <p>Similarly we could generalize the <code>Reader</code> Monad  to its transformer variant.</p> <pre><code>-- mtl definition, please don't execute it.\ndata ReaderT r m a = ReaderT { run' :: r -&gt; m a }\n\ninstance Monad m =&gt; Functor (ReaderT r m) where \n    -- fmap :: (a -&gt; b) -&gt; ReaderT r m a -&gt; ReaderT r m b\n    fmap f rma = ReaderT (\\r -&gt; do \n        a &lt;- run' rma r\n        return (f a))\n\ninstance Monad m =&gt; Applicative (ReaderT r m)  where  \n    -- pure :: a -&gt; ReaderT r m a\n    pure a = ReaderT (\\r -&gt; return a)\n    -- (&lt;*&gt;) :: ReaderT r m (a -&gt; b) -&gt; ReaderT r m a -&gt; ReaderT r m b\n    rmf &lt;*&gt; rma = ReaderT (\\r -&gt; do \n        f &lt;- run' rmf r\n        a &lt;- run' rma r  \n        return (f a))\n\ninstance Monad m =&gt; Monad (ReaderT r m) where \n    -- (&gt;&gt;=) :: ReaderT r m a -&gt; (a -&gt; ReaderT r m b) -&gt; ReaderT r m b\n    rma &gt;&gt;= f = ReaderT (\\r -&gt; do \n        a &lt;- run' rma r \n        run' (f a) r)\n\n\ninstance Monad m =&gt; MonadReader r (ReaderT r m) where\n    -- ask :: ReaderT r m r\n    ask = ReaderT (\\r -&gt; return r)\n    -- local :: (r -&gt; r) -&gt; ReaderT r m a -&gt; ReaderT r m a\n    local f rma = ReaderT (\\r -&gt; \n        let t = f r\n        in run' rma t)\n\ntype Reader r a = ReaderT r Identity a\n</code></pre> <p>Note that the order of how Monad Transfomers being stacked up makes a difference,</p> <p>For instance, can you explain what the difference between the following two monad object types is?</p> <pre><code>type ReaderState r s a = ReaderT r (StateT s Identity) a  \ntype StateReader r s a = StateT s (ReaderT r Identity) a\n</code></pre>"},{"location":"notes/fp_haskell/","title":"50.054 - Instroduction to Haskell","text":""},{"location":"notes/fp_haskell/#learning-outcomes","title":"Learning Outcomes","text":"<p>By the end of this class, you should be able to</p> <ul> <li>Develop simple implementation in Haskell using List, Conditional, and Recursion</li> <li>Model problems and design solutions using Algebraic Datatype and Pattern Matching</li> <li>Compile and execute simple Haskell programs</li> </ul>"},{"location":"notes/fp_haskell/#what-is-haskell","title":"What is Haskell?","text":"<p>Haskell is a hybrid programming language which combines Object Oriented Paradigm and Functional Programming Paradigm. Haskell has many backends, including JVM, node.js and native.</p> <p>Haskell is widely used in the industry and the research communities. There many industry projects and open source projects were implemented mainly in Haskell, e.g. Apache Spark, Kafka, Akka, Play! and etc. For more details in how Haskell is used in the real-world business, you may refer to the following for further readings.</p> <ul> <li>Haskell in Production: Standard Chartered</li> <li>Haskell in Production: Microsoft</li> <li>Who is using Haskell</li> </ul>"},{"location":"notes/fp_haskell/#haskell-hello-world","title":"Haskell Hello World","text":"<p>Let's say we have a Haskell file named <code>HelloWorld.hs</code></p> <pre><code>main = print \"hello world\"\n</code></pre> <p>We can execute it via either</p> <pre><code>runghc HelloWorld.hs\n</code></pre> <p>or to compile it then run</p> <pre><code>ghc HelloWorld.hs &amp;&amp; ./HelloWorld\n</code></pre> <p>In the cohort problems, we are going to rely on a Haskell project manager named <code>cabal</code> to build, execute and test our codes.</p>"},{"location":"notes/fp_haskell/#functional-programming-in-haskell-at-a-glance","title":"Functional Programming in Haskell at a glance","text":"<p>In this module, we focus and utilise mostly the functional programming feature of Haskell.</p> Lambda Calculus Haskell Variable \\(x\\) <code>x</code> Constant \\(c\\) <code>1</code>, <code>2</code>, <code>True</code>, <code>False</code> Lambda abstraction \\(\\lambda x.t\\) <code>\\x-&gt;t</code> Function application \\(t_1\\ t_2\\) <code>t1 t2</code> Conditional \\(if\\ t_1\\ then\\ t_2\\ else\\ t_3\\) <code>if t1 then t2 else t3</code> Let Binding \\(let\\ x = t_1\\ in\\ t_2\\) <code>let x = t1 in t2</code> Recursion \\(let\\ f = (\\mu g.\\lambda x.g\\ x)\\ in\\ f\\ 1\\) <code>let f x = f x in f 1</code> <p>Similar to other mainstream languages, defining recursion in Haskell is straight-forward, we just make reference to the recursive function name in its body.</p> <pre><code>fac :: Int -&gt; Int \nfac x = if x == 0 \n        then 1 \n        else x * fac (x-1)\n\nfac 10\n</code></pre> <p>where <code>fac :: Int -&gt; Int</code> denotes a type annotation to the function <code>fac</code>, which is optional in Haskell. The Haskell compiler will always reconstruct (infer) the missing type annotation.</p>"},{"location":"notes/fp_haskell/#haskell-strict-and-lazy-evaluation","title":"Haskell Strict and Lazy Evaluation","text":"<p>Let <code>f</code> be a non-terminating function</p> <pre><code>f x = f x\n</code></pre> <p>The following shows that the function application in Haskell is using lazy evaluation.</p> <pre><code>g x = 1\ng (f 1) -- terminates with 1 \n</code></pre> <p>To force the argument to be strictly evaluate before the function, </p> <pre><code>let x = f 1 \nin seq x (g x) -- does not terminate\n</code></pre> <p>where <code>seq</code> is a builtin GHC function that forces its first argument to be reduced before its second argument. By applying it we achieve certain level of strict evaluation.</p>"},{"location":"notes/fp_haskell/#list-data-type","title":"List Data type","text":"<p>We consider a commonly used builtin data type in Haskell, the list data type. In Haskell, the following define some list values.</p> <ol> <li><code>[]</code> - an empty list.</li> <li><code>[1,2]</code> - a list contains two numerical values.</li> <li><code>[\"a\"]</code> - an string list contains one value.</li> <li><code>1:[2,3]</code> - prepends a value <code>1</code> to a list containing <code>2</code> and <code>3</code>.</li> <li><code>[\"hello\"] ++ [\"world\"]</code> - concatenating two string lists.</li> </ol> <p>To iterate through the items in a list, we can use pattern matching in Haskell. </p> <pre><code>sum :: [Int] -&gt; Int \nsum l = case l of \n    { [] -&gt; 0\n    ; hd:tl -&gt; hd + sum tl\n    }\n</code></pre> <p>in which <code>case l of { [] -&gt; 0; hd:tl -&gt; hd + sum tl}</code> denotes a pattern-matching expression in Haskell. It is similar to the switch statement found in other main stream languages, except that it has more perks.</p> <p>In this expression, we pattern match the input list <code>l</code> against two list patterns, namely:</p> <ul> <li><code>[]</code> the empty list, and</li> <li><code>hd:tl</code> the non-empty list</li> </ul> <p>Note that here <code>[]</code> and <code>hd:tl</code> are not list values, because they are appearing after a <code>case ... of</code> keyword and on the left of an arrow <code>-&gt;</code>.</p> <p>When there is no confusion, we could dopr the <code>{ }</code> and the <code>;</code> in the case patterns, e.g. </p> <pre><code>sum :: [Int] -&gt; Int \nsum l = case l of \n    [] -&gt; 0\n    hd:tl -&gt; hd + sum tl\n</code></pre> <p>Pattern cases are visited from top to bottom (or left to right). In this example, we first check whether the input list <code>l</code> is an empty list. If it is empty, the sum of an empty list must be <code>0</code>. </p> <p>If the input list <code>l</code> is not an empty list, it must have at least one element. The pattern <code>hd:tl</code> extracts the first element of the list and binds it to a local variable <code>hd</code> and the remainder (which is the sub list formed by taking away the first element from <code>l</code>) is bound to <code>hd</code>. We often call <code>hd</code> as the head of the list and <code>tl</code> as the tail. We would like to remind that <code>hd</code> is storing a single integer in this case, and <code>tl</code> is capturing a list of integers.</p> <p>If the case pattern is the outer most expression in a function body, we could rewrite it as follows,</p> <pre><code>sum :: [Int] -&gt; Int \nsum [] = 0\nsum (hd:tl) = hd + sum tl \n</code></pre> <p>One advantage of implementing the <code>sum</code> function in FP style is that it is much closer to its math specification.</p> \\[ \\begin{array}{rl} sum(l) = &amp; \\left [     \\begin{array}{ll}     0 &amp; {l\\ is\\ empty} \\\\     head(l)+sum(tail(l)) &amp; {otherwise}     \\end{array} \\right . \\end{array} \\] <p>Let's consider another example.</p> <pre><code>reverse :: [Int] -&gt; [Int] \nreverse l = case l of \n    [] -&gt; [] \n    hd:tl -&gt; reverse tl ++ [hd]\n</code></pre> <p>The function <code>reverse</code> takes a list of integers and generates a new list which is in the reverse order of the orginal one. We apply a similar strategy to break down the problem into two sub-problems via the <code>match</code> expression.</p> <ul> <li>When the input list <code>l</code> is an empty list, we return an empty list. The reverse of an empty list is an empty list</li> <li>When the input <code>l</code> is not empty, we make use of the pattern <code>hd:tl</code> to extract the head and the tail of the list</li> </ul> <p>We apply <code>reverse</code> recursively to the tail and then concatenate it with a list containing the head.</p> <p>You may notice that the same <code>reverse</code> function can be applied to lists of any element type, and not just integers, as long as all elements in a list share the same type. Therefore, we can rewrite the <code>reverse</code> function into a generic version as follows:</p> <pre><code>reverse :: [a] -&gt; [a] \nreverse l = case l of \n    [] -&gt; [] \n    hd:tl -&gt; reverse tl ++ [hd]\n</code></pre> <p>Note that the optional type annotation contains a type parameter (type variable) <code>a</code>, with which we specify that the element type of the list is <code>a</code> (any possible type). The type parameter is resolved when we apply <code>reverse</code> to a actual argument. For instance in <code>reverse [1,2,3]</code> the Haskell compiler will resolve <code>a=Int</code> assuming <code>1, 2, 3</code> are integers and in <code>reverse [\"a\",\"b\"]</code> it will resolve <code>a=String</code>.</p>"},{"location":"notes/fp_haskell/#a-note-on-recursion","title":"A Note on Recursion","text":"<p>Note that recursive calls to <code>reverse</code> will incur additional memory space in the machine in form of additional function call frames on the call stack.</p> <p>A call stack frame has to created to \"save\" the state of function execution such as local variables. As nested recursive calls are being built up, the machine might run out of memory. This is also known as Stack Overflow Error.</p> <p>While simple recursions that make a few tens of or hundreds of nested calls won't harm a lot, we need to rethink when we note that a recursion is going to be executed for a large number of iterations. One way to address this issue is to rewrite non-tail recursion into tail-recursion.</p> <p>A tail-recursion is a recursive function in which the recursive call occurs at the last instruction. </p> <p>For instance, the <code>reverse</code> function presented earlier is not. The following variant is a tail recursion</p> <pre><code>reverse l = go l [] \n    where go [] acc = acc\n          go (hd:tl) acc = go tl (hd:acc)\n</code></pre> <p>In the above definition, we rely on an inner function <code>go</code> which is recursively defined. In <code>go</code>, the recursion take places at the last instruction in the <code>(hd:tl)</code> case. The trick is to pass around an accumulated output <code>acc</code> in each recursive call.</p> <p>As compiler technology evolves, many modern FP language compilers are able to detect a subset of non-tail recursions and automatically transform them into the tail recursive version. </p> <p>However Haskell does not automatically re-write a non-tail recursion into a tail recursion, and leaves it as a programmer's task.</p>"},{"location":"notes/fp_haskell/#map-fold-and-filter","title":"Map, Fold and Filter","text":"<p>Consider the following function</p> <pre><code>addToEach :: Int -&gt; [Int] -&gt; [Int]\naddToEach x [] = []\naddtoEach x (y:ys) = \n    let yx = y + x \n    in yx : (addToEach x ys)\n</code></pre> <p>It takes two inputs, an integer <code>x</code> and an integer list <code>l</code>, and adds <code>x</code> to every element in <code>l</code> and put the results in the output list.</p> <p>For instance <code>addToEach 1  [1,2,3]</code> yields <code>[2,3,4]</code>.</p> <p>The above can rewritten by using a generic library function shipped with Haskell.</p> <pre><code>addToEach x l = map (\\y -&gt; y + x) l\n</code></pre> <p>The method <code>map</code> is a method of the list class that takes a function as input argument and applies it to all elements in the list object.</p> <p>We can observe that the input list and the output list of the <code>map</code> method must be of the same type and have the same length.</p> <p>Recall in the <code>sum</code> function introduced in the earlier section. It takes a list of integers and \"collapses\" them into one number by summation. We can rewrite it using a fold function.</p> <pre><code>sum :: [Int] -&gt; Int \nsum l = foldl (\\acc x -&gt; acc + x) 0 l \n</code></pre> <p>The <code>foldl</code> method takes a binary function and a base accumulator as inputs, and aggregates the elements from the list using the binary function.  In particular, the binary aggreation function assumes the first argument is the accumulator.</p> <p>Besides <code>foldl</code>, there exists a <code>foldr</code> method, in which the binary aggregation function expects the second argument is the accumulator.</p> <pre><code>sum l = foldr (\\x acc -&gt; x + acc) 0 l \n</code></pre> <p>So what is the difference between <code>foldl</code> and <code>foldr</code>?  What happen if you run the following? Can you explain the difference?</p> <pre><code>l = [\"a\",\"better\",\"world\", \"by\", \"design\"]\nfoldl (\\acc x -&gt; acc ++ \" \" ++ x) \"\" l \nfoldr (\\x acc -&gt; x ++ \" \" ++ acc) \"\" l \n</code></pre> <p>Note that in Haskell, a string is represented as a list of characters.  Since <code>++</code> is the list concatenation operator, in the above it concatenates two string values.</p> <p>Intuitively, <code>foldl (\\acc x -&gt; acc ++ \" \" ++ x) \"\" l</code> aggregates the list of words using the aggregation function by nesting the recursive calls to the left.</p> <pre><code>((((\"\" ++ \" \" ++ \"a\") ++ \" \" ++ \"better\") ++ \" \" ++ \"world\") ++ \" \" ++ \"by\") ++ \" \" ++ \"design\"\n</code></pre> <p>where <code>foldr (\\x acc -&gt; x ++ \" \" ++ acc) \"\" l</code> aggregates the list of words by nesting the recursive calls to the right.</p> <pre><code>\"a\" ++ \" \" ++ ( \"better\" ++ \" \" ++ (\"world\" ++ \" \" ++ (\"by\" ++ \" \" ++ (\"design\" ++ \" \" ++\"\"))))\n</code></pre> <p>The method <code>filter</code> takes a boolean test function and applies it to the elements in the list, keeping those whose test result is true and dropping those whose result is false.</p> <pre><code>l = [1,2,3,4]\n\neven :: Int -&gt; Bool\neven x = x `mod` 2 == 0 \n\nfilter even l \n</code></pre> <p>returns <code>[2,4]</code>.</p> <p>Note: in Haskell, <code>mod</code> is a prelude function (predefined function). <code>mod x y</code> that computes the remainder of the division of <code>x / y</code>. When we enclose a binary function with `` in Haskell, we can use it in an infix notation.</p> <pre><code>l = ['a','1','0','d']\nfilter Data.Char.isDigit l\n</code></pre> <p>returns <code>['1','0']</code>.</p> <p>Note that <code>isDigit</code> is a function defined in the module <code>Data.Char</code>.</p> <p>With <code>map</code>, <code>foldLeft</code> and <code>filter</code>, we can express the implementation of algorithms in a concise and elegant way. For instance, the following function implements the quicksort algorithm:</p> <pre><code>qsort :: [Int] -&gt; [Int] \nqsort [] = []\nqsort [x] = [x] \nqsort (p:rest) = \n    let ltp = filter (&lt; p) rest\n        gep = filter (&gt;= p) rest \n    in qsort ltp ++ [p] ++ qsort gep\n</code></pre> <p>which resembles the math specification</p> \\[ \\begin{array}{cc} qsort(l) = &amp; \\left[     \\begin{array}{ll}     l &amp; |l| &lt; 2 \\\\     qsort(\\{x|x \\in l \\wedge x &lt; head(l) \\}) \\uplus \\{head(l)\\} \\uplus qsort(\\{x|x\\in l \\wedge \\neg(x &lt; head(l)) \\}) &amp; otherwise     \\end{array} \\right . \\end{array} \\] <p>where \\(\\uplus\\) unions two bags and maintains the order.</p>"},{"location":"notes/fp_haskell/#concatmap-and-list-comprehension","title":"concatMap and list-comprehension","text":"<p>There is a variant of <code>map</code> method, consider</p> <p><pre><code>l = [1 .. 5]\n\nfoo :: Int -&gt; [Int]\nfoo i = if i `mod` 2 == 0\n        then [i]\n        else []\n\nmap foo l \n</code></pre> would yield <pre><code>[[], [2], [], [4], []]\n</code></pre></p> <p>We would like to get rid of the nested lists and flatten the outer list. </p> <p>One possibility is to:</p> <p><pre><code>concat (map foo l)\n</code></pre> where <code>concat</code> is a function that \"joins\" the sub lists in a list of lists via <code>++</code>. </p> <p>Alternatively, we can use <code>concatMap</code> directly.</p> <pre><code>concatMap foo l\n</code></pre> <p>Like <code>map</code>, <code>concatMap</code> applies its parameter function  to every element in the list. Unlike <code>map</code>, <code>concatMap</code> expects the parameter function produces a list, thus it will join all the sub-lists into one list.</p> <p>With <code>map</code> and <code>concatMap</code>, we can define complex list transformation operations like the following:</p> <pre><code>listProd :: [a] -&gt; [b] -&gt; [(a,b)]\nlistProd la lb = concatMap (\\a -&gt; map (\\b -&gt; (a,b)) lb) la\n\nl2 = ['a', 'b', 'c']\nlistProd l l2\n</code></pre> <p>which produces:</p> <pre><code>[(1,'a'),(1,'b'),(1,'c'),(2,'a'),(2,'b'),(2,'c'),(3,'a'),(3,'b'),(3,'c'),(4,'a'),(4,'b'),(4,'c')]\n</code></pre> <p>Note that Haskell supports list comprehension via the <code>[ ... | ... ] ... yield</code> construct. We could re-express <code>listProd</code> as follows:</p> <p><pre><code>listProd2 :: [a] -&gt; [b] -&gt; [(a,b)]\nlistProd2 la lb = [ (a,b) | a &lt;- la, b &lt;- lb] \n</code></pre> The Haskell compiler desugars list comprehension expressions:</p> <pre><code>[ e | x1 &lt;- e1,  x2 &lt;- e2, ..., xn &lt;- en ] \n````\ninto:\n\n```hs\nconcatMap (\\x1 -&gt; concatMap (\\x2 -&gt; ... map (\\xn -&gt; e) ... ) e2) e1\n</code></pre> <p>The above syntactic sugar not only works for the list data type but any data type with <code>concatMap</code> and <code>map</code> defined (as we will see in the upcoming lessons).</p> <p>A forward reference note. Some of you probably have read about monad operation may find that the above can be rewritten using a do-notation syntax, e.g. <pre><code> listProd3 :: [a] -&gt; [b] -&gt; [(a,b)]\n listProd3 la lb = do { a &lt;- la; b &lt;- lb; return (a,b) }\n</code></pre> which behaves the same and will be desugared into the same form with <code>concatMap</code> and <code>map</code>. This is because the monadic functor primitive operation for lists are <code>map</code> and <code>concatMap</code>. We will discuss this in a few weeks time. </p>"},{"location":"notes/fp_haskell/#algebraic-datatype","title":"Algebraic Datatype","text":"<p>In OOP languages, like Java and C#, we use classes and interfaces to define (abstraction of) data types, making using of the OOP concepts that we have learned. This style of defining data types using abstraction and encapsulation is also known as the abstract datatype.</p> <p>Like many other languages, Haskell supports user defined data type. It takes a different approach, Algebraic Datatype.</p> <p>Consider the following Extended BNF of a math expression.</p> <p>In computer science, extended Backus\u2013Naur form (EBNF) is a family of metasyntax notations, any of which can be used to express a context-free grammar. EBNF is used to make a formal description of a formal language such as a computer programming language. EBNF.</p> \\[ \\begin{array}{rccl} {\\tt (Math Exp)} &amp; e &amp; ::= &amp; e + e \\mid e - e \\mid  e * e \\mid e / e \\mid c \\\\ {\\tt (Constant)} &amp; c &amp; ::= &amp; ... \\mid -1 \\mid 0 \\mid 1 \\mid ... \\end{array} \\] <p>And we would like to implement a function <code>eval</code> which evaluates a \\({\\tt (Math Exp)}\\) to a value.</p> <p>If we were to implement the above with OOP, we would probably use inheritance to extend subclasses of \\({\\tt (Math Exp)}\\), and use if-else statements with <code>instanceof</code> to check for a specific subclass instance. Alternatively, we can also rely on visitor pattern or delegation.</p> <p>It turns out that using Abstract Datatypes to model the above result in some engineering overhead.</p> <ul> <li>Firstly, encapsulation and abstract tend to hide the underlying structure of the given object (in this case, the \\({\\tt Math Exp})\\) terms)</li> <li>Secondly, using inheritance to model the sum of data types is not perfect (Note: the \"sum\" here refers to having a fixed set of alternatives of a datatype, not the summation for numerical values)</li> <li>For instance, there is no way to stop users of the library code from extending new instances of \\({\\tt (MathExp)}\\)</li> </ul> <p>The algebraic datatype is an answer to these issues. In essence, it is a type of data structure that consists of products and sums.</p> <p>In Haskell, we use <code>data</code> to define Algebraic datatypes.</p> <pre><code>data MathExp = \n    Plus  MathExp MathExp | \n    Minus MathExp MathExp |\n    Mult  MathExp MathExp |\n    Div   MathExp MathExp | \n    Const Int\n</code></pre> <p>In the above the <code>MathExp</code> datatype, there are exactly 5 alternatives. Let's take at look at one case, for instance <code>Plus MathExp MathExp</code>, which states that a plus expression has two operands, both of which are of type <code>MathExp</code>.</p> <p>Alternatively, we can use the GADT style with <code>where</code> keyword.</p> <p><pre><code>{-# LANGUAGE GADTs #-}\n\ndata MathExp where\n    Plus  :: MathExp -&gt; MathExp -&gt; MathExp\n    Minus :: MathExp -&gt; MathExp -&gt; MathExp\n    Mult  :: MathExp -&gt; MathExp -&gt; MathExp\n    Div   :: MathExp -&gt; MathExp -&gt; MathExp\n    Const :: Int -&gt; MathExp\n</code></pre> <code>{-# LANGUAGE GADTs #-}</code> declares a language extension pragma. </p> <p>We can represent the math expression <code>(1+2) * 3</code> as <code>Mult (Plus (Const 1) (Const 2)) (Const 3)</code>.  Note that we call <code>Plus</code> , <code>Minus</code>, <code>Mult</code>, <code>Div</code> and <code>Const</code> \"data constructors\", as we use them to construct values of the algebraic datatype <code>MathExp</code>.</p> <p>Next let's implement an evaluation function based the specification:</p> \\[ eval(e) = \\left [ \\begin{array}{cl}                 eval(e_1) + eval(e_2) &amp; if\\ e = e_1+e_2 \\\\                 eval(e_1) - eval(e_2) &amp; if\\ e = e_1-e_2 \\\\                 eval(e_1) * eval(e_2) &amp; if\\ e = e_1*e_2 \\\\                 eval(e_1) / eval(e_2) &amp; if\\ e = e_1/e_2 \\\\                 c &amp; if\\ e = c                 \\end{array}         \\right. \\] <pre><code>eval :: MathExp -&gt; Int\neval e = case e of \n    Plus  e1 e2 -&gt; eval e1 + eval e2\n    Minus e1 e2 -&gt; eval e1 - eval e2\n    Mult  e1 e2 -&gt; eval e1 * eval e2\n    Div   e1 e2 -&gt; eval e1 `div` eval e2\n    Const i     -&gt; i\n</code></pre> <p>In Haskell, algebraic datatype values can be accessed (destructured) via pattern matching.</p> <p>If we run:</p> <pre><code>eval (Mult (Plus (Const 1) (Const 2)) (Const 3))\n</code></pre> <p>we get <code>9</code> as result.</p> <p>Let's consider another example where we can implement some real-world data structures using the algebraic datatype.</p> <p>Suppose for experimental purposes, we would like to re-implement the list datatype in Haskell (even though a builtin one already exists). For simplicity, let's consider a monomorphic version (no generic) version. </p> <p>We will look into the generic version in the next lesson</p> <p>In the following we consider the specification of the <code>MyList</code> data type in EBNF:</p> \\[ \\begin{array}{rccl} {\\tt (MyList)} &amp; l &amp; ::= &amp; Nil \\mid Cons(i,l) \\\\ {\\tt (Int)} &amp; i &amp; ::= &amp; 1 \\mid 2 \\mid   ... \\end{array} \\] <p>And we implement the above in Haskell:</p> <pre><code>data MyList = Nil | Cons Int MyList\n</code></pre> <p>Question: Can you redefine the above using <code>data ... where</code> in GADT style?</p> <p>Next we implement the <code>map</code> function based on the following specification</p> \\[ map(f, l) = \\left [ \\begin{array}{ll}             Nil &amp; if\\ l = Nil\\\\             Cons(f(hd), map(f, tl)) &amp; if\\ l = Cons(hd, tl)             \\end{array} \\right . \\] <p>Then we could implement the map function</p> <pre><code>mapML :: (Int -&gt; Int) -&gt; MyList -&gt; MyList \nmapML f Nil          = Nil \nmapML f (Cons hd tl) = Cons (f hd) (mapML f tl)\n</code></pre> <p>Running <code>mapML (\\x -&gt; x+1) (Cons 1 Nil)</code> yields <code>Cons 2 Nil</code>.</p> <p>yields the same output as above.</p>"},{"location":"notes/fp_haskell/#summary","title":"Summary","text":"<p>In this lesson, we have discussed</p> <ul> <li>Haskell's FP vs Lambda Calculus</li> <li>How to use the list datatype to model and manipulate collections of multiple values.</li> <li>How to use algebraic data type to define user customized data type to solve complex problems.</li> </ul>"},{"location":"notes/fp_haskell_poly/","title":"50.054 - Parametric Polymorphism and Adhoc Polymorphism","text":""},{"location":"notes/fp_haskell_poly/#learning-outcomes","title":"Learning Outcomes","text":"<p>By this end of this lesson, you should be able to </p> <ul> <li>develop parametrically polymorphic Haskell code using Generic, Algebraic Datatype</li> <li>safely mix parametric polymoprhism with adhoc polymoprhism (overloading) using type classes </li> <li>develop generic programming style code using <code>Functor</code> type class.</li> <li>make use of <code>Maybe</code> and <code>Either</code> to handle and manipulate errors and exceptions. </li> </ul>"},{"location":"notes/fp_haskell_poly/#currying","title":"Currying","text":"<p>In functional programming, we could rewrite a function with multiple arguments into a function that takes the first argument and returns another function that takes the remaining arguments.</p> <p>For example,</p> <pre><code>sum :: (Int,Int) -&gt; Int\nsum (x,y) = x + y\n</code></pre> <p>can be rewritten into </p> <pre><code>sum_curry :: Int -&gt; Int -&gt; Int \nsum_curry x y = x + y\n</code></pre> <p>These two functions are equivalent except that</p> <ol> <li>Their invocations are different, e.g. </li> </ol> <pre><code>sum (1,2)\nsum_curry 1 2\n</code></pre> <ol> <li>It is easier to reuse the curried version to define other function, e.g.</li> </ol> <pre><code>plus1 :: Int -&gt; Int \nplus1 x = sum_curry 1 x\n</code></pre>"},{"location":"notes/fp_haskell_poly/#function-composition","title":"Function Composition","text":"<p>In math, let \\(g\\) and \\(f\\) be functions, then</p> \\[ (g \\circ f)(x) \\equiv g(f(x)) \\] <p>In Haskell, there exists a prelude function (predefined function) <code>(.)</code> which composes two functions </p> <pre><code>-- prelude definitions, please don't execute it.\n(.) :: (b -&gt; c) -&gt; (a -&gt; b) -&gt; a -&gt; c \n(.) g f = \\x -&gt; g (f x)\n</code></pre> <p>For example</p> <pre><code>f x = 2 * x + 3 \ng x = x * x \n\n(.) g f 2 == g (f 2)\n</code></pre> <p>In Haskell, the symbols enclosed in a pair of parenthesis are user-defined infix operators. Hence <code>(.) g f 2</code> in the above can be rewritten as </p> <pre><code>(g . f) 2\n</code></pre> <p>Note we have to put extra parathenses, since function applications are left associated, without the paranthesis  <code>g . f 2</code> will be parsed as <code>((g .) f) 2</code> by Haskell which is ill-typed.</p>"},{"location":"notes/fp_haskell_poly/#generics","title":"Generics","text":"<p>Generics is also known as type variables. It enables a language to support parametric polymoprhism. </p>"},{"location":"notes/fp_haskell_poly/#polymorphic-functions","title":"Polymorphic functions","text":"<p>Recall that the <code>reverse</code> function introduced in the last lesson</p> <pre><code>reverse :: [Int] -&gt; [Int] \nreverse l = case l of \n    [] -&gt; [] \n    hd:tl -&gt; reverse tl ++ [hd]\n</code></pre> <p>We argue that the same implementation should work for all lists regardless of their elements' type. Thus, we would replace <code>Int</code> by a type variable <code>a</code>.</p> <pre><code>reverse :: [a] -&gt; [a] \nreverse l = case l of \n    [] -&gt; [] \n    hd:tl -&gt; reverse tl ++ [hd]\n</code></pre>"},{"location":"notes/fp_haskell_poly/#polymorphic-algebraic-datatype","title":"Polymorphic Algebraic Datatype","text":"<p>Recall that the following Algebraic Datatype from the last lesson. </p> <pre><code>data MyList = Nil | Cons Int MyList\n\nmapML f Nil          = Nil \nmapML f (Cons hd tl) = Cons (f hd) (mapML f tl)\n</code></pre> <p>Same observation applies. <code>MyList</code> could have a generic element type <code>a</code> instead of <code>Int</code> and <code>mapML</code> should remains unchanged.</p> <pre><code>data MyList a = Nil | Cons a MyList\n-- mapML definition remains unchanged\n</code></pre> <p>After the update,  <code>MyList</code> does represent a type, but a type constructor. This is because  <code>MyList</code> itself is not a type, but <code>MyList Int</code>, <code>MyList String</code> or <code>MyList a</code> are types. </p>"},{"location":"notes/fp_haskell_poly/#type-class","title":"Type class","text":"<p>Suppose we would like to convert some of the Haskell values to JSON strings, we could rely on overloading.</p> <pre><code>toJS :: Int -&gt; String \ntoJS v = show v\n\ntoJS :: String -&gt; String \ntoJS v = \"'\" ++ v ++ \"'\"\n\ntoJS :: Bool -&gt; String \ntoJS True = \"true\"\ntoJS False = \"false\" \n</code></pre> <p><code>show</code> is a prelude function that converts values to string.</p> <p>However the above is rejected by ghc. </p> <pre><code> Multiple declarations of \u2018toJS\u2019\n</code></pre> <p>We could give different names to the different versions of <code>toJS</code> but this s</p> <pre><code>intToJS :: Int -&gt; String \nintToJS v = show v\n\nstrToJS :: String -&gt; String \nstrToJS v = \"'\" ++ v ++ \"'\"\n\nboolToJS :: Bool -&gt; String \nboolToJS True = \"true\"\nboolToJS False = \"false\" \n</code></pre> <p>This becomes hard to manage as we consider complex datatype.</p> <pre><code>data Contact = Email String | Phone String \n\ncontactToJS :: Contact -&gt; String \ncontactToJS (Email e) = \"{'email': \" ++ strToJS e ++\"}\"\ncontactToJS (Phone ph) = \"{'Phone': \" ++ strToJS ph ++\"}\"\n</code></pre> <p>For now, let's bear with this cumbersomeness and continue to extend our <code>toJS</code> funcitons to handle the follwing data types</p> <pre><code>data Team   =  Team [Person]\ndata Person =  Person String [Contact]\n\nteamToJS :: Team -&gt; String \nteamToJS (Team members) = \"{'team':{ 'members' : \" ++  personsToJS members ++  \"}}\"\n\npersonToJS :: Person -&gt; String \npersonToJS (Person name contacts) = \n    \"{'person':{ 'name':\" ++ strToJS name ++ \", 'contacts':\" ++ contactsToJS contacts ++ \"}}\"\n\npersonsToJS :: [Person] -&gt; String\npersonsToJS persons = \n    let ps = map personToJS persons\n    in \"[\" ++ interleave \",\" ps ++ \"]\"\n\ninterleave :: String -&gt; [String] -&gt; String\ninterleave del [] = []\ninterleave del [x] = x\ninterleave del (x:xs) = x ++ del ++ interleave del xs\n\ncontactsToJS :: [Contact] -&gt; String \ncontactsToJS contacts = \n    let cs = map contactToJS contacts \n    in \"[\" ++ interleave \",\" cs ++ \"]\" \n</code></pre> <p>The second issue is that the <code>personsToJS</code> and <code>contactsToJS</code> are the identical modulo the variable names (and the types). Can we combine two into one?</p> <pre><code>listToJS :: (a -&gt; String) -&gt; [a] -&gt; String \nlistToJS f l = \n    let xs = map f l \n    in \"[\" ++ interleave \",\" xs ++ \"]\" \n\npersonsToJS l = listToJS personToJS l\ncontactsToJS l = listToJS contactToJS l\n</code></pre> <p>The issue is partially resolved, because <code>listToJS</code> expects a function argument of type <code>a -&gt; String</code> although  by specification, we want to restrict it to be one of the <code>toJS</code> functions we defined earlier, but we can't enforce it.</p> <p>At this stage with have many different versions of <code>toJS</code> with different implementations and different shapes of type signature. It is a not a good approach to manage software.</p> <p>One solution to address these issues is to use type class.</p> <pre><code>{-# LANGUAGE FlexibleInstances #-}\n\nclass JS a where \n    toJS :: a -&gt; String \n</code></pre> <p>In the above, we define a type class <code>JS</code> via the <code>class ... where</code> keywords.  If this is the first time you encounter Haskell type class, you could treat it as the Haskell way of definining an interface in Java.  In the above definition, we define an type class <code>JS a</code> which says whatever type <code>a</code> could be in <code>JS a</code> shoud have an obligational implementation of <code>toJS :: a -&gt; String</code>.  </p> <p>The GHC pragma <code>{-# LANGUAGE FlexibleInstances #-}</code> indicates that we need to enable the flexible-insances extension to support <code>JS String</code> (which is <code>JS [Char]</code>). Without this pragma, we can't define complex type expression type class instances that involving a type constructor being applied to non type variables.</p> <p>Using <code>instance ... where</code> keywords, we define some type class instances (concrete implementation) of <code>JS a</code> as follows</p> <p><pre><code>instance JS Int where \n    toJS v = show v\n\ninstance {-# OVERLAPS #-} JS String where \n    toJS v = \"'\" ++ v ++ \"'\"\n\ninstance JS Bool where \n    toJS True = \"true\"\n    toJS False = \"false\" \n\ninstance JS Contact where \n    toJS (Email e) = \"{'email': \" ++ toJS e ++\"}\"\n    toJS (Phone ph) = \"{'Phone': \" ++ toJS ph ++\"}\"\n\ninstance JS Person where \n    toJS (Person name contacts) = \n        \"{'person':{ 'name':\" ++ toJS name ++ \", 'contacts':\" ++ toJS contacts ++ \"}}\"\n\ninstance JS Team where \n    toJS (Team members) = \"{'team':{ 'members' : \" ++  toJS members ++  \"}}\"\n\ninstance JS b =&gt; JS [b] where \n    toJS as = \n        let xs = map toJS l \n        in \"[\" ++ interleave \",\" xs ++ \"]\" \n</code></pre> In each instance, we \"specialize\" the type parameter <code>a</code> in <code>JS a</code> with another more concreate type. In the body of the instance, we provide the concrete implementation of the <code>toJS</code> function with the specific type. </p> <ul> <li>One alarming thing is that the <code>JS [b]</code> instance is overlapping with <code>JS String</code>, because in Haskell <code>String</code> is a type alias of <code>[Char]</code>. Hence we argue that <code>JS [Char]</code> is overlapping with <code>JS [b]</code>. Hence we need to add an instance pragma <code>{-# OVERLAPS #-}</code> to tell the ghc compiler to try apply <code>JS [Char]</code> whenever possible, otherwise, try <code>JS [b]</code>. </li> <li>Another \"magical\" thing of Haskell type class is that the use of the <code>toJS</code> function is overloaded based on the type context which can be automatically resolved by the compiler. For example, in the body of the <code>JS Team</code> instance, the use <code>toJS members</code> is resolved to the isntance <code>JS [Person]</code> which will be given by the instances <code>JS [b]</code> and <code>JS Person</code>. </li> <li>Thirdly, the <code>JS [b]</code> instance, relies on a context, namely <code>JS b =&gt;</code>. The context <code>JS b</code> introduces an \"type level assumption\" under which the use of <code>toJS</code> in <code>map toJS l</code> must be well-defined given <code>l</code> has type <code>[b]</code> and <code>JS b</code> has been assumed existing. </li> </ul> <p>Finally, we can test the code, </p> <p><pre><code>myTeam = Team [ Person \"kenny\" [Email \"kenny_lu@sutd.edu.sg\"], \n    Person \"simon\" [Email \"simon_perrault@sutd.edu.sg\"]]\n\ntoJS myTeam \n</code></pre> yields</p> <pre><code>'team':{ 'members':['person':{ 'name':'kenny',  'contacts':['email': 'kenny_lu@sutd.edu.sg'] },'person':{ 'name':'simon',  'contacts':['email': 'simon_perrault@sutd.edu.sg'] }] }\n</code></pre> <p>Note that when we call a function that requires a type class context, we do not need to provide the argument for the type class instance. </p> <pre><code>printAsJSON :: JS a =&gt; a -&gt; IO () \nprintAsJSON v = print (toJS v)\n\nprintAsJSON myTeam\n</code></pre> <p>Type class enables us to develop modular and resusable codes. It is related to a topic of Generic Programming. In computer programming, generic programming refers to the coding approach which an instance of code is written once and used for many different types/instances of values/objects.</p> <p>In the next few section, we consider some common patterns in FP that are promoting generic programming.</p>"},{"location":"notes/fp_haskell_poly/#functor","title":"Functor","text":"<p>Recall that we have a <code>map</code> method for list datatype. </p> <pre><code>l = [1,2,3]\nmap (\\x -&gt; x + 1) l\n</code></pre> <p>Can we make <code>map</code> to work for other data type? For example</p> <pre><code>data BTree a = Empty | \n    Node a (BTree a) (BTree a) -- ^ a node with a value and the left and right sub trees.\n</code></pre> <p>It turns out that extending <code>map</code> to different datatypes is similar to <code>toJS</code> function that we implemented earlier. We consider using a type class for this purpose. The following is a prelude type class in GHC.</p> <pre><code>-- prelude definitions, please don't execute it.\nclass Functor t where \n    fmap :: (a -&gt; b) -&gt; t a -&gt; t b\n</code></pre> <p>In the above type class definition, <code>t</code> denotes a type parameter of kind <code>* -&gt; *</code>. A kind is a type of types. In the above, <code>* -&gt; *</code> it means <code>Functor</code>'s argument must be a type constructor. For instance, it could be <code>MyList</code> or <code>BTree</code> and etc, but not <code>Int</code> and <code>Bool</code>. (C.f. In the type class <code>JS</code>, the type argument has kind <code>*</code>.)</p> <p>The following is a prelude type class instance for <code>Functor List</code> (or as short-hand <code>Functor []</code>)  <pre><code>-- prelude definitions, please don't execute it.\ninstance Functor List where \n    fmap f l = map f l\n</code></pre></p> <p>For the <code>BTree</code> data type, we need to provide the type class instance as follows,</p> <pre><code>instance Functor BTree where \n    fmap f Empty = Empty\n    fmap f (Node v lft rgt) = \n        Node (f v) (fmap f lft) (fmap f rgt)\n</code></pre> <p>Some examples</p> <pre><code>l = [1,2,3]\nfmap (\\x -&gt; x + 1) l \n-- yields [2,3,4]\n\nt = Node 2 (Node 1 Empty Empty) (Node 3 Empty Empty)\nfmap (\\x -&gt; x + 1) t\n-- yields Node 3 (Node 2 Empty Empty) (Node 4 Empty Empty)\n</code></pre>"},{"location":"notes/fp_haskell_poly/#functor-laws","title":"Functor Laws","text":"<p>All instances of functor must obey a set of mathematic laws for their computation to be predictable.</p> <p>Let <code>i</code> be a functor instance 1. Identity: <code>\\i-&gt; fmap \\x-&gt;x i</code> \\(\\equiv\\) <code>\\y -&gt; y</code>. When performing the mapping operation, if the values in the functor are mapped to themselves, the result will be an unmodified functor. 2. Composition Morphism: <code>\\i-&gt; fmap (f . g) i</code> \\(\\equiv\\) <code>(\\i -&gt; fmap f i) . (\\j -&gt; fmap g j)</code>. If two sequential mapping operations are performed one after the other using two functions, the result should be the same as a single mapping operation with one function that is equivalent to applying the first function to the result of the second.</p>"},{"location":"notes/fp_haskell_poly/#foldable","title":"Foldable","text":"<p>Similarly we find a prelude type class <code>Foldable</code> for <code>foldl</code> and <code>foldr</code> operations</p> <pre><code>-- prelude definitions, please don't execute it.\nclass Foldable t where \n    foldl :: (b -&gt; a -&gt; b) -&gt; b -&gt; t a -&gt; b\n    foldr :: (a -&gt; b -&gt; b) -&gt; b -&gt; t a -&gt; b\n\n\ninstance Foldable List where \n    foldl f acc [] = acc \n    foldl f acc (x:xs) = foldl f (f acc x) xs\n    foldr f acc [] = acc\n    foldr f acc (x:xs) = f x (foldr f acc xs)\n</code></pre> <p>As for the <code>BTree</code> datatype, we need to define the instance as follows</p> <pre><code>instance Foldable BTree where \n    foldl f acc Empty = acc\n    foldl f acc (Node v lft rgt) = \n        let acc1 = f acc v\n            acc2 = foldl f acc1 lft\n        in foldl f acc2 rgt\n    foldr f acc Empty = acc\n    foldr f acc (Node v lft rgt) = \n        f v (foldr f (foldr f acc rgt) lft)\n\nfoldl (\\x y -&gt; x + y) 0 l -- yields 6\nfoldl (\\x y -&gt; x + y) 0 t -- yields 6\n</code></pre>"},{"location":"notes/fp_haskell_poly/#maybe-and-either","title":"Maybe and Either","text":"<p>Recall in the earlier lesson, we encountered the following example. </p> <pre><code>data MathExp = \n    Plus  MathExp MathExp | \n    Minus MathExp MathExp |\n    Mult  MathExp MathExp |\n    Div   MathExp MathExp | \n    Const Int\n\neval :: MathExp -&gt; Int\neval e = case e of \n    Plus  e1 e2 -&gt; eval e1 + eval e2\n    Minus e1 e2 -&gt; eval e1 - eval e2\n    Mult  e1 e2 -&gt; eval e1 * eval e2\n    Div   e1 e2 -&gt; eval e1 `div` eval e2\n    Const i     -&gt; i\n</code></pre> <p>An error occurs when we try to evalue a <code>MathExp</code> which contains a division by zero sub-expression. Executing </p> <p><pre><code>eval (Div (Const 1)  (Minus (Const 2) (Const 2)))\n</code></pre> yields</p> <pre><code>*** Exception: divide by zero\n</code></pre> <p>Like other main stream languages, we could use <code>try-catch</code> statement to handle the exception.  One downside of this approach is that at compile type it is hard to track the unhandled exceptions.</p> <p>A more fine-grained approach is to use algebraic datatype to \"inform\" the compiler (and other programmers who use this function and datatypes) that this function is not always producing an integer as the result.</p> <p>Consider the following prelude Haskell datatype <code>Maybe</code></p> <pre><code>-- prelude definitions, please don't execute it.\ndata Maybe a = None | Just a\n</code></pre> <pre><code>eval :: MathExp -&gt; Maybe Int \neval (Plus e1 e2) = case eval e1 of \n    Nothing -&gt; Nothing \n    Just v1 -&gt; case eval e2 of \n        Nothing -&gt; Nothing \n        Just v2 -&gt; Just (v1 + v2)\neval (Minus e1 e2) = case eval e1 of \n    Nothing -&gt; Nothing \n    Just v1 -&gt; case eval e2 of \n        Nothing -&gt; Nothing \n        Just v2 -&gt; Just (v1 - v2)\neval (Mult e1 e2) = case eval e1 of \n    Nothing -&gt; Nothing \n    Just v1 -&gt; case eval e2 of \n        Nothing -&gt; Nothing \n        Just v2 -&gt; Just (v1 * v2)\neval (Div e1 e2) = case eval e1 of \n    Nothing -&gt; Nothing \n    Just v1 -&gt; case eval e2 of \n        Nothing -&gt; Nothing\n        Just 0  -&gt; Nothing \n        Just v2 -&gt; Just (v1 `div` v2)\neval (Const v) = Just v \n</code></pre> <p>When we execute <code>eval (Div (Const 1)  (Minus (Const 2) (Const 2)))</code>,  we get <code>Nothing</code> as the result instead of the exception. One advantage of this is that whoever is using <code>eval</code> function has to respect that its return type is <code>Maybe Int</code> instead of just <code>Int</code> therefore, a <code>case</code> pattern matching must be applied before using the result to look out for potential <code>Nothing</code> value.</p> <p>There are still two drawbacks. Firstly, the updated version of the <code>eval</code> function is much more verbose compared to the original unsafe version. We will address this issue in the next lesson. Secondly, we lose the chance of reporting where the division by zero has occured. Let's address the second issue.</p> <p>We could instead of using <code>Maybe</code>, use the <code>Either</code> datatype, which is also defined in the prelude.</p> <pre><code>-- prelude definitions, please don't execute it.\ndata Either a b = Left a | Right b\n</code></pre> <p>We adjust the definition as follows</p> <pre><code>type ErrMsg = String\neval :: MathExp -&gt; Either ErrMsg Int \neval (Plus e1 e2) = case eval e1 of \n    Left err -&gt; Left err \n    Right v1 -&gt; case eval e2 of \n        Left err -&gt; Left err \n        Right v2 -&gt; Right (v1 + v2)\neval (Minus e1 e2) = case eval e1 of \n    Left err -&gt; Left err \n    Right v1 -&gt; case eval e2 of \n        Left err -&gt; Left err \n        Right v2 -&gt; Right (v1 - v2)\neval (Mult e1 e2) = case eval e1 of \n    Left err -&gt; Left err \n    Right v1 -&gt; case eval e2 of \n        Left err -&gt; Left err \n        Right v2 -&gt; Right (v1 * v2)\neval (Div e1 e2) = case eval e1 of \n    Left err -&gt; Left err \n    Right v1 -&gt; case eval e2 of \n        Left err -&gt; Left err \n        Right 0  -&gt; Left (\"div by zero caused by \" ++ show (Div e1 e2))\n        Right v2 -&gt; Right (v1 `div` v2)\neval (Const v) = Right v \n</code></pre> <p>To make <code>show (Div e1 e2)</code> to works, we need to make the <code>MathExp</code> type as an instance of the <code>Show</code> type class or a quick fix</p> <pre><code>data MathExp = ...\n    deriving Show -- auto derive the Show instance for this data type.\n</code></pre> <p>Executing <code>eval (Div (Const 1)  (Minus (Const 2) (Const 2)))</code> yields</p> <pre><code>Left \"div by zero caused by Div (Const 1) (Minus (Const 2) (Const 2))\"\n</code></pre>"},{"location":"notes/fp_haskell_poly/#summary","title":"Summary","text":"<p>In this lesson, we have discussed </p> <ul> <li>how to develop parametrically polymorphic haskell code using Generic, Algebraic Datatype</li> <li>how to safely mix parametric polymoprhism with adhoc polymoprhism (overloading) using type classes </li> <li>how to develop generic programming style code using <code>Functor</code> type class.</li> <li>how to make use of <code>Maybe</code> and <code>Either</code> to handle and manipulate errors and exceptions. </li> </ul>"},{"location":"notes/fp_haskell_poly/#appendix","title":"Appendix","text":""},{"location":"notes/fp_haskell_poly/#generalized-algebraic-data-type","title":"Generalized Algebraic Data Type","text":"<p>Generalized Algebraic Data Type is an extension to Algebraic Data Type, in which each case extends a more specific version of the top level algebraic data type. Consider the following example.</p> <p>Firstly, we need some type acrobatics to encode nature numbers on the level of type. </p> <pre><code>data Zero = Zero \ndata Succ a = Succ a  \n</code></pre> <p>We use these two data types to encode natural numbers, </p> <pre><code>Zero             -- as 0\nSucc Zero        -- as 1\nSucc (Succ Zero) -- as 2\n</code></pre> <p>Next we define our GADT <code>SList s a</code> which is a generic list of elements <code>a</code> and with size <code>s</code>. </p> <pre><code>data SList s a where \n    Nil  :: SList Zero a                        -- ^ additional type constraint, s = Zero\n    Cons :: a -&gt; SList n a -&gt; SList (Succ n) a  -- ^ additional type constraint, s = (Succ n) for some n\n</code></pre> <p>In the first subcase <code>Nil</code>, it is declared with the type of <code>SList Zero a</code> which indicates on type level that the list is empty. In the second case <code>Cons</code>, we define it to have the type <code>SList (Succ n) a</code> for some natural number <code>n</code>. This indicates on the type level that the list is non-empty. </p> <p>Having these information lifted to the type level allows us to define a type safe <code>head</code> function.</p> <pre><code>head :: SList (Succ n) a -&gt; a \nhead (Cons hd tl) = hd \n</code></pre> <p>Compiling <code>head Nil</code> yields a type error. </p> <p>Similarly we can define a size-aware function <code>snoc</code> which add an element at the tail of a list. </p> <pre><code>snoc :: a -&gt; SList n a -&gt; SList (Succ n) a \nsnoc v nil          = Cons v nil\nsnoc v (Cons hd tl) = Cons hd (snoc v tl)\n</code></pre> <p>If for some reason, we replace the 2<sup>nd</sup> clause of <code>snoc</code> with </p> <p><pre><code>snoc v (Cons hd tl) = snoc v tl\n</code></pre> It will result in a compilation error. </p>"},{"location":"notes/fp_intro/","title":"50.054 - Introduction to functional programming","text":""},{"location":"notes/fp_intro/#learning-outcomes","title":"Learning Outcomes","text":"<p>By the end of this lesson, you should be able to:</p> <ul> <li>Characterize functional programming</li> <li>Comprehend, evaluate lambda terms</li> <li>Differentiate different evaluation strategies</li> <li>Implement simple algorithms using Lambda Calculus</li> </ul>"},{"location":"notes/fp_intro/#what-is-functional-programming","title":"What is Functional programming?","text":"<p>Functional programming as suggested by its name, is a programming paradigm in which functions are first class values. In the ideal model of FP, computations are stateless. Variables are bound once and remain unchanged afterwards. Computation is performed by rewriting the current expression into the (reduced) expression via evaluation rules.</p>"},{"location":"notes/fp_intro/#how-fp-differs-from-other-programming-languages","title":"How FP differs from other programming languages?","text":"<p>The main differences were listed in the earlier section.</p> <p>However many modern program languages (including those are not FP) adopted many \"features\" from the functional programming paradigm. It has been proven that the FP coding style improves code qualities in many aspects.</p> <p>Consider the following two different implementations of the insertion sort algorithm (assuming that the readers having prior knowledge of Python and insertion sort algorithm):</p> <pre><code>def isort(vals):\n   for i in range(1, len(vals)):\n      curr = i   \n      for j in range(i, 0, -1):\n         # scan backward to insert vals[curr] into the right pos\n         if vals[curr] &gt; vals[j-1]:\n            vals[curr], vals[j-1] = vals[j-1], vals[curr]\n            curr = j-1\n   return vals\n</code></pre> <pre><code>def isort2(vals):\n   def insert(x, xs):\n      # invarant: xs is already sorted in descending order\n      if len(xs) &gt; 0:\n         if x &gt; xs[0]:\n            return [x] + xs\n         else:\n            return [xs[0]] + insert(x, xs[1:])\n      else:\n         return [x]\n   def isort_sub(sorted, to_be_sorted):\n      # invariant sorted is already sorted in descending order\n      if len(to_be_sorted) &gt; 0:\n         val = to_be_sorted[0]\n         to_be_sorted_next = to_be_sorted[1:]\n         sorted_next = insert(val, sorted)\n         return isort_sub(sorted_next, to_be_sorted_next)\n      else:\n         return sorted\n   return isort_sub([], vals)\n</code></pre> <p><code>isort</code> is implemented in the imperative style; the way we are familiar with.</p> <p><code>isort2</code> is implemented in a functional programming style, we've seen it but we are not too familar with it. We probably won't code in <code>isort2</code> in Python, because:</p> <ol> <li>it is lengthy</li> <li>it is less efficient, as it involves recursion (function call stack is building up) and there are too many list slicing and concatenation operations.</li> </ol> <p>But why are people are interested in FP? The reason is that the invariant of <code>isort</code> is much harder to derive compared to <code>isort2</code> in which the nested functions' parameters are the subject of the invariants, and the variables in <code>isort2</code> are mostly immutable, i.e. they don't change over execution, and we won't need symbolic execution or variable renaming.</p> <p>Furthermore in some FP languages with advanced typing systems such as type constraints and dependent types, these invariants in <code>isort2</code> can be expressed as type constraints, which can be verified by the compiler.</p> <p>What about the inefficiency? Most of the FP compilers handle recursions with care and are able to optimize them into efficient code. Data structure in FP are inductively defined, and optimizations such as shallow cloning are used to avoid data structure reconstruction.</p> <p>In fact many modern FP languagues are quite fast. For example:</p> <ul> <li>https://benchmarksgame-team.pages.debian.net/benchmarksgame/fastest/ghc-clang.html</li> <li>https://thume.ca/2019/04/29/comparing-compilers-in-rust-haskell-c-and-python/</li> </ul>"},{"location":"notes/fp_intro/#why-fp-in-compiler-design","title":"Why FP in Compiler Design?","text":"<p>Implementing a compiler requires rigorous software design and engineering principles. Bugs arising from a compiler have severe implication in softwares developed in the language that it compiles.</p> <p>To establish correctness results, testing is not sufficient to eliminate errors in the compilers. When designing a compiler, we often begin with formal reasoning with mathematical foundation as the specification. As we learn later in this module, these specifications are presented in a form in which resembles the data structures and accessor methods found in many functional programming languages. Thus, it is arguably easier to implement the given specification in function programs compared to other programming paradigms.</p> <p>One key difference is that in FP, there is no for-loop or while-loop. Iteration has to be implemented via recursive functions.</p> <p>This implies that loop invariances are not constraints among the input and output of these recurisve function.</p> <p>In many main stream functional programming languages, such as Ocaml, Haskell and Scala are shipped with powerful type systems which allow us to express some of the properties of the algorithms in terms of type constraints, by doing so, these (invariant) properties are verifiable by the compilers of function programming languages.</p>"},{"location":"notes/fp_intro/#lambda-calculus","title":"Lambda Calculus","text":"<p>Lambda Calculus is the minimal core of many functional programming languages. It consists of the Lambda expression and the evaluation rule(s).</p>"},{"location":"notes/fp_intro/#lambda-expression","title":"Lambda Expression","text":"<p>This comic gives a very easy way to understand Lambda Expressions</p> <p>The valid syntax of lambda expression is described as the following EBNF (Extended Backus Naur Form) grammar:</p> \\[ \\begin{array}{rccl}  {\\tt (Lambda\\ Terms)} &amp; t &amp; ::= &amp; x \\mid \\lambda x.t \\mid t\\ t \\end{array} \\] <p>Where:</p> <ul> <li>Each line denotes a grammar rule</li> <li>The left hand side (LHS) of the <code>::=</code> is a non-terminal symbol, in this case \\(t\\) is a non-terminal symbol.</li> <li>The RHS of the <code>::=</code> is a set of alternatives, separated by <code>|</code>. Each alternative denote a possible outcome of expanding the LHS non-terminal. In this case \\(t\\) has three possibilities, i.e. \\(x\\), \\(\\lambda x.t\\) or \\(t\\ t\\).</li> <li>\\(x\\) denotes a variable,</li> <li>\\(\\lambda x.t\\) denotes a lambda abstraction.</li> <li>Within a lambda abstraction,  \\(x\\) is the bound variable (c.f. formal argument of the function) and \\(t\\) is the body.</li> <li>\\(t\\ t\\) denotes a function application.</li> </ul> <p>For example, the following are three instances of \\(t\\).</p> <ol> <li>\\(x\\)</li> <li>\\(\\lambda x.x\\)</li> <li>\\((\\lambda x.x)\\ y\\)</li> </ol> <p>Note that given a lambda term, there might be multiple ways of parsing (interpreting) it. For instance, Given \\(\\lambda x.x\\ \\lambda y.y\\), we could interpret it as either</p> <ol> <li>\\((\\lambda x.x)\\ (\\lambda y.y)\\), or</li> <li>\\(\\lambda x.(x\\ \\lambda y.y)\\)</li> </ol> <p>As a convention, in the absence of parentheses, we take 2 as the default interpretation. We should include parentheses whenever ambiguity arise as much as we can.</p>"},{"location":"notes/fp_intro/#evaluation-rules","title":"Evaluation Rules","text":"<p>Lambda calculus is very simple and elegant. To execute (or we often say \"to evaluate\") a given lambda term, we apply the evaluation rules to rewrite the term.</p> <p>There are only two rules to consider.</p> <p>Each rule is defined via a reduction relation \\(t \\longrightarrow t'\\), which reads as \\(t\\) is reduced to \\(t'\\) by a step.</p>"},{"location":"notes/fp_intro/#beta-reduction","title":"Beta Reduction","text":"\\[ \\begin{array}{rl} {\\tt (\\beta\\ reduction)} &amp; (\\lambda x.t_1)\\ t_2 \\longrightarrow [t_2/x] t_1 \\end{array} \\] <p>What's new here is the term \\([t_2/x]\\), which is a meta term, which refers to a substitution. \\([t_2/x]t_1\\) denotes the application of the substitution \\([t_2/x]\\) to \\(t_1\\), Informally speaking it means we replace every occurrence of the formal argument \\(x\\) in \\(t_1\\) with \\(t_2\\).</p> <p>For instance, recall our earlier example:</p> \\[ \\begin{array}{rl} (\\lambda x.x)\\ (\\lambda y.y) &amp; \\longrightarrow_{\\scriptsize {\\tt (\\beta\\ reduction)}} \\\\ \\lbrack(\\lambda y.y)/x \\rbrack x &amp; \\longrightarrow _{\\scriptsize {\\tt (substitution)}} \\\\ \\lambda y.y \\end{array} \\] <p>It is common understanding in programming that there are scopes of variables. We can reuse the same name for different variables in different scopes without affecting the meanings of the program. Consider a variant of our running example:</p> \\[  (\\lambda x.x)\\ {\\tt (\\lambda x. x)} \\] <p>Here, we use a different font type for variables named \\(x\\) in different scopes. \\(x\\) is bound in the first lambda abstraction and \\({\\tt x}\\) is bound in the second lambda abstraction. It behaves the same as the original running example except for the name of the variable in the second lambda abstraction.</p> <p>To formally define the substitution operation used in the \\(\\beta\\) reduction rule, we need to compute the free variables, i.e. variables that are not bound.</p> \\[ \\begin{array}{rcl} fv(x) &amp; = &amp; \\{x\\}\\\\ fv(\\lambda x.t) &amp; = &amp; fv(t) - \\{x\\} \\\\ fv(t_1\\ t_2) &amp; = &amp; fv(t_1) \\cup fv(t_2) \\end{array} \\] <p>For instance.</p> \\[ \\begin{array}{rcl} fv(\\lambda x.x) &amp; = &amp; fv(x) - \\{x\\} \\\\                 &amp; = &amp; \\{ \\} \\end{array} \\] \\[ \\begin{array}{rcl} fv(\\lambda x.x\\ (\\lambda z.y\\ z)) &amp; = &amp; fv(x\\ (\\lambda z.y\\ z)) - \\{x\\} \\\\    &amp; = &amp; (\\{x\\} \\cup fv(\\lambda z.y\\ z)) - \\{x\\} \\\\    &amp; = &amp; (\\{x\\} \\cup (fv(y\\ z) - \\{z\\})) - \\{x\\} \\\\    &amp; = &amp; (\\{x\\} \\cup ((\\{y\\} \\cup \\{z\\}) - \\{z\\})) - \\{x\\} \\\\    &amp; = &amp; (\\{x\\} \\cup (\\{y, z\\} - \\{z\\})) - \\{x\\} \\\\    &amp; = &amp; \\{ y \\} \\end{array} \\] <p>One common error we often encounter is capturing the free variables.</p> <p>Consider:</p> \\[ (\\lambda x. \\lambda y.x\\ y)\\ ({\\tt y}\\ w) \\] <p>Note:</p> \\[ fv((\\lambda x. \\lambda y.x\\ y)\\ ({\\tt y}\\ w)) =  \\{ {\\tt y}, w \\} \\] <p>Thus:</p> \\[ \\begin{array}{rl} (\\lambda x. \\lambda y.x\\ y)\\ ({\\tt y}\\ w) &amp; \\longrightarrow \\\\ \\lbrack({\\tt y}\\ w)/x\\rbrack \\lambda y.x\\ y &amp; \\longrightarrow  \\\\ \\lambda y. ({\\tt y}\\ w)\\ y \\end{array} \\] <p>Error! We captured the free variable \\({\\tt y}\\) in the lambda abstraction accidentally via substitution. Now the free variable \\({\\tt y}\\) is \"mixed up\" with the lambda bound variable \\(y\\) by mistake.</p>"},{"location":"notes/fp_intro/#substitution-and-alpha-renaming","title":"Substitution and Alpha Renaming","text":"<p>In the following we consider all the possible cases for subsititution</p> \\[ \\begin{array}{rcll}  \\lbrack t_1 / x \\rbrack x &amp; = &amp; t_1 \\\\  \\lbrack t_1 / x \\rbrack y &amp; = &amp; y &amp; {\\tt if}\\  x \\neq y \\\\  \\lbrack t_1 / x \\rbrack (t_2\\ t_3) &amp; = &amp; \\lbrack t_1 / x \\rbrack t_2\\  \\lbrack t_1 / x \\rbrack t_3 &amp; \\\\  \\lbrack t_1 / x \\rbrack \\lambda y.t_2 &amp; = &amp; \\lambda y. \\lbrack t_1 / x  \\rbrack t_2 &amp; {\\tt if}\\  y\\neq x\\  {\\tt and}\\  y \\not \\in fv(t_1) \\end{array} \\] <p>In case  </p> \\[ y\\neq x\\  {\\tt and} \\ y \\not \\in fv(t_1) \\] <p>is not satified, we need to rename the lambda bound variables that are clashing. Recall:</p> \\[ (\\lambda x. \\lambda y.x\\ y)\\ ({\\tt y}\\ w) \\] <p>We rename the inner lambda bound variable \\(y\\) to \\(z\\):</p> \\[ (\\lambda x. \\lambda z.x\\ z)\\ ({\\tt y}\\ w) \\] <p>to avoid clashing, prior applying the \\(\\beta\\) reduction. The renaming operation is also known as the \\(\\alpha\\) renaming.</p>"},{"location":"notes/fp_intro/#evaluation-strategies","title":"Evaluation strategies","text":"<p>So far we have three rules (roughly)  $\\beta $ reduction, substitution, and  $\\alpha $ renaming.</p> <p>Given a lambda term, in order to evaluate it, we need to identify places that we can apply these rules.</p> <p>We call a (sub-)expression of shape \\((\\lambda x.t_1)\\ t_2\\) a redex.</p> <p>The task is to look for redexes in a lambda term and rewrite them by applying  \\(\\beta\\) reduction and substitution, and sometimes  \\(\\alpha\\) renaming to avoid capturing free variables.</p> <p>But in what order shall we apply these rules?</p> <p>There are two mostly known strategies</p> <ol> <li>Inner-most, leftmost - Applicative Order Reduction (AOR)</li> <li>Outer-most, leftmost - Normal Order Reduction (NOR)</li> </ol> <p>Consider \\((\\lambda x. ((\\lambda x. x)\\ x))\\ (\\lambda y.y)\\),</p> <ul> <li>AOR:</li> </ul> \\[ \\begin{array}{rll} (\\lambda x. (\\underline{(\\lambda x. x)\\ x}))\\ (\\lambda y.y)  &amp; \\longrightarrow_{\\tt (\\beta\\ reduction)} &amp;\\\\ \\underline{(\\lambda x.x)\\ (\\lambda y.y)}  &amp; \\longrightarrow_{\\tt (\\beta\\ reduction)} \\\\ \\lambda y.y \\end{array} \\] <ul> <li>NOR:</li> </ul> \\[ \\begin{array}{rl} \\underline{(\\lambda x. ((\\lambda x. x)\\ x))}\\ (\\lambda y.y)  &amp; \\longrightarrow_{\\tt(\\alpha)} \\\\ \\underline{(\\lambda z. [z/x]((\\lambda x.x)\\ x))}\\ (\\lambda y.y) &amp; \\longrightarrow_{\\tt (substitution)} \\\\ \\underline{(\\lambda z. ((\\lambda x. x)\\ z))\\ (\\lambda y.y)}  &amp; \\longrightarrow_{\\tt(\\beta)} \\\\   \\underline{(\\lambda x. x)\\ (\\lambda y.y)} &amp; \\longrightarrow_{\\tt (\\beta)}  \\\\ \\lambda y.y \\end{array} \\]"},{"location":"notes/fp_intro/#interesting-notes","title":"Interesting Notes","text":"<ol> <li>Some connection with real world languages:</li> <li>Call By Value semantics (CBV, found in C, C++, etc.) is like AOR except that we do not evaluate under lambda abstractions.</li> <li> <p>Call By Name semantics (CBN, found in Haskell, etc.) is like NOR except that we do not evaluate under lambda abstractions.</p> </li> <li> <p>AOR or NOR, which one is better?</p> </li> <li>By Church-Rosser Theorem, if a lambda term can be evaluated in   two different ways and both ways terminate, both will yield the same   result.</li> <li>Recall our earlier example.</li> <li> <p>So how can it be non-terminating? Consider:</p> \\[ \\begin{array}{rl} (\\lambda x.x\\ x)\\ (\\lambda x.x\\ x) &amp; \\longrightarrow \\\\ \\lbrack(\\lambda x.x\\ x)/x\\rbrack (x\\ x) &amp; \\longrightarrow \\\\ (\\lambda x.x\\ x)\\ (\\lambda x.x\\ x)  &amp; \\longrightarrow \\\\ ... \\end{array} \\] </li> <li> <p>NOR seems computationally more expensive, but is also more likely to terminate than AOR. Consider how     \\(((\\lambda x.\\lambda y.x)\\ x)\\  ((\\lambda x.x\\ x)\\ (\\lambda x.x\\ x))\\) terminates in NOR with \\(x\\), but diverges in AOR.</p> </li> <li>NOR can be used to evaluate terms that deals with infinite data.</li> </ol>"},{"location":"notes/fp_intro/#let-binding","title":"Let Binding","text":"<p>Let-binding allows us to introduce local (immutable) variables.</p>"},{"location":"notes/fp_intro/#approach-1-extending-the-syntax-and-evaluation-rules","title":"Approach 1 - extending the syntax and evaluation rules","text":"<p>We extend the syntax with let-binding:</p> \\[ \\begin{array}{rccl}  {\\tt (Lambda\\ Terms)} &amp; t &amp; ::= &amp; x \\mid \\lambda x.t \\mid t\\ t \\mid let\\ x=\\ t\\ in\\ t \\end{array} \\] <p>and the evaluation rule:</p> \\[ \\begin{array}{rl} {\\tt (Let)} &amp; let\\ x=t_1\\ in\\ t_2 \\longrightarrow [t_1/x]t_2 \\\\ \\\\ \\end{array} \\] <p>and the substitution rule and the free variable function \\(fv()\\):</p> <pre><code>\\begin{array}{rcl}\n\\lbrack t_1 / x \\rbrack let\\ y = t_2\\ in\\ t_3 &amp; = &amp; let\\ y = \\lbrack t_1 / x \\rbrack t_2\\ in\\ \\lbrack t_1 / x \\rbrack t_3 &amp; {\\tt if}\\  y\\neq x\\  {\\tt and}\\  y \\not \\in fv(t_1) \\\\\n\\end{array} \n</code></pre> <pre><code>\\begin{array}{rcl}\nfv(let\\ x=t_1\\ in\\ t_2) &amp; = &amp; (fv(t_1) - \\{x\\}) \\cup fv(t_2) \\\\\n\\end{array}\n</code></pre> <p>Note that the alpha renaming should be applied when name clash arises.</p>"},{"location":"notes/fp_intro/#approach-2-desugaring","title":"Approach 2 - desugaring","text":"<p>In the alternative approach, we could use a pre-processing step to desugar the let-binding into an application. In compiler context, desugaring refers to the process of rewriting the source code from some high-level form to the core language.</p> <p>We can rewrite:</p> \\[ let\\ x=t_1\\ in\\ t_2 \\] <p>into:</p> \\[ (\\lambda x.t_2)\\ t_1 \\] <p>where \\(x \\not\\in fv(t_1)\\).</p> <p>What happen if \\(x \\in fv(t_1)\\)? It forms a recursive definition. We will look into recursion in a later section.</p>"},{"location":"notes/fp_intro/#conditional-expression","title":"Conditional Expression","text":"<p>A language is pretty much useless without conditional \\(if\\ t_1\\ then\\ t_2\\ else\\ t_3\\). There are at least two different ways of incorporating conditional expression in our lambda term language.</p>"},{"location":"notes/fp_intro/#approach-1-extending-the-syntax-and-the-evaluation-rules","title":"Approach 1 - Extending the syntax and the evaluation rules","text":"<p>We could extend the grammar</p> \\[ \\begin{array}{rccl}  {\\tt (Lambda\\ Terms)} &amp; t &amp; ::= &amp; x \\mid \\lambda x.t \\mid t\\ t \\mid let\\ x=\\ t\\ in\\ t \\mid  if\\ t\\ then\\ t\\ else\\ t \\mid t\\ op\\ t \\mid c \\\\  {\\tt (Builtin\\ Operators)} &amp; op &amp; ::= &amp; + \\mid - \\mid * \\mid / \\mid\\ == \\\\  {\\tt (Builtin\\ Constants)} &amp; c &amp; ::= &amp; 0 \\mid 1 \\mid ... \\mid true \\mid false \\end{array} \\] <p>and the evaluation rules</p> \\[ \\begin{array}{rc} {\\tt (\\beta\\ reduction)} &amp; (\\lambda x.t_1)\\ t_2 \\longrightarrow [t_2/x] t_1 \\\\ \\\\ {\\tt (ifI)} &amp; \\begin{array}{c}                t_1 \\longrightarrow t_1'  \\\\                \\hline                if\\ t_1\\ then\\ t_2\\ else\\ t_3 \\longrightarrow if\\ t_1'\\ then\\ t_2\\ else\\ t_3                \\end{array} \\\\ \\\\ {\\tt (ifT)} &amp;  if\\ true\\ then\\ t_2\\ else\\ t_3 \\longrightarrow t_2 \\\\ \\\\ {\\tt (ifF)} &amp;  if\\ false\\ then\\ t_2\\ else\\ t_3 \\longrightarrow t_3 \\\\ \\\\ {\\tt (OpI1)} &amp; \\begin{array}{c}                 t_1 \\longrightarrow t_1' \\\\                 \\hline                 t_1\\ op\\ t_2\\  \\longrightarrow t_1'\\ op\\ t_2                 \\end{array} \\\\ \\\\ {\\tt (OpI2)} &amp; \\begin{array}{c}                 t_2 \\longrightarrow t_2' \\\\                 \\hline                 c_1\\ op\\ t_2\\  \\longrightarrow c_1\\ op\\ t_2'                 \\end{array} \\\\ \\\\ {\\tt (OpC)} &amp;  \\begin{array}{c}                 invoke\\ low\\ level\\ call\\  op(c_1, c_2) = c_3 \\\\                 \\hline                   c_1\\ op\\ c_2\\  \\longrightarrow c_3                 \\end{array} \\\\ \\\\                 ... \\end{array} \\] <p>In the above we use a horizontal line to separate complex deduction rules that have some premise. The relations and statement written above the horizontal line are called the premises, and the relation the written below is called the conclusion. The conclusion holds if the premises are valid.</p> <ul> <li>The rule \\({\\tt (ifI)}\\) states that if we can evaluate  \\(t_1\\) to  \\(t_1'\\), then  \\(if\\ t_1\\ then\\ t_2\\ else\\ t_3\\) can be evaluated to  \\(if\\ t_1' \\ then\\ t_2\\ else\\ t_3\\). In otherwords, for us to reduce \\(if\\ t_1\\ then\\ t_2\\ else\\ t_3\\) to \\(if\\ t_1' \\ then\\ t_2\\ else\\ t_3\\), a pre-condition is to reduce \\(t_1\\) to \\(t_1'\\). </li> <li>The rule  \\({\\tt (ifT)}\\) states that if the conditional expression is \\(true\\), the entire term is evaluated to the then-branch.</li> <li>The rule  \\({\\tt (ifF)}\\) is similar. Rules  \\({\\tt (OpI1)}\\) and ${\\tt (OpI2)} $ are similar to rule \\({\\tt (IfI)}\\).</li> <li>The rule  \\({\\tt (OpC)}\\) invokes the built-in low level call to apply the binary operation to the two operands  \\(c_1\\) and  \\(c_2\\).  </li> </ul> <p>The substitution rules and free variable function \\(fv()\\) are also extended too</p> \\[ \\begin{array}{rcll}  \\lbrack t_1 / x \\rbrack c &amp; = &amp; c \\\\     \\lbrack t_1 / x \\rbrack t_2\\ op\\ t_3 &amp; = &amp; (\\lbrack t_1 / x \\rbrack t_2)\\ op\\ (\\lbrack t_1 / x \\rbrack t_3) \\\\    \\lbrack t_1 / x \\rbrack if\\ t_2\\ then\\ t_3\\ else\\ t_4 &amp; = &amp; if\\ \\lbrack t_1 / x \\rbrack t_2\\ then\\ \\lbrack t_1 / x \\rbrack t_3\\ else\\ \\lbrack t_1 / x \\rbrack t_4 \\\\  \\end{array} \\] \\[ \\begin{array}{rcl} fv(t_1\\ op\\ t_2) &amp; = &amp; fv(t_1) \\cup fv(t_2) \\\\  fv(if\\ t_1\\ then\\ t_2\\ else\\ t_3) &amp; = &amp; fv(t_1) \\cup fv(t_2) \\cup fv(t_3) \\\\ fv(c) &amp; = &amp; \\{\\} \\\\ \\end{array} \\] <p>Let's consider an example:</p> \\[ \\begin{array}{rl} (\\lambda x.if\\ x==0\\ then\\  0\\  else\\  10/x)\\ 2 &amp; \\longrightarrow_{\\scriptsize {\\tt \\beta}} \\\\ \\lbrack 2/x \\rbrack if\\ x==0\\ then\\  0\\  else\\  10/x &amp; \\longrightarrow_{\\scriptsize {\\tt (substitution)}} \\\\ if\\ 2==0\\ then\\  0\\  else\\  10/2 &amp; \\longrightarrow_{\\scriptsize {\\tt (IfI)}} \\\\ if\\ false\\ then\\ 0\\  else\\  10/2 &amp; \\longrightarrow_{\\scriptsize {\\tt (IfF)}} \\\\ 10/2 &amp; \\longrightarrow_{\\scriptsize {\\tt (OpC)}} \\\\ 5 \\end{array} \\]"},{"location":"notes/fp_intro/#approach-2-church-encoding","title":"Approach 2 - Church Encoding","text":"<p>Instead of extending the syntax and evaluation rules, we could encode the conditional expression in terms of the basic lambda terms.</p> <p>Thanks to Church-encoding (discovered by Alonzo Church), we can encode boolean data and if-then-else using Lambda Calculus.</p> <p>Let's define:</p> <ul> <li>\\(true\\) as \\(\\lambda x.\\lambda y.x\\)</li> <li>\\(false\\) as \\(\\lambda x.\\lambda y.y\\)</li> <li>\\(ite\\) (read as if-then-else) as \\(\\lambda e_1. \\lambda e_2. \\lambda e_3. e_1\\ e_2\\ e_3\\)</li> </ul> <p>We assume the function application is left associative, i.e. \\(e_1\\ e_2\\ e_3 \\equiv  (e_1\\ e_2)\\ e_3\\). For example,</p> \\[ \\begin{array}{rl} ite\\ true\\ w\\ z &amp; = \\\\ (\\lambda e_1. \\lambda e_2. \\lambda e_3. e_1\\ e_2\\ e_3)\\ true\\ w\\ z &amp; \\longrightarrow \\\\ true\\ w\\ z &amp; =  \\\\ (\\lambda x.\\lambda y.x)\\ w\\ z &amp; \\longrightarrow  \\\\ w \\end{array} \\]"},{"location":"notes/fp_intro/#recursion","title":"Recursion","text":"<p>To make our language turing complete, we need to support loops. The way to perform loops in lambda calculus is via recursion.</p> <p>Similar to the conditional expression, there are at least two ways of introducing recursion to our language.</p>"},{"location":"notes/fp_intro/#approach-1-extending-the-syntax-and-the-evaluation-rules_1","title":"Approach 1 - Extending the syntax and the evaluation rules","text":"<p>We extend the syntax with a mu-abstraction (\\(\\mu\\)):</p> \\[ \\begin{array}{rccl}  {\\tt (Lambda\\ Terms)} &amp; t &amp; ::= &amp; ... \\mid \\mu f.t \\end{array} \\] <p>and the evaluation rules:</p> \\[ \\begin{array}{rl} {\\tt (\\beta\\ reduction)} &amp; (\\lambda x.t_1)\\ t_2 \\longrightarrow [t_2/x] t_1 \\\\ {\\tt (NOR)} &amp; \\begin{array}{c}                 t_1 \\longrightarrow t_1' \\\\                 \\hline                 t_1\\ t_2 \\longrightarrow t_1'\\ t_2                 \\end{array} \\\\ {\\tt (unfold)} &amp; \\mu f.t \\longrightarrow [(\\mu f.t)/f] t  \\\\ \\end{array} \\] <p>Note that we include the  ${\\tt (NOR)} $ rule into our evaluation rules to fix the evaluation strategy, and we only reduce the redexes that are not inside a lambda abstraction, otherwise the program does not terminate.</p> <p>We include the following cases for the free variable function \\(fv()\\) and the substitution $$ \\begin{array}{rcl} fv(\\mu f.t) &amp; = &amp; fv(t) - {f} \\end{array} $$ and  $$ \\begin{array}{rcl}  \\lbrack t_1 / x \\rbrack \\mu f.t_2 &amp; = &amp; \\mu f.\\lbrack t_1 / x \\rbrack t_2 &amp; {\\tt if}  f\\neq x  {\\tt and}  f \\not \\in fv(t_1) \\end{array} $$</p> <p>For instance:</p> \\[ \\begin{array}{rl} (\\mu f.\\lambda x.if\\ x==1\\ then\\ 1\\ else\\ x*(f\\ (x-1)))\\ 3 &amp; \\longrightarrow_{\\scriptsize {\\tt(NOR)+(unfold)}} \\\\ (\\lbrack (\\mu f.\\lambda x.if\\ x==1\\ then\\ 1\\ else\\ x*(f\\ (x-1)))/f \\rbrack \\lambda x.if\\ x==1\\ then\\ 1\\ else\\ x*(f\\ (x-1)))\\ 3 &amp; \\longrightarrow_{\\scriptsize {\\tt (substitution) + (\\alpha)}} \\\\ (\\lambda x.if\\ x==1\\ then\\ 1\\ else\\ x*((\\mu f.\\lambda y.if\\ y==1\\ then\\ 1\\ else\\ f\\ (y-1))\\ (x-1)))\\ 3 &amp; \\longrightarrow_{\\scriptsize {\\tt (\\beta)}} \\\\ \\lbrack 3/x \\rbrack if\\ x==1\\ then\\ 1\\ else\\ x*((\\mu f.\\lambda y.if\\ y==1\\ then\\ 1\\ else\\ f\\ (y-1))\\ (x-1)) &amp; \\longrightarrow_{\\scriptsize {\\tt (substitution)}} \\\\ if\\ 3==1\\ then\\ 1\\ else\\ 3*((\\mu f.\\lambda y.if\\ y==1\\ then\\ 1\\ else\\ f\\ (y-1))\\ (3-1)) &amp; \\longrightarrow_{\\scriptsize {\\tt (ifI)+(OpC)}} \\\\ if\\ false\\ then\\ 1\\ else\\ 3*((\\mu f.\\lambda y.if\\ y==1\\ then\\ 1\\ else\\ f\\ (y-1))\\ (3-1)) &amp; \\longrightarrow_{\\scriptsize {\\tt (ifF)}} \\\\ 3*((\\mu f.\\lambda y.if\\ y==1\\ then\\ 1\\ else\\ f\\ (y-1))\\ (3-1)) &amp; \\longrightarrow_{\\scriptsize {\\tt (OpI2)}} \\\\ ... \\\\ 3*(2*1) \\end{array} \\] <p>Another important point to note is that the set of rewriting rules we have gathered so far \\({\\tt (\\beta-reduction)}\\), \\({\\tt (NOR)}\\), \\({\\tt (unfold)}\\), \\({\\tt (IfT)}\\), \\({\\tt (IfF)}\\), \\({\\tt (IfI)}\\), \\({\\tt (OpC)}\\), \\({\\tt (OpI2)}\\) and \\({\\tt (OpI1)}\\) are syntax-directed, i.e. the LHS of the \\(\\longrightarrow\\) in the conclusion, which is AKA the head of the rule is unique if we try to rules in this specific order. A clear advantage of this is that we can view this deduction rule system as an algorithm, i.e. an implementation that resembles this specification exists. We will see this in the later part of this course. </p>"},{"location":"notes/fp_intro/#approach-2-church-encoding_1","title":"Approach 2 - Church Encoding","text":"<p>Alternatively, recursion can be encoded using the fix-pointer combinator (AKA  $Y $-combinator). Let $Y $ be</p> \\[ \\lambda f.((\\lambda y. (f\\ (y\\ y)))~(\\lambda x.(f\\ (x\\ x)))) \\] <p>We find that for any function \\(g\\), we have \\(Y\\ g = g\\ (Y\\ g)\\).</p> <p>We will work on the derivation during exercise.</p> <p>Let's try to implement the factorial function over natural numbers:</p> \\[ \\begin{array}{cc}    fac(n) = \\left [          \\begin{array}{ll}             1 &amp;  {if}~ n = 0 \\\\             n*fac(n-1) &amp; {otherwise}          \\end{array} \\right . \\end{array} \\] <p>Our goal is to look for a fixpoint function \\(Fac\\) such that \\(Y\\ Fac \\longrightarrow Fac\\ (Y\\ Fac)\\) and \\(Y\\ Fac\\) implements the above definition.</p> <p>Let \\(Fac\\) be</p> \\[ \\begin{array}{c}  \\lambda fac. \\lambda n. ite\\ (iszero\\ n)\\ one\\ (mul\\ n\\ (fac\\ (pred\\ n))) \\end{array} \\] <p>where \\(iszero\\) tests whether a number is 0 in Church Encoding. \\(mul\\) multiplies two numbers. \\(pred\\) takes a number and return its predecessor in natural number order. Then \\(Y\\ Fac\\) will be the implementation of the factorial function described above.</p>"},{"location":"notes/fp_intro/#discussion-1","title":"Discussion 1","text":"<p>How to define the following?</p> <ul> <li>\\(one\\)</li> <li>\\(iszero\\)</li> <li>\\(mul\\)</li> <li>\\(pred\\)</li> </ul>"},{"location":"notes/fp_intro/#discussion-2","title":"Discussion 2","text":"<p>The current evaluation strategy presented resembles the call-by-need semantics, in which the function arguments are not evaluated until they are needed. What modification will be required if we want to implement a call-by-value semantics (AKA. strict evaluation)?</p> <p>We will work on the two topics discussed above during the cohort class.</p>"},{"location":"notes/fp_intro/#summary","title":"Summary","text":"<p>We have covered</p> <ul> <li>Syntax (lambda terms) and Semantics (\\(\\beta\\) reduction, substitution, \\(\\alpha\\) renaming).</li> <li>Evaluation strategies, their properties and connection to real world programming</li> <li>Extending lambda calculus to support conditional and loop</li> <li>Via language extension (we will use)</li> <li>Via Church encoding (fun but not very pragmatic in our context)</li> </ul>"},{"location":"notes/handout/","title":"50.054 Compiler Design and Program Analysis Course Handout","text":""},{"location":"notes/handout/#this-page-will-be-updated-regularly-sync-up-often","title":"This page will be updated regularly. Sync up often.","text":""},{"location":"notes/handout/#course-description","title":"Course Description","text":"<p>This course aims to introduce a new programming paradigm to the learners, Functional programming and the suite of advanced language features and design patterns for software design and development. By building on top of these techniques, the course curates the process of designing a modern compiler, through syntax analysis, intermediate presentation construction, semantic analysis and code generation. More specifically the course focuses on the application of program analysis in areas of program optimization and software security analysis. </p>"},{"location":"notes/handout/#module-learning-outcomes","title":"Module Learning Outcomes","text":"<p>By the end of this module, students are able to</p> <ol> <li>Implement software solutions using functional programming language and applying design patterns</li> <li>Define the essential components in a program compilation pipeline</li> <li>Design a compiler for an imperative programming language</li> <li>Optimise the generated machine codes by applying program analysis techniques</li> <li>Detect bugs and security flaws in software by applying program analysis techniques</li> </ol>"},{"location":"notes/handout/#measurable-outcomes","title":"Measurable Outcomes","text":"<ol> <li>Develop a parser for an imperative programming language with assignment, if-else and loop (AKA the source language) using Functional Programming</li> <li>Implement a type checker for the source language</li> <li>Develop a static analyser to eliminate dead codes</li> <li>Implement the register allocation algorithm in the target code generation module</li> <li>Develop a static analyser to identify potential security flaws in the source language</li> </ol>"},{"location":"notes/handout/#topics","title":"Topics","text":"<ol> <li>Functional Programming : Expression, Function, Conditional</li> <li>Functional Programming : List, Algebraic data type and Pattern Matching</li> <li>Functional Programming : Type class</li> <li>Functional Programming : Generic and Functor</li> <li>Functional Programming : Applicative and Monad</li> <li>Syntax analysis: Lexing</li> <li>Syntax analysis: Parsing (LL, LR, SLR)</li> <li>Syntax analysis: Parser Combinator</li> <li>Intermediate Representation: Pseudo-Assembly</li> <li>Intermediate Representation: SSA</li> <li>Semantic analysis: Dynamic Semantics</li> <li>Semantic analysis: Type checking</li> <li>Semantic analysis: Type Inference</li> <li>Semantic analysis: Sign analysis</li> <li>Semantic analysis: Liveness analysis </li> <li>Code Gen: Instruction selection</li> <li>Code Gen: Register allocation</li> <li>Memory Management</li> </ol>"},{"location":"notes/handout/#resource","title":"Resource","text":"<p>The main resources are lecture slides, tutorial sessions, and online documentations. There are no official textbooks. But the following are useful for reference and deeper understanding of some topics.</p> <ol> <li>Compilers: Principles, Techniques, and Tools by Alfred V. Aho, Monica S. Lam, Ravi Sethi, and Jeffrey D. Ullman</li> <li>Modern Compiler Implementation in ML by Andrew W. Appel</li> <li>Types and Programming Languages by Benjamin C. Pierce</li> <li>Static Program Analysis by Anders M\u00f8ller and Michael I. Schwartzbach</li> <li>Learn you a Haskell for Great Good! - <code>https://learnyouahaskell.com/</code></li> </ol>"},{"location":"notes/handout/#instructors","title":"Instructors","text":"<ul> <li>Kenny Lu (kenny_lu@sutd.edu.sg) <ul> <li>Office Hour: Wednesday 3:00-4:30pm (please send email to arrange)</li> </ul> </li> </ul>"},{"location":"notes/handout/#communication","title":"Communication","text":"<p>If you have course/assignment/project related questions, please post it on the dedicated MS teams channel.</p>"},{"location":"notes/handout/#assessment","title":"Assessment","text":"<ul> <li>Mid-term 10%</li> <li>Project 35%</li> <li>Homework 20%</li> <li>Final 30%</li> <li>Class Participation 5%</li> </ul>"},{"location":"notes/handout/#things-you-need-to-prepare","title":"Things you need to prepare","text":"<ul> <li>If you are using Windows 10 or Windows 11, please install ubuntu subsystems<ul> <li>Win10</li> <li>Win11</li> </ul> </li> <li>If you are using Linux, it should be perfect.</li> <li>If you are using Mac, please install homebrew.</li> <li>Install Haskell tools<ol> <li>Install ghcup &gt;= 0.1.30.0 <code>https://www.haskell.org/ghcup/</code></li> <li>Install ghc &gt;= 9.6.6 (via the <code>ghcup tui</code> command)</li> <li>Install cabal &gt;= 3.10.3.0 (via the <code>ghcup tui</code> command)</li> <li>(Optional) install hls &gt;= 2.7.0.0 (via the <code>ghcup tui</code> command)</li> <li>Install stack &gt;= 2.15.5 (via the <code>ghcup tui</code> command)</li> </ol> </li> <li>IDE: It's your choice, but VSCode works fine.<ol> <li>if you are using VSCode, it is recommended to install the \"Haskell\" extension (by Haskell).</li> </ol> </li> <li>You should try test your setup by attempting Homework 1 on your own. </li> </ul>"},{"location":"notes/handout/#schedule","title":"Schedule","text":"<p>Schedule</p>"},{"location":"notes/handout/#project","title":"Project","text":"<p>The aim of the project is to apply the techniques and concepts taught in this module to develop a simple compiler for the SIMP language. </p> <p>You may work as a team (up to max 3 members). Please register your team here.  </p> <ul> <li>Lab 1 (10%, Deadline - 17 Nov 2024 23:59)</li> <li>Lab 2 (10%, Deadline - 1  Dec 2024 23:59)</li> <li>Lab 3 (15%, Deadline - 15 Dec 2024 23:59)</li> </ul>"},{"location":"notes/handout/#submission-policy-and-plagiarism","title":"Submission Policy and Plagiarism","text":"<ol> <li>You will do the assignment/project on your own (own teams) and will not copy paste solutions from someone else.</li> <li>You will not post any solutions related to this course to a private/public repository that is accessible by the public/others.</li> <li>Students are allowed to have a private repository for their assignment which no one can access. </li> <li>For projects, students can only invite their partners as collaborators to a private repository.</li> <li>Failing to follow the Code of Honour will result in failing the course and/or being submitted to the University Disciplinary Committee. The consequences apply to both the person who shares their work and the person who copies the work.</li> </ol>"},{"location":"notes/handout/#make-up-and-alternative-assessment","title":"Make Up and Alternative Assessment","text":"<p>Make ups for Final exam will be administered when there is an official Leave of Absence from OSA. There will be only one make up. There will be no make-up if students miss the make up test. </p>"},{"location":"notes/introduction/","title":"50.054 - Introduction","text":""},{"location":"notes/introduction/#module-description","title":"Module Description","text":"<p>This course aims to introduce a new programming paradigm to the learners, Functional programming and the suite of advanced language features and design patterns for software design and development. By building on top of these techniques, the course curates the process of designing a modern compiler, through syntax analysis, intermediate presentation construction, semantic analysis and code generation. More specifically the course focuses on the application of program analysis in areas of software verification, program optimization and software security analysis.</p>"},{"location":"notes/introduction/#module-learning-objective","title":"Module Learning Objective","text":"<p>By the end of this module, you should be able to</p> <ul> <li>Comprehend and reason functional programming</li> <li>Develop functional program to solve real world problem</li> <li>Identify the major components in compiler development</li> <li>Explain and implement different techniques and algorithms used in compiler development</li> </ul>"},{"location":"notes/introduction/#what-is-compilation","title":"What is compilation?","text":"<p>Program compilation is a process of mapping a source program into a target program. The source program is often generated by programmers or some higher level design processes and its structure ressemble the design, the specification or the algorithm. The target program is meant to be executed in the target platform and is generated based on the specific target environment requirement, e.g. hardware requirement, code size requirement and etc.</p> <p>A compiler is a software system that manages the program compliation process.</p>"},{"location":"notes/introduction/#what-properties-a-good-compiler-should-posess","title":"What properties a good compiler should posess?","text":"<p>An ideal compiler should be:</p> <ol> <li>Correct. The produced target program should behave the same as the the source program.</li> <li>Reliable. Any errors that could arise in the program should be detected and reported before execution.</li> <li>Generating Efficient Code. The produced target program should be optimized and running efficiently in the target platform.</li> </ol> <p>Some optional properties,</p> <ol> <li>User friendly. The error report should be comprehensive.</li> <li>Intelligent. Helps to automate some of the repeatitive tasks.</li> <li>...</li> </ol>"},{"location":"notes/ir_pseudo_assembly/","title":"50.054 - Pseudo Assembly","text":""},{"location":"notes/ir_pseudo_assembly/#learning-outcomes","title":"Learning Outcomes","text":"<p>By the end of this lesson, you should be able to</p> <ol> <li>Describe the syntax of the source language SIMP.</li> <li>Describe the syntax of the intermediate representation language pseudo-assembly.</li> <li>Describe how pseudo-assembly program is executed.</li> <li>Apply Maximal Munch algorithms to generate a pseudo-assembly code from a given SIMP source code.</li> </ol>"},{"location":"notes/ir_pseudo_assembly/#recap-the-compiler-pipeline","title":"Recap the compiler pipeline","text":"<p>Recall the compiler pipeline</p> <pre><code>graph LR\nA[Lexing] --&gt;B[Parsing] --&gt; C[Semantic Analysis] --&gt; D[Optimization] --&gt; E[Target Code Generation]</code></pre> <ul> <li>Lexing</li> <li>Input: Source file in String</li> <li>Output: A sequence of valid tokens according to the language specification (grammar)</li> <li>Parsing</li> <li>Input: Output from the Lexer</li> <li>Output: A parse tree representing parsed result according to the parse derivation</li> </ul> <p>And recall that a parse tree can be considered the first intermediate representation (IR). However the parse tree is to close to the source level which makes it hard to be used for code generation.  For now let's fast forward to consider another IR which is closer to the target code, we refer to it as pseudo assembly. In this unit, we skip the semantic analysis and consider a direct translation from the source language (SIMP) to the pseudo assembly.</p>"},{"location":"notes/ir_pseudo_assembly/#the-simp-language","title":"The SIMP Language","text":"<p>We consider the syntax of SIMP as follows</p> \\[ \\begin{array}{rccl} (\\tt Statement) &amp; S &amp; ::= &amp; X = E ; \\mid return\\ X ; \\mid nop; \\mid if\\ E\\ \\{ \\overline{S} \\}\\ else\\ \\{ \\overline{S} \\} \\mid while\\ E\\ \\{ \\overline{S} \\} \\\\ (\\tt Expression) &amp; E &amp; ::= &amp; E\\ OP\\ E \\mid X \\mid C  \\mid (E) \\\\ (\\tt Statements) &amp; \\overline{S} &amp; ::= &amp; S \\mid S\\ \\overline{S} \\\\ (\\tt Operator) &amp; OP &amp; ::= &amp; + \\mid - \\mid * \\mid &lt;  \\mid == \\\\  (\\tt Constant) &amp; C &amp; ::= &amp; 0 \\mid 1 \\mid 2 \\mid ... \\mid true \\mid false \\\\  (\\tt Variable) &amp; X &amp; ::= &amp; a \\mid b \\mid c \\mid d \\mid ... \\end{array} \\] <p>For simplicity, we ignore functions and procedures. We assume a special variable \\(input\\) serving as the input argument to the program. We write \\(\\overline{S}\\) to denote a sequence of statements. \\(return\\) statement takes a variable instead of an expression. \\(nop\\) stands a \"no-op\" statement, which implies no action preformed. The rest of the syntax is very similar to Java and C except that the type annotations are omitted. </p> <p>For example (Example SIMP1)</p> <pre><code>x = input;\ns = 0;\nc = 0;\nwhile c &lt; x {\n    s = c + s;\n    c = c + 1;\n}\nreturn s;\n</code></pre>"},{"location":"notes/ir_pseudo_assembly/#pseudo-assembly","title":"Pseudo Assembly","text":"<p>We consider the Pseudo Assembly language as follows.</p> \\[ \\begin{array}{rccl} (\\tt Labeled\\ Instruction) &amp; li  &amp; ::= &amp; l : i \\\\  (\\tt Instruction)   &amp; i   &amp; ::= &amp; d \\leftarrow s \\mid d \\leftarrow s\\ op\\ s \\mid ret \\mid ifn\\ s\\ goto\\ l \\mid goto\\ l \\\\  (\\tt Labeled\\ Instructions)   &amp; lis   &amp; ::= &amp; li \\mid li\\ lis \\\\  (\\tt Operand)       &amp; d,s &amp; ::= &amp; r \\mid c \\mid t \\\\ (\\tt Temp\\ Var)      &amp; t   &amp; ::= &amp; x \\mid y \\mid ...  \\\\ (\\tt Label)         &amp; l   &amp; ::= &amp; 1 \\mid 2 \\mid ... \\\\ (\\tt Operator)      &amp; op  &amp; ::= &amp; + \\mid - \\mid &lt; \\mid == \\\\  (\\tt Constant)      &amp; c   &amp; ::= &amp; 0 \\mid 1 \\mid 2 \\mid ... \\\\  (\\tt Register)      &amp; r &amp;   ::= &amp; r_{ret} \\mid r_1 \\mid r_2 \\mid ...   \\end{array} \\] <p>where \\(li\\), a labeled instruction, is a label \\(l\\) associated with an instruction \\(i\\). For simplicity, we use positive integers as labels.  An instruction is either a move operation (moving value from source operand \\(s\\) to destination operatnd \\(d\\)), a binary move operation, a return instruction, a conditional jump instruction and a jump instruction. Some non-syntactical restriction exists, e.g. a constant can't be used in a destination position. In Psuedo Assembly, we use <code>0</code> to denote <code>false</code> and <code>1</code>  to denote <code>true</code>. \\(r_{ret}\\) is a special register for the return statement.</p> <p>Example (PA1) <pre><code>1: x &lt;- input\n2: s &lt;- 0\n3: c &lt;- 0\n4: t &lt;- c &lt; x \n5: ifn t goto 9\n6: s &lt;- c + s\n7: c &lt;- c + 1\n8: goto 4\n9: rret &lt;- s\n10: ret\n</code></pre></p>"},{"location":"notes/ir_pseudo_assembly/#informal-specification-of-pseudo-assembly","title":"Informal Specification of Pseudo Assembly","text":"<p>We assume that statements of a pseudo assembly program are stored in a list. There exists a mapping from labels to the corresponding instructions, When we execute a pseudo assembly program, we use a program counter to keep track of the current execution context (i.e. the current labeled instruction being considered) and use a set to keep track of the variable to value mapping. </p> <p>For example when we execute the above program with <code>input = 2</code></p> Program Counter Local  Memory Next Instr 1 {input: 2, x : 2} 2 2 {input: 2, x : 2, s : 0} 3 3 {input: 2, x : 2, s : 0, c : 0} 4 4 {input: 2, x : 2, s : 0, c : 0, t : 1} 5 5 {input: 2, x : 2, s : 0, c : 0, t : 1} 6 6 {input: 2, x : 2, s : 0, c : 0, t : 1} 7 7 {input: 2, x : 2, s : 0, c : 1, t : 1} 8 8 {input: 2, x : 2, s : 0, c : 1, t : 1} 4 4 {input: 2, x : 2, s : 0, c : 1, t : 1} 5 5 {input: 2, x : 2, s : 0, c : 1, t : 1} 6 6 {input: 2, x : 2, s : 1, c : 1, t : 1} 7 7 {input: 2, x : 2, s : 1, c : 2, t : 1} 8 8 {input: 2, x : 2, s : 1, c : 2, t : 1} 4 4 {input: 2, x : 2, s : 1, c : 2, t : 0} 5 5 {input: 2, x : 2, s : 1, c : 2, t : 0} 9 9 {input: 2, x : 2, s : 1, c : 2, t : 0, rret : 1} 10 10 {input: 2, x : 2, s : 1, c : 2, t : 0, rret : 1} - <p>For the time being, we use a table to illusrate the execution of an PA program. Each entry in the table has 3 fields, the program counter, the current local memory (mapping from operands to values), and the next intruction. Move and binary operations updates the local memory. For non-jump instructions, the next instruction is the current instruction label incremeented by 1. For goto, the next instruction is the one associated with the destination label. For conditional jump, the next instruction is dependent on the source operand's value. We study the formal specification of the up-coming lessons.</p>"},{"location":"notes/ir_pseudo_assembly/#maximal-munch-algorithm","title":"Maximal Munch Algorithm","text":"<p>To convert a SIMP program into the pseudo assembly, we could consider the Maximal Munch Algorithm which is described in terms of the set of deduction rules in the following. </p> \\[ \\begin{array}{rc} {\\tt (mAssign)} &amp; \\begin{array}{c}                 G_a(X)(E) \\vdash lis  \\\\                \\hline                G_s(X = E) \\vdash lis                \\end{array} \\\\  \\end{array}   \\] <p>In case we have an assignment statement \\(X = E\\), we call a helper function \\(G_a\\) to generate the Peudo Assembly (PA) labeled instructions.</p> \\[ \\begin{array}{rc} {\\tt (mReturn)} &amp; \\begin{array}{c}      G_a(r_{ret})(X) \\vdash lis \\ \\ l\\ {\\tt is\\ a\\ fresh\\ label} \\\\      \\hline      G_s(return\\ X) \\vdash lis + [ l: ret ]      \\end{array} \\end{array} \\] <p>In case we have a return statement \\(return\\ E\\), we make use of the same helper function \\(G_a\\) to generate the instructions of assigning \\(E\\) to the special register \\(r_{ret}\\). We then generate a new label \\(l\\), and append \\(l:ret\\) to the instructions.</p> \\[ \\begin{array}{rc} {\\tt (mSequence)} &amp; \\begin{array}{c}                 {\\tt for}\\ l \\in \\{1,n\\} ~~ G_s(S_l) \\vdash lis_l \\\\                \\hline                G_s(S_1;...;S_n) \\vdash lis_1 + ... +  lis_n                \\end{array}  \\end{array}   \\] <p>In case we have a sequence of statements, we apply \\(G_s\\) recurisvely to the individual statements in order, then we merge all the results by concatenation.</p> \\[ \\begin{array}{rl}      {\\tt (mIf)} &amp; \\begin{array}{c}                t\\ {\\tt is\\ a\\ fresh\\ var} \\\\                 G_a(t)(E) \\vdash lis_0 \\\\                l_{IfCondJ}\\ {\\tt is\\ a\\ fresh\\ label} \\\\                G_s(S_2) \\vdash lis_2 \\\\                 l_{EndThen}\\ {\\tt  is\\ a\\ fresh\\ label} \\\\                  l_{Else}\\ {\\tt is\\ the\\ next\\ label (w/o\\ incr)} \\\\                 G_s(S_3) \\vdash lis_3 \\\\                 l_{EndElse}\\ {\\tt is\\ a\\ fresh\\ label} \\\\                l_{EndIf}\\ {\\tt is\\ the\\ next\\ label\\ (w/o\\ incr)} \\\\                 lis_1 = [l_{IfCondJ}: ifn\\ t\\ goto\\ l_{Else} ] \\\\                 lis_2' = lis_2 + [l_{EndThen}:goto\\ l_{EndIf}] \\\\                 lis_3' = lis_3 + [l_{EndElse}:goto\\ l_{EndIf}] \\\\                 \\hline                  G_s(if\\ E\\ \\{S_2\\}\\ else\\ \\{S_3\\}) \\vdash lis_0 + lis_1 + lis_2' + lis_3'                                \\end{array} \\\\   \\end{array} \\] <p>In case we have a if-else statement, we  1. generate a fresh variable \\(t\\), and call \\(G_a(t)(E)\\) to convert the conditional expression into PA instructions. 2. generate a new label \\(l_{IfCondJ}\\) (conditional jump). 3. call \\(G_s(S_2)\\) to generate the PA instructions for the then branch. 4. generate a new label \\(l_{EndThen}\\) which is associated with the \"end-of-then-branch\" goto instruction. 5. peek into the label generator to find out what is the next upcoming label and refer to it as \\(l_{Else}\\).  6. call \\(G_s(S_3)\\) to generate the PA instructions for the else branch. 7. generate a new label \\(l_{EndElse}\\), which is associated with the \"end-of-else-branch\" goto instruction. (Note that we can assume the next instruction after this is the end of If, in case of nested if-else.) 8. peek into the label generator to find out what the next upcoming label is and refer to it as \\(l_{EndIf}\\) </p> \\[ \\begin{array}{rl}      {\\tt (mWhile)} &amp; \\begin{array}{c}                     l_{While}\\ {\\tt is\\ the\\ next\\ label\\ (w/o\\ incr)} \\\\                      t\\ {\\tt is\\ a\\ fresh\\ var} \\\\                          G_a(t)(E) \\vdash lis_0 \\\\                      l_{WhileCondJ}\\ {\\tt is\\ a\\ fresh\\ label} \\\\                      G_s(S) \\vdash lis_2\\\\                      l_{EndBody}\\ {\\tt is\\ a\\ fresh\\ label} \\\\                       l_{EndWhile}\\ {\\tt is\\ the\\ next\\ label\\ (w/o\\ incr)} \\\\                      lis_1 = [l_{WhileCondJ}: ifn\\ t\\ goto\\ l_{EndWhile}] \\\\                     lis_2' = lis_2 + [ l_{EndBody}: goto\\ l_{While} ] \\\\                     \\hline                     G_s(while\\ E\\ \\{S\\}) \\vdash lis_0 + lis_1 + lis_2'                            \\end{array} \\\\   \\end{array} \\] <p>In case we have a while statement, we  1. peek into the label generator to find out what is the next upcoming label and refer it as \\(l_{While}\\), which can be used later as the reference for the backward jump.  2. generate a fresh variable \\(t\\), and call \\(G_a(t)(E)\\) to convert the conditional expression into PA instructions. 3. generate a new label \\(l_{WhileCondJ}\\) (conditional jump). 4. call \\(G_s(S)\\) to generate the PA instructions for the body. 5. generate a new label \\(l_{EndBody}\\) which is associated with the \"end-of-loop-body\" goto instruction. 6. peek into the label generator to find out what is the next upcoming label and refer to it as \\(l_{EndWhile}\\). (Note that we can assume the next instruction after this is the end of While, in case of nested while) 7. peek into the label generator to find out what the next upcoming label is and refer to it as \\(l_{EndIf}\\) </p> <p>The above summarizes all cases of \\(G_s(S)\\). We now consider the sub algorithm, \\(G_a(d)(E)\\), it takes a destination operand and SIMP expression \\(E\\) and return a set of labeled instructions. </p> \\[ {\\tt (mNOp)} ~~ G_s(nop) \\vdash []  \\] <p>The case of \\(nop\\) statement is stratight-forward.</p> \\[ \\begin{array}{rc} {\\tt (mConst)} &amp; \\begin{array}{c}                l\\ {\\tt  is\\ a\\ fresh\\ label}\\\\ c = conv(C) \\\\                \\hline                G_a(X)(C) \\vdash [l : X \\leftarrow c]                 \\end{array} \\\\  \\end{array} \\] <p>In the above rule, given a SIMP variable \\(X\\) and a constant \\(C\\) we generate a labeled instruction \\(X \\leftarrow c\\). where \\(c\\) is the PA constant converted from SIMP's counter-part through the \\(conv()\\) function. </p> \\[ \\begin{array}{rcl} conv(true) &amp; = &amp;  1\\\\ conv(false) &amp; = &amp; 0\\\\ conv(C) &amp; =&amp;  C \\end{array} \\] <p>For simplicity, we omitted the conversion from the SIMP variable to the IR temp variable. </p> \\[ \\begin{array}{rc} {\\tt (mVar)} &amp; \\begin{array}{c}                l\\ {\\tt  is\\ a\\ fresh\\ label} \\\\                \\hline                G_a(X)(Y) \\vdash [l : X \\leftarrow Y]                 \\end{array} \\\\  \\end{array}   \\] <p>In the above rule, we generate labeled instruction for the case of assigning a SIMP variable \\(Y\\) to another variable \\(X\\). The treat is similar to the case of \\({\\tt (Const)}\\).</p> \\[  \\begin{array}{rc} {\\tt (mParen)} &amp; \\begin{array}{c}                G_a(X)(E) \\vdash lis                \\\\ \\hline                G_a(X)((E)) \\vdash lis                \\end{array} \\end{array} \\] <p>In the rule \\({\\tt (mParen)}\\), we generate the IR labeled instructions by calling the generation algorithm recursively with the inner expression.</p> \\[ \\begin{array}{rc} {\\tt (mOp)} &amp; \\begin{array}{c}                t_1\\ {\\tt is\\ a\\ fresh\\ var} \\\\                G_a(t_1)(E_1) \\vdash lis_1 \\\\                t_2\\ {\\tt is\\ a\\ fresh\\ var} \\\\                G_a(t_2)(E_2) \\vdash lis_2 \\\\                l\\ {\\tt  is\\ a\\ fresh\\ label} \\\\                \\hline                G_a(X)(E_1 OP E_2) \\vdash lis_1 + lis_2 + [l : X \\leftarrow t_1 OP t_2]                 \\end{array} \\\\  \\end{array}   \\] <p>The above rule handles the case where the RHS of the SIMP assignment statement is a binary operation \\(E_1\\ OP\\ E_2\\), we generate two temp variables \\(t_1\\) and \\(t_2\\), and apply the generation function recursively to \\(E_1\\) and \\(E_2\\). Finally we concatenate the results \\(lis_1\\) and \\(lis_2\\) with the binary move operation \\(X \\leftarrow t_1\\ OP\\ t_2\\).</p> <p>For example, given the source in Example SIMP1, we apply the above algorithm and observe the following derivation.</p> <ol> <li>Firstly we apply \\({\\tt (mSequence)}\\) rule to individual statement,</li> </ol> <pre><code>Gs(x = input; s = 0; c = 0;\nwhile c &lt; x {\n    s = c + s;\n    c = c + 1;\n}\nreturn s;) \n---&gt;\nGs(x = input) ; Gs( s = 0) ; Gs(c = 0) ; Gs( while c &lt; x  { s = c + s; c = c + 1;}) ; Gs(return s);\n</code></pre> <p>The derivation for <code>Gs(x = input)</code> is trivial, we apply \\({\\tt (mAssign)}\\) rule.  <pre><code>Gs(x = input) \n---&gt; # using (mAssign) rule\nGa(x)(input)\n---&gt; # using (mVar) rule\n---&gt; [ 1: x &lt;- input ] \n</code></pre> Similarly we generate </p> <pre><code>Gs( s = 0)\n---&gt; # using (mAssign) rule\nGa(s)(0)\n---&gt; # using (mConst) rule\n---&gt; [ 2: s &lt;- 0 ] \n</code></pre> <p>and </p> <pre><code>Gs(c = 0)\n---&gt; # using (mAssign) rule\nGa(c)(0)\n---&gt; # using (mConst) rule\n---&gt; [ 3: c &lt;- 0 ] \n</code></pre> <ol> <li>Next we consider the while statement</li> </ol> <pre><code>Gs(\nwhile c &lt; x {\n    s = c + s;\n    c = c + 1;\n}\n)\n---&gt; # using (mWhile) rule\n  # the condition exp\n  t is a fresh var\n  Ga(t)(c&lt;x) ---&gt; # using (mOp) rule\n     t1 is a fresh var\n     Ga(t1)(x) ---&gt; [4: t1 &lt;- x]\n     t2 is a fresh var \n     Ga(t2)(c) ---&gt; [5: t2 &lt;- c]\n  ---&gt; [4: t1 &lt;- x, 5: t2 &lt;-c, 6: t &lt;- t1 &lt; t2 ]\n  # the conditional jump, we generate a new label 7 reserved for whilecondjump\n  # the while loop body\n  Gs[ s = c + s; c = c + 1]\n  ---&gt; # (mSequence), (mOp) and (mOp) rules\n  [ 8: t3 &lt;- c, 9: t4 &lt;- s, 10: t5 &lt;- t3 + t4,  11: t6 &lt;- c, 12: t7 &lt;- 1, 13: t8 &lt;- t6 + t7 ]\n  # end of the while loop\n  [ 14: goto 4 ]\n  # the conditional jump \n  ---&gt; [7: ifn t goto 15 ]\n---&gt;  # putting altogther\n[4: t1 &lt;- x, 5: t2 &lt;- c, 6:  t &lt;- t1 &lt; t2,   7: ifn t goto 15, \n 8: t3 &lt;- c, 9: t4 &lt;- s, 10: t5 &lt;- t3 + t4,  11: t6 &lt;- c, \n 12: t7 &lt;- 1, 13: t8 &lt;- t6 + t7, 14: goto 4] \n</code></pre> <ol> <li>Finally we convert the return statement <pre><code>Gs(return s)\n---&gt; # (mReturn) rule\n[15: r_ret &lt;- s, 16: ret]\n</code></pre></li> </ol> <p>Putting 1,2,3 together</p> <pre><code>1: x &lt;- input\n2: s &lt;- 0\n3: c &lt;- 0\n4: t1 &lt;- x\n5: t2 &lt;- c\n6: t &lt;- t1 &lt; t2\n7: ifn t goto 15 \n8: t3 &lt;- c\n9: t4 &lt;- s\n10: t5 &lt;- t3 + t4\n11: t6 &lt;- c \n12: t7 &lt;- 1 \n13: t8 &lt;- t6 + t7\n14: goto 4\n15: rret &lt;- s\n16: ret\n</code></pre> <p>As we observe, we don't quite get the exact output as example PA1. The main reason is that we generate extra steps thanks to the \\({\\tt (mOp)}\\) rule, (in which each operand of the binary operator takes up a new instruction).</p>"},{"location":"notes/ir_pseudo_assembly/#maximal-munch-algorithm-v2","title":"Maximal Munch Algorithm V2","text":"<p>Since the \\({\\tt (mOp)}\\) rule is the culprit of causing extra move steps generated in the IR.</p> <p>We consider a variant the Maximal Munch Algorithm. Instead of using \\(G_a(X)(E)\\) to generate  labeled instructions \\(lis\\) , we use a different sub system \\(G_e(E)\\) to generate a pair of results, \\(\\hat{e}, \\check{e}\\). where \\(\\check{e}\\) is a sequence of label instructions generated from \\(E\\) and \\(\\hat{e}\\) is the \"result\" operand storing the final result of \\(\\check{e}\\).</p> <p>The adjusted \\({\\tt (mConst)}\\), \\({\\tt (mVar)}\\) and \\({\\tt (mOp)}\\) rules are as follows,</p> \\[  \\begin{array}{rc} {\\tt (m2Const)} &amp; \\begin{array}{c}            G_e(C) \\vdash (conv(C), [])            \\end{array}  \\end{array}   \\] \\[ \\begin{array}{rc} {\\tt (m2Var)} &amp; \\begin{array}{c}            G_e(Y) \\vdash (Y, [])       \\end{array}  \\end{array}   \\] <p>The rules \\({\\tt (m2Const)}\\) and \\({\\tt (m2Var)}\\) are simple. We just return the constant (variable) as the \\(\\hat{e}\\) with an empty set of label instructions.</p> \\[ \\begin{array}{rc} {\\tt (m2Paren)} &amp; \\begin{array}{c}            G_e(E) \\vdash (\\hat{e}, \\check{e})           \\\\ \\hline           G_e((E)) \\vdash (\\hat{e}, \\check{e})      \\end{array}  \\end{array}   \\] <p>In the rule \\({\\tt (m2Paren)}\\), we generate the results by recursivelly applying the algorithm to the inner expression.</p> \\[ \\begin{array}{rc} {\\tt (m2Op)} &amp; \\begin{array}{c}            G_e(E_1) \\vdash (\\hat{e}_1, \\check{e}_1) \\\\            G_e(E_2) \\vdash (\\hat{e}_2, \\check{e}_2) \\\\            t \\ {\\tt is\\ a\\ fresh\\ variable.} \\\\            l \\ {\\tt is\\ a\\ fresh\\ label.} \\\\            \\hline           G_e(E_1 OP E_2) \\vdash (t, \\check{e}_1 + \\check{e}_2 + [l : t \\leftarrow \\hat{e}_1 OP \\hat{e}_2])            \\end{array} \\\\  \\end{array}   \\] <p>In the \\({\\tt (m2Op)}\\) rule, we call \\(G_e(\\cdot)\\) recursively to generate the  results for \\(E_1\\) and \\(E_2\\), namely \\((\\hat{e}_1, \\check{e}_1)\\) and \\((\\hat{e}_2, \\check{e}_2)\\).  We then use them to synthesis the final output. </p> <p>The \\(G_s(S)\\) rules are now calling \\(G_e(E)\\) instead of \\(G_a(X)(E)\\). </p> \\[ \\begin{array}{rc} {\\tt (m2Assign)} &amp; \\begin{array}{c}       G_e(E) \\vdash (\\hat{e}, \\check{e})  \\ \\       l\\ {\\tt is\\ a\\ fresh\\ label.} \\\\       \\hline      G_s(X = E) \\vdash \\check{e} + [ l : X \\leftarrow \\hat{e}]      \\end{array} \\\\  \\end{array}   \\] \\[ \\begin{array}{rc} {\\tt (m2Return)} &amp; \\begin{array}{c}      G_s(return\\ X) \\vdash \\check{e} + [ l_1 : r_{ret} \\leftarrow X,  l_2: ret ]      \\end{array}  \\end{array} \\] \\[ \\begin{array}{rl} {\\tt (m2If)} &amp; \\begin{array}{c}           G_e(E) \\vdash (\\hat{e}, \\check{e}) \\\\            l_{IfCondJ}\\ {\\tt is\\ a\\ fresh\\ label} \\\\           G_s(S_2) \\vdash lis_2 \\\\            l_{EndThen}\\ {\\tt  is\\ a\\ fresh\\ label} \\\\             l_{Else}\\ {\\tt is\\ the\\ next\\ label (w/o\\ incr)} \\\\            G_s(S_3) \\vdash lis_3 \\\\            l_{EndElse}\\ {\\tt is\\ a\\ fresh\\ label} \\\\           l_{EndIf}\\ {\\tt is\\ the\\ next\\ label\\ (w/o\\ incr)} \\\\            lis_1 = [l_{IfCondJ}: ifn\\ \\hat{e}\\ goto\\ l_{Else} ] \\\\            lis_2' = lis_2 + [l_{EndThen}:goto\\ l_{EndIf}] \\\\            lis_3' = lis_3 + [l_{EndElse}:goto\\ l_{EndIf}] \\\\            \\hline             G_s(if\\ E\\ \\{S_2\\}\\ else\\ \\{S_3\\}) \\vdash \\check{e} + lis_1 + lis_2' + lis_3'                          \\end{array}  \\end{array} \\] \\[ \\begin{array}{rl} {\\tt (m2While)} &amp; \\begin{array}{c}           l_{While}\\ {\\tt is\\ the\\ next\\ label\\ (w/o\\ incr)} \\\\            G_e(E) \\vdash (\\hat{e}, \\check{e}) \\\\            l_{WhileCondJ}\\ {\\tt is\\ a\\ fresh\\ label} \\\\            G_s(S) \\vdash lis_2\\\\            l_{EndBody}\\ {\\tt is\\ a\\ fresh\\ label} \\\\             l_{EndWhile}\\ {\\tt is\\ the\\ next\\ label\\ (w/o\\ incr)} \\\\            lis_1 = [l_{WhileCondJ}: ifn\\ \\hat{e}\\ goto\\ l_{EndWhile}] \\\\           lis_2' = lis_2 + [ l_{EndBody}: goto\\ l_{While} ] \\\\           \\hline           G_s(while\\ E\\ \\{S\\}) \\vdash  \\check{e} + lis_1 + lis_2'                      \\end{array}  \\end{array} \\] <p>By comparing this version with the first one, we note that we reduce the number of labels as well as the numbers of temp variables being created througout the conversion from SIMP to PA. </p> <p>For example, if we apply the optimized version of Maximal Munch to the example SIMP1, we should obtain example PA1 as result.</p>"},{"location":"notes/liveness_analysis/","title":"50.054 - Liveness Analysis","text":""},{"location":"notes/liveness_analysis/#learning-outcomes","title":"Learning Outcomes","text":"<ol> <li>Define the liveness analysis problem</li> <li>Apply lattice and fixed point algorithm to solve the liveness analysis problem</li> </ol>"},{"location":"notes/liveness_analysis/#recall","title":"Recall","text":"<p><pre><code>// SIMP1\nx = input;\ny = 0;\ns = 0;\nwhile (y &lt; x) { \n    y = y + 1;\n    t = s;  // t is not used.\n    s = s + y;  \n}\nreturn s;\n</code></pre> In the above program the statement <code>t = s</code> is redundant as <code>t</code> is not used.</p> <p>It can be statically detected by a liveness analysis. </p>"},{"location":"notes/liveness_analysis/#liveness-analysis","title":"Liveness Analysis","text":"<p>A variable is consideredd live at a program location \\(v\\) if it may be used in another program location \\(u\\) if we follow the execution order, i.e. in the control flow graph there exists a path from \\(v\\) to \\(u\\). Otherwise, the variable is considered not live or dead.  Note that from this analysis a variable is detected to be live, it is actually \"maybe-live\" since we are using a conservative approximation via lattice theory. On the hand, the negation, i.e. dead is definite.</p> <p>By applying this analysis to the above program, we can find out at the program locations where variables must be dead.</p>"},{"location":"notes/liveness_analysis/#defining-the-lattice-for-livenesss-analysis","title":"Defining the Lattice for Livenesss Analysis","text":"<p>Recall from the previous lesson, we learned that if \\(A\\) be a set, then \\(({\\cal P}(A), \\subseteq)\\) forms a complete lattice, where \\({\\cal P}(A)\\) the power set of \\(A\\).</p> <p>Applying this approach the liveness analysis, we consider the powerset  the set of all variables in the program.</p> <p>Let's recast the <code>SIMP1</code> program into pseudo assembly, let's label it as <code>PA1</code></p> <pre><code>1: x &lt;- input\n2: y &lt;- 0\n3: s &lt;- 0\n4: b &lt;- y &lt; x\n5: ifn b goto 10\n6: y &lt;- y + 1\n7: t &lt;- s\n8: s &lt;- s + y\n9: goto 4\n10: rret &lt;- s\n11: ret\n</code></pre> <p>In <code>PA1</code> we find the set of variables \\(V = \\{input, x, y, s, t, b\\}\\), if we construct a powerset lattice \\(({\\cal P(V)}, \\subseteq)\\), we see the following hasse diagram</p> <pre><code>graph TD;\n    N58[\"{b}\"] --- N64[\"{}\"] \n    N59[\"{t}\"] --- N64[\"{}\"] \n    N60[\"{s}\"] --- N64[\"{}\"] \n    N61[\"{y}\"] --- N64[\"{}\"] \n    N62[\"{x}\"] --- N64[\"{}\"] \n    N63[\"{input}\"] --- N64[\"{}\"] \n    N43[\"{t,b}\"] --- N58[\"{b}\"] \n    N44[\"{s,b}\"] --- N58[\"{b}\"] \n    N46[\"{y,b}\"] --- N58[\"{b}\"] \n    N49[\"{x,b}\"] --- N58[\"{b}\"] \n    N53[\"{input,b}\"] --- N58[\"{b}\"] \n    N43[\"{t,b}\"] --- N59[\"{t}\"] \n    N45[\"{s,t}\"] --- N59[\"{t}\"] \n    N47[\"{y,t}\"] --- N59[\"{t}\"] \n    N50[\"{x,t}\"] --- N59[\"{t}\"] \n    N54[\"{input,t}\"] --- N59[\"{t}\"] \n    N44[\"{s,b}\"] --- N60[\"{s}\"] \n    N45[\"{s,t}\"] --- N60[\"{s}\"] \n    N48[\"{y,s}\"] --- N60[\"{s}\"] \n    N51[\"{x,s}\"] --- N60[\"{s}\"] \n    N55[\"{input,s}\"] --- N60[\"{s}\"] \n    N46[\"{y,b}\"] --- N61[\"{y}\"] \n    N47[\"{y,t}\"] --- N61[\"{y}\"] \n    N48[\"{y,s}\"] --- N61[\"{y}\"] \n    N52[\"{x,y}\"] --- N61[\"{y}\"] \n    N56[\"{input,y}\"] --- N61[\"{y}\"] \n    N49[\"{x,b}\"] --- N62[\"{x}\"] \n    N50[\"{x,t}\"] --- N62[\"{x}\"] \n    N51[\"{x,s}\"] --- N62[\"{x}\"] \n    N52[\"{x,y}\"] --- N62[\"{x}\"] \n    N57[\"{input,x}\"] --- N62[\"{x}\"] \n    N53[\"{input,b}\"] --- N63[\"{input}\"] \n    N54[\"{input,t}\"] --- N63[\"{input}\"] \n    N55[\"{input,s}\"] --- N63[\"{input}\"] \n    N56[\"{input,y}\"] --- N63[\"{input}\"] \n    N57[\"{input,x}\"] --- N63[\"{input}\"] \n    N23[\"{s,t,b}\"] --- N43[\"{t,b}\"] \n    N24[\"{y,t,b}\"] --- N43[\"{t,b}\"] \n    N27[\"{x,t,b}\"] --- N43[\"{t,b}\"] \n    N33[\"{input,t,b}\"] --- N43[\"{t,b}\"] \n    N23[\"{s,t,b}\"] --- N44[\"{s,b}\"] \n    N25[\"{y,s,b}\"] --- N44[\"{s,b}\"] \n    N28[\"{x,s,b}\"] --- N44[\"{s,b}\"] \n    N34[\"{input,s,b}\"] --- N44[\"{s,b}\"] \n    N23[\"{s,t,b}\"] --- N45[\"{s,t}\"] \n    N26[\"{y,s,t}\"] --- N45[\"{s,t}\"] \n    N29[\"{x,s,t}\"] --- N45[\"{s,t}\"] \n    N35[\"{input,s,t}\"] --- N45[\"{s,t}\"] \n    N24[\"{y,t,b}\"] --- N46[\"{y,b}\"] \n    N25[\"{y,s,b}\"] --- N46[\"{y,b}\"] \n    N30[\"{x,y,b}\"] --- N46[\"{y,b}\"] \n    N36[\"{input,y,b}\"] --- N46[\"{y,b}\"] \n    N24[\"{y,t,b}\"] --- N47[\"{y,t}\"] \n    N26[\"{y,s,t}\"] --- N47[\"{y,t}\"] \n    N31[\"{x,y,t}\"] --- N47[\"{y,t}\"] \n    N37[\"{input,y,t}\"] --- N47[\"{y,t}\"] \n    N25[\"{y,s,b}\"] --- N48[\"{y,s}\"] \n    N26[\"{y,s,t}\"] --- N48[\"{y,s}\"] \n    N32[\"{x,y,s}\"] --- N48[\"{y,s}\"] \n    N38[\"{input,y,s}\"] --- N48[\"{y,s}\"] \n    N27[\"{x,t,b}\"] --- N49[\"{x,b}\"] \n    N28[\"{x,s,b}\"] --- N49[\"{x,b}\"] \n    N30[\"{x,y,b}\"] --- N49[\"{x,b}\"] \n    N39[\"{input,x,b}\"] --- N49[\"{x,b}\"] \n    N27[\"{x,t,b}\"] --- N50[\"{x,t}\"] \n    N29[\"{x,s,t}\"] --- N50[\"{x,t}\"] \n    N31[\"{x,y,t}\"] --- N50[\"{x,t}\"] \n    N40[\"{input,x,t}\"] --- N50[\"{x,t}\"] \n    N28[\"{x,s,b}\"] --- N51[\"{x,s}\"] \n    N29[\"{x,s,t}\"] --- N51[\"{x,s}\"] \n    N32[\"{x,y,s}\"] --- N51[\"{x,s}\"] \n    N41[\"{input,x,s}\"] --- N51[\"{x,s}\"] \n    N30[\"{x,y,b}\"] --- N52[\"{x,y}\"] \n    N31[\"{x,y,t}\"] --- N52[\"{x,y}\"] \n    N32[\"{x,y,s}\"] --- N52[\"{x,y}\"] \n    N42[\"{input,x,y}\"] --- N52[\"{x,y}\"] \n    N33[\"{input,t,b}\"] --- N53[\"{input,b}\"] \n    N34[\"{input,s,b}\"] --- N53[\"{input,b}\"] \n    N36[\"{input,y,b}\"] --- N53[\"{input,b}\"] \n    N39[\"{input,x,b}\"] --- N53[\"{input,b}\"] \n    N33[\"{input,t,b}\"] --- N54[\"{input,t}\"] \n    N35[\"{input,s,t}\"] --- N54[\"{input,t}\"] \n    N37[\"{input,y,t}\"] --- N54[\"{input,t}\"] \n    N40[\"{input,x,t}\"] --- N54[\"{input,t}\"] \n    N34[\"{input,s,b}\"] --- N55[\"{input,s}\"] \n    N35[\"{input,s,t}\"] --- N55[\"{input,s}\"] \n    N38[\"{input,y,s}\"] --- N55[\"{input,s}\"] \n    N41[\"{input,x,s}\"] --- N55[\"{input,s}\"] \n    N36[\"{input,y,b}\"] --- N56[\"{input,y}\"] \n    N37[\"{input,y,t}\"] --- N56[\"{input,y}\"] \n    N38[\"{input,y,s}\"] --- N56[\"{input,y}\"] \n    N42[\"{input,x,y}\"] --- N56[\"{input,y}\"] \n    N39[\"{input,x,b}\"] --- N57[\"{input,x}\"] \n    N40[\"{input,x,t}\"] --- N57[\"{input,x}\"] \n    N41[\"{input,x,s}\"] --- N57[\"{input,x}\"] \n    N42[\"{input,x,y}\"] --- N57[\"{input,x}\"] \n    N8[\"{y,s,t,b}\"] --- N23[\"{s,t,b}\"] \n    N9[\"{x,s,t,b}\"] --- N23[\"{s,t,b}\"] \n    N13[\"{input,s,t,b}\"] --- N23[\"{s,t,b}\"] \n    N8[\"{y,s,t,b}\"] --- N24[\"{y,t,b}\"] \n    N10[\"{x,y,t,b}\"] --- N24[\"{y,t,b}\"] \n    N14[\"{input,y,t,b}\"] --- N24[\"{y,t,b}\"] \n    N8[\"{y,s,t,b}\"] --- N25[\"{y,s,b}\"] \n    N11[\"{x,y,s,b}\"] --- N25[\"{y,s,b}\"] \n    N15[\"{input,y,s,b}\"] --- N25[\"{y,s,b}\"] \n    N8[\"{y,s,t,b}\"] --- N26[\"{y,s,t}\"] \n    N12[\"{x,y,s,t}\"] --- N26[\"{y,s,t}\"] \n    N16[\"{input,y,s,t}\"] --- N26[\"{y,s,t}\"] \n    N9[\"{x,s,t,b}\"] --- N27[\"{x,t,b}\"] \n    N10[\"{x,y,t,b}\"] --- N27[\"{x,t,b}\"] \n    N17[\"{input,x,t,b}\"] --- N27[\"{x,t,b}\"] \n    N9[\"{x,s,t,b}\"] --- N28[\"{x,s,b}\"] \n    N11[\"{x,y,s,b}\"] --- N28[\"{x,s,b}\"] \n    N18[\"{input,x,s,b}\"] --- N28[\"{x,s,b}\"] \n    N9[\"{x,s,t,b}\"] --- N29[\"{x,s,t}\"] \n    N12[\"{x,y,s,t}\"] --- N29[\"{x,s,t}\"] \n    N19[\"{input,x,s,t}\"] --- N29[\"{x,s,t}\"] \n    N10[\"{x,y,t,b}\"] --- N30[\"{x,y,b}\"] \n    N11[\"{x,y,s,b}\"] --- N30[\"{x,y,b}\"] \n    N20[\"{input,x,y,b}\"] --- N30[\"{x,y,b}\"] \n    N10[\"{x,y,t,b}\"] --- N31[\"{x,y,t}\"] \n    N12[\"{x,y,s,t}\"] --- N31[\"{x,y,t}\"] \n    N21[\"{input,x,y,t}\"] --- N31[\"{x,y,t}\"] \n    N11[\"{x,y,s,b}\"] --- N32[\"{x,y,s}\"] \n    N12[\"{x,y,s,t}\"] --- N32[\"{x,y,s}\"] \n    N22[\"{input,x,y,s}\"] --- N32[\"{x,y,s}\"] \n    N13[\"{input,s,t,b}\"] --- N33[\"{input,t,b}\"] \n    N14[\"{input,y,t,b}\"] --- N33[\"{input,t,b}\"] \n    N17[\"{input,x,t,b}\"] --- N33[\"{input,t,b}\"] \n    N13[\"{input,s,t,b}\"] --- N34[\"{input,s,b}\"] \n    N15[\"{input,y,s,b}\"] --- N34[\"{input,s,b}\"] \n    N18[\"{input,x,s,b}\"] --- N34[\"{input,s,b}\"] \n    N13[\"{input,s,t,b}\"] --- N35[\"{input,s,t}\"] \n    N16[\"{input,y,s,t}\"] --- N35[\"{input,s,t}\"] \n    N19[\"{input,x,s,t}\"] --- N35[\"{input,s,t}\"] \n    N14[\"{input,y,t,b}\"] --- N36[\"{input,y,b}\"] \n    N15[\"{input,y,s,b}\"] --- N36[\"{input,y,b}\"] \n    N20[\"{input,x,y,b}\"] --- N36[\"{input,y,b}\"] \n    N14[\"{input,y,t,b}\"] --- N37[\"{input,y,t}\"] \n    N16[\"{input,y,s,t}\"] --- N37[\"{input,y,t}\"] \n    N21[\"{input,x,y,t}\"] --- N37[\"{input,y,t}\"] \n    N15[\"{input,y,s,b}\"] --- N38[\"{input,y,s}\"] \n    N16[\"{input,y,s,t}\"] --- N38[\"{input,y,s}\"] \n    N22[\"{input,x,y,s}\"] --- N38[\"{input,y,s}\"] \n    N17[\"{input,x,t,b}\"] --- N39[\"{input,x,b}\"] \n    N18[\"{input,x,s,b}\"] --- N39[\"{input,x,b}\"] \n    N20[\"{input,x,y,b}\"] --- N39[\"{input,x,b}\"] \n    N17[\"{input,x,t,b}\"] --- N40[\"{input,x,t}\"] \n    N19[\"{input,x,s,t}\"] --- N40[\"{input,x,t}\"] \n    N21[\"{input,x,y,t}\"] --- N40[\"{input,x,t}\"] \n    N18[\"{input,x,s,b}\"] --- N41[\"{input,x,s}\"] \n    N19[\"{input,x,s,t}\"] --- N41[\"{input,x,s}\"] \n    N22[\"{input,x,y,s}\"] --- N41[\"{input,x,s}\"] \n    N20[\"{input,x,y,b}\"] --- N42[\"{input,x,y}\"] \n    N21[\"{input,x,y,t}\"] --- N42[\"{input,x,y}\"] \n    N22[\"{input,x,y,s}\"] --- N42[\"{input,x,y}\"] \n    N2[\"{x,y,s,t,b}\"] --- N8[\"{y,s,t,b}\"] \n    N3[\"{input,y,s,t,b}\"] --- N8[\"{y,s,t,b}\"] \n    N2[\"{x,y,s,t,b}\"] --- N9[\"{x,s,t,b}\"] \n    N4[\"{input,x,s,t,b}\"] --- N9[\"{x,s,t,b}\"] \n    N2[\"{x,y,s,t,b}\"] --- N10[\"{x,y,t,b}\"] \n    N5[\"{input,x,y,t,b}\"] --- N10[\"{x,y,t,b}\"] \n    N2[\"{x,y,s,t,b}\"] --- N11[\"{x,y,s,b}\"] \n    N6[\"{input,x,y,s,b}\"] --- N11[\"{x,y,s,b}\"] \n    N2[\"{x,y,s,t,b}\"] --- N12[\"{x,y,s,t}\"] \n    N7[\"{input,x,y,s,t}\"] --- N12[\"{x,y,s,t}\"] \n    N3[\"{input,y,s,t,b}\"] --- N13[\"{input,s,t,b}\"] \n    N4[\"{input,x,s,t,b}\"] --- N13[\"{input,s,t,b}\"] \n    N3[\"{input,y,s,t,b}\"] --- N14[\"{input,y,t,b}\"] \n    N5[\"{input,x,y,t,b}\"] --- N14[\"{input,y,t,b}\"] \n    N3[\"{input,y,s,t,b}\"] --- N15[\"{input,y,s,b}\"] \n    N6[\"{input,x,y,s,b}\"] --- N15[\"{input,y,s,b}\"] \n    N3[\"{input,y,s,t,b}\"] --- N16[\"{input,y,s,t}\"] \n    N7[\"{input,x,y,s,t}\"] --- N16[\"{input,y,s,t}\"] \n    N4[\"{input,x,s,t,b}\"] --- N17[\"{input,x,t,b}\"] \n    N5[\"{input,x,y,t,b}\"] --- N17[\"{input,x,t,b}\"] \n    N4[\"{input,x,s,t,b}\"] --- N18[\"{input,x,s,b}\"] \n    N6[\"{input,x,y,s,b}\"] --- N18[\"{input,x,s,b}\"] \n    N4[\"{input,x,s,t,b}\"] --- N19[\"{input,x,s,t}\"] \n    N7[\"{input,x,y,s,t}\"] --- N19[\"{input,x,s,t}\"] \n    N5[\"{input,x,y,t,b}\"] --- N20[\"{input,x,y,b}\"] \n    N6[\"{input,x,y,s,b}\"] --- N20[\"{input,x,y,b}\"] \n    N5[\"{input,x,y,t,b}\"] --- N21[\"{input,x,y,t}\"] \n    N7[\"{input,x,y,s,t}\"] --- N21[\"{input,x,y,t}\"] \n    N6[\"{input,x,y,s,b}\"] --- N22[\"{input,x,y,s}\"] \n    N7[\"{input,x,y,s,t}\"] --- N22[\"{input,x,y,s}\"] \n    N1[\"{input,x,y,s,t,b}\"] --- N2[\"{x,y,s,t,b}\"] \n    N1[\"{input,x,y,s,t,b}\"] --- N3[\"{input,y,s,t,b}\"] \n    N1[\"{input,x,y,s,t,b}\"] --- N4[\"{input,x,s,t,b}\"] \n    N1[\"{input,x,y,s,t,b}\"] --- N5[\"{input,x,y,t,b}\"] \n    N1[\"{input,x,y,s,t,b}\"] --- N6[\"{input,x,y,s,b}\"] \n    N1[\"{input,x,y,s,t,b}\"] --- N7[\"{input,x,y,s,t}\"] </code></pre> <p>In the above lattice, the \\(\\top\\) is the full set of \\(V\\) and the \\(\\bot\\) is the empty set \\(\\{\\}\\). The order \\(\\subseteq\\) is the subset relation \\(\\sqsubseteq\\).</p>"},{"location":"notes/liveness_analysis/#defining-the-monotone-constraint-for-liveness-analysis","title":"Defining the Monotone Constraint for Liveness Analysis","text":"<p>In Sign Analysis the state variable \\(s_i\\) denotes the mapping of the variables to the sign abstract values after the instruction \\(i\\) is executed.</p> <p>In Liveness Analysis, we define the state variable \\(s_i\\) as the set of variables may live before the execution of the instruction \\(i\\).</p> <p>In Sign Analysis the \\(join(s_i)\\) function is defined as the  least upper bound of all the states that are preceding \\(s_i\\) in the control flow.</p> <p>In Liveness Analysis, we define the \\(join(s_i)\\) function as follows</p> \\[ join(s_i) = \\bigsqcup succ(s_i) \\] <p>where \\(succ(s_i)\\) returns the set of successors of \\(s_i\\) according to the control flow graph.</p> <p>The monotonic functions can be defined by the following cases.</p> <ul> <li>case \\(l:ret\\), \\(s_l = \\{\\}\\)</li> <li>case \\(l: t \\leftarrow src\\), \\(s_l = join(s_l) - \\{ t \\} \\cup var(src)\\)</li> <li>case \\(l: t \\leftarrow src_1\\ op\\ src_2\\), \\(s_l = join(s_l) - \\{t\\} \\cup var(src_1) \\cup var(src_2)\\)</li> <li>case \\(l: r \\leftarrow src\\), \\(s_l = join(s_l) \\cup var(src)\\)</li> <li>case \\(l: r \\leftarrow src_1\\ op\\ src_2\\), \\(s_l = join(s_l) \\cup var(src_1) \\cup var(src_2)\\)</li> <li>case \\(l: ifn\\ t\\ goto\\ l'\\), \\(s_l = join(s_l) \\cup \\{ t \\}\\)</li> <li>other cases: \\(s_l = join(s_l)\\)</li> </ul> <p>The helper function \\(var(src)\\) returns the set of variables (either empty or singleton) from operand \\(src\\).</p> \\[ \\begin{array}{rcl} var(r) &amp; = &amp; \\{ \\} \\\\  var(t) &amp; = &amp; \\{ t \\} \\\\  var(c) &amp; = &amp; \\{ \\} \\end{array} \\] <p>By applying the <code>PA</code> program above we have</p> <p><pre><code>s11 = {}\ns10 = join(s10) U {s}               = {s}\ns9  = join(s9)                      = s4\ns8  = (join(s8) - {s}) U {s, y}     = (s9 - {s}) U {s, y}\ns7  = (join(s7) - {t}) U {s}        = (s8 - {t}) U {s}\ns6  = (join(s6) - {y}) U {y}        = (s7 - {y}) U {y}\ns5  = join(s5) U {b}                = s6 U s10 U {b}\ns4  = (join(s4) - {b}) U {y, x}     = (s5 - {b}) U {y, x}\ns3  = join(s3) - {s}                = s4 - {s}\ns2  = join(s2) - {y}                = s3 - {y}\ns1  = (join(s1) - {x}) U {input}    = (s2 - {x}) U {input}\n</code></pre> For the ease of seeing the change of \"flowing\" direction, we order the state variables in descending order.</p> <p>By turning the above equation system to a monotonic function</p> \\[ \\begin{array}{rcl} f_1(s_{11}, s_{10}, s_9, s_8, s_7, s_6, s_5, s_4, s_3, s_2, s_1) &amp; = &amp; \\left (     \\begin{array}{c}      \\{\\}, \\\\       \\{s\\}, \\\\      s_4, \\\\      (s_9 -\\{s\\}) \\cup \\{s,y\\}, \\\\      (s_8 - \\{t\\}) \\cup \\{s\\}, \\\\      (s_7 - \\{y\\}) \\cup \\{y\\}, \\\\      s_6 \\cup s_{10} \\cup \\{b\\}, \\\\      (s_5 - \\{b\\}) \\cup \\{y, x\\}, \\\\      s_4 - \\{s\\}, \\\\      s_3 - \\{y\\}, \\\\      (s_2 - \\{x\\}) \\cup \\{ input \\}     \\end{array}      \\right ) \\end{array} \\] <p>Question, can you show that \\(f_1\\) is a monotonic function?</p> <p>By applying the naive fixed point algorithm (or its optimized version) with starting states <code>s1 = ... = s11 = {}</code>, we solve the above constraints and find</p> <pre><code>s11 = {}\ns10 = {s}\ns9  = {y,x,s}\ns8  = {y,x,s}\ns7  = {y,x,s}\ns6  = {y,x,s}\ns5  = {y,x,s,b}\ns4  = {y,x,s}\ns3  = {y, x}\ns2  = {x}\ns1  = {input}\n</code></pre> <p>From which we can identify at least two possible optimization opportunities.</p> <ol> <li><code>t</code> is must be dead throughout the entire program. Hence instruction <code>7</code> is redundant.</li> <li><code>input</code> only lives at instruction 1. If it is not holding any heap references, it can be freed. </li> <li><code>x,y,b</code> lives until instruction 9. If they are not holding any heap references, they can be freed.</li> </ol>"},{"location":"notes/liveness_analysis/#forward-vs-backward-analysis","title":"Forward vs Backward Analysis","text":"<p>Given an analysis in which the monotone equations are defined by deriving the current state based on the predecessors's states, we call this analysis a forward analysis. </p> <p>Given an analysis in which the monotone equations are defined by deriving the current state based on the successor's states, we call this analysis a  backward analysis.</p> <p>For instance, the sign analysis is a forward analysis and the liveness analysis is a backward analysis.</p>"},{"location":"notes/liveness_analysis/#may-analysis-vs-must-analysis","title":"May Analysis vs Must Analysis","text":"<p>Given an analysis that makes use of powerset lattice, it is a may analysis if it gives an over-approximation. For example, liveness analysis analyses the set of variables that may be \"live\" at a program point. </p> <p>Given an analysis that makes use of powerset lattice, it is a must analysis if it gives an under-approximation. For example, if we negate the result of a liveness analysis to analyse the set of variables that must be \"dead\" at a program point. In this analysis we can keep track of the set of variables must be dead and use \\(\\sqcap\\) (which is \\(\\cap\\)) instead of \\(\\sqcup\\) (which is \\(\\cup\\)).</p>"},{"location":"notes/name_analysis/","title":"50.054 - Name Analysis","text":""},{"location":"notes/name_analysis/#learning-outcomes","title":"Learning Outcomes","text":"<ol> <li>Articulate the purpose of name analysis.</li> <li>Describe the properties of the static single assignment forms.</li> <li>Implement the static single assignment construction and deconstruction algorithms.</li> </ol>"},{"location":"notes/name_analysis/#what-is-name-analysis","title":"What is Name Analysis","text":"<p>Given a source program (or AST), the compiler needs to check for each identifier defined (i.e. name).</p> <ol> <li>Is it a variable name or a function name? This is not an issue for SIMP language as we don't deal with function at the momement.</li> <li>Is the variable name of type int or bool? This has been addressed via the type inference and type checking in the previous unit.</li> <li>What is the scope of the variable?</li> <li>Has the variable been declared before used?</li> <li>Where is the defined variable used?</li> </ol>"},{"location":"notes/name_analysis/#variable-scope","title":"Variable Scope","text":"<p>Consider the following Python program, </p> <p><pre><code>x = -1\n\ndef f():\n    x = 1\n    return g()\n\ndef g():\n    print(x)\n\nf()\n</code></pre> When the program is executed, we observe <code>-1</code> being printed. The variable <code>x=1</code> in <code>f()</code> does not modify the <code>x=-1</code> in the outer scope. Hence when <code>g()</code> is called, the variable <code>x</code> being printed is from the global scope <code>x=-1</code>. This is known as static scoping. </p>"},{"location":"notes/name_analysis/#static-variable-scoping","title":"Static Variable Scoping","text":"<p>For a programming language with static variable scoping, the relation between a variable's definition and its reference is defined by its syntactic structure, (also known as lexical structure). For instance the earlier example shows that Python is using static variable scoping, because the Python program has the following syntactic structure (e.g. Syntax Tree).</p> <pre><code>graph TD;\n    Main --&gt; x1[\"x=-1\"];\n    Main --&gt; f;\n    f --&gt; x2[x=1];\n    Main --&gt; g;\n    g --&gt; usex1[\"print(x)\"];</code></pre> <p>Thus the <code>print(x)</code> of <code>g</code> uses the <code>x</code> defined in its parent node.</p>"},{"location":"notes/name_analysis/#dynamic-variable-scoping","title":"Dynamic Variable Scoping","text":"<p>For a programming language with dynamic scoping, the relation between a variable's definition and its reference is defined by the dynamic call stack. </p> <p><pre><code>$x = -1;\n\nsub f {\n    local $x = 1;\n    return g();\n}\n\nsub g {\n    print $x;\n}\nf()\n</code></pre> In the above, it is the same program coded in <code>perl</code>. Except that in perl, variables with <code>local</code> are defined using dynamic scoping. As a result, <code>1</code> is printed when the program is executed. When a program with dynamic variable scoping is executed, its variable reference follows the </p> <pre><code>graph TD;\n    Main --&gt; x1[\"x=-1\"];\n    Main --&gt; f;\n    f --&gt; x2[x=1];\n    f --&gt; g;\n    g --&gt; usex1[\"print(x)\"];    </code></pre> <p>As illustrated by the dynamic call graph above, the variable <code>x</code> in <code>print(x)</code> refers to <code>g</code>'s caller, i.e. <code>f</code>, which is <code>1</code>.</p>"},{"location":"notes/name_analysis/#more-on-static-variable-scoping","title":"More On Static Variable Scoping","text":"<p>Static Variable Scoping is dominating the program language market now. Most of the main stream languages uses static variable scoping thanks to its ease of reasoning, e.g. C, C++, Python, Java and etc. Among these languages, there are also some minor variant of static variable scoping implementation.</p> <p>Consider the following Python program.</p> <p><pre><code>def main(argv):\n    x = 1\n    if len(argv) == 0:\n        x = 2\n    else:\n        y = 1\n    print(y)\n</code></pre> when the input <code>argv</code> is a non-empty list, the function <code>main</code> prints <code>1</code> as results. However when <code>argv</code> is an empty list, a run-time error arises.</p> <p>Consider the \"nearly-the-same\" program in Java.</p> <pre><code>class Main {\n    public static int main(String[] argv) {\n        int x = 1;\n        if (argv.length &gt; 0){\n            x = 2;\n        } else {\n            int y = 1;\n        }\n        System.out.println(y.toString());\n        return 1;\n    }\n}\n</code></pre> <p>Java returns a compilation error, complaining variable <code>y</code> being use in the <code>System.out.println</code> function can't be resolved.</p> <p>The difference here is that in Python, all variables declared in a function share the same scope. While in Java, variable's scope is further divided based on the control flow statement such as if-else, while and etc. In the above example, the variable <code>y</code>'s scope is only within the else branch but not outside.</p> <p>In SIMP, we assume the same variable scoping implementation as Python, i.e. all variables declared in a function shared the same scope, and since the SIMP language we have so far does not support function call, we further simplify the problem that all variables are sharing same scope. </p> <p>However how might we detect the run-time error similar to what we've observed from the last Python example? </p> <p>Let's recast the example in SIMP, let's call it <code>SIMP_ERR1</code> <pre><code>// SIMP_ERR1\nx = 1;\nif input == 0 {\n    x = 2;\n} else {\n    y = 1;\n}\nreturn y;\n</code></pre></p> <p>The above program will cause an error when <code>input == 0</code>. It is typeable based on the type inference algorithm we studied in the previous class.  Let's consider its pseudo assembly version. The Maximal Munch algorithm v2 produces the following given the SIMP program.</p> <p><pre><code>// PA_ERR1\n1:  x &lt;- 1\n2:  t &lt;- input == 0\n3:  ifn t goto 6\n4:  x &lt;- 2\n5:  goto 7\n6:  y &lt;- 1\n7:  rret &lt;- y\n8:  ret\n</code></pre> Same error arises when <code>input == 0</code>. </p>"},{"location":"notes/name_analysis/#static-single-assignment-form","title":"Static Single Assignment form","text":"<p>Static Single Assignment (SSA) form is an intermediate representation  widely used in compiler design and program verification. </p> <p>In a static single assignment form, </p> <ul> <li>Each variable is only allowed to be assigned once syntactically, i.e. it only appears in the LHS of the assignment once. </li> <li>\\(\\phi\\)-assignments are placed at the end of branching statements to merge different (re)-definition of the same variable (from the source program). </li> </ul> <p>SSA form construction is one of the effective ways to analysis</p> <ol> <li>the scope of variables</li> <li>the use-def relationship of variables</li> </ol>"},{"location":"notes/name_analysis/#unstructured-ssa-form","title":"Unstructured SSA Form","text":"<p>Suppose we extend the pseudo assembly with \\(\\phi\\)-assignment statements, </p> \\[ \\begin{array}{rccl} (\\tt Labeled\\ Instruction) &amp; li  &amp; ::= &amp; l : \\overline{\\phi}\\ i \\\\  (\\tt Instruction)   &amp; i   &amp; ::= &amp; d \\leftarrow s \\mid d \\leftarrow s\\ op\\ s \\mid ret \\mid ifn\\ s\\ goto\\ l \\mid goto\\ l \\\\  (\\tt PhiAssignment) &amp; \\phi &amp; ::= &amp; d \\leftarrow phi(\\overline{l:s}) \\\\  (\\tt Labeled\\ Instructions)   &amp; lis   &amp; ::= &amp; li \\mid li\\ lis \\\\  (\\tt Operand)       &amp; d,s &amp; ::= &amp; r \\mid c \\mid t \\\\ (\\tt Temp\\ Var)      &amp; t   &amp; ::= &amp; x \\mid y \\mid ...  \\\\ (\\tt Label)         &amp; l   &amp; ::= &amp; 1 \\mid 2 \\mid ... \\\\ (\\tt Operator)      &amp; op  &amp; ::= &amp; + \\mid - \\mid &lt; \\mid == \\mid ... \\\\  (\\tt Constant)      &amp; c   &amp; ::= &amp; 0 \\mid 1 \\mid 2 \\mid ... \\\\  (\\tt Register)      &amp; r &amp;   ::= &amp; r_{ret} \\mid r_1 \\mid r_2 \\mid ...   \\end{array} \\] <p>The syntax is largely unchanged, except that for each labeled instruction, there exists a sequence of phi assignments \\(\\overline{\\phi}\\). (which could be empty) before the actual instruction \\(i\\). When \\(\\overline{\\phi}\\) is empty, we omit it from the syntax.</p> <p>we are able convert any \"well-defined\" pseudo assembly program into an SSA form. Since we build the SSA forms from some unstructured language program (i.e. no nested control flow statements), we call them unstructured SSA forms.</p> <p>Suppose we have the following pseudo assembly program </p> <pre><code>// PA1\n1: x &lt;- input\n2: s &lt;- 0\n3: c &lt;- 0\n4: t &lt;- c &lt; x \n5: ifn t goto 9\n6: s &lt;- c + s\n7: c &lt;- c + 1\n8: goto 4\n9: rret &lt;- s\n10: ret\n</code></pre> <p>Note that variables <code>s</code> and <code>c</code> are re-assigned in the loop. </p> <p>The SSA form of the above is </p> <pre><code>// SSA_PA1\n1: x0 &lt;- input\n2: s0 &lt;- 0\n3: c0 &lt;- 0\n4: s1 &lt;- phi(3:s0, 8:s2)\n   c1 &lt;- phi(3:c0, 8:c2)\n   t0 &lt;- c1 &lt; x0\n5: ifn t0 goto 9\n6: s2 &lt;- c1 + s1\n7: c2 &lt;- c1 + 1\n8: goto 4\n9: rret &lt;- s1 \n10: ret\n</code></pre> <p>In the above example, we inserted a set of phi assigments to label 4. Every variable/register is strictly assigned once. We need to introduce a new \"variant\" of the same source variable whenever re-assignment is needed. More specifically, in instruction with label 4, we use two phi assignments to merge the multiple definitions of the same source variable.  </p> <p>There are two possible preceding instructions that lead us to the following instruction <pre><code>4: s1 &lt;- phi(3:s0, 9:s2)\n   c1 &lt;- phi(3:c0, 9:c2)\n</code></pre> namely, 3 and 9. When the preceding instruction is 3, the above phi assignments will assign <code>s0</code> to <code>s1</code> and <code>c0</code> to <code>c1</code>. Otherwise, <code>s2</code> is assigned to <code>s1</code> and <code>c2</code> is assigned to <code>c1</code>.</p> <p>To cater for the phi assignment, we extend the small step operational semantics from \\(\\(P \\vdash (L, li) \\longrightarrow (L', li')\\)\\) </p> <p>to </p> \\[P \\vdash (L, li, p) \\longrightarrow (L', li', p')\\] <p>The third component \\(p\\) in the program context is a label from the preceding instruction based on the execution. </p> \\[ {\\tt (pConst)} ~~~ P \\vdash (L, l:  d \\leftarrow c, p) \\longrightarrow (L \\oplus (d,c), P(l+1), l) \\] \\[ {\\tt (pRegister)} ~~~P \\vdash (L, l: d \\leftarrow r, p) \\longrightarrow (L \\oplus (d,L(r)), P(l+1), l) \\] \\[ {\\tt (pTempVar)} ~~~P \\vdash (L, l: d \\leftarrow t, p ) \\longrightarrow (L \\oplus (d,L(t)), P(l+1), l) \\] \\[ {\\tt (pGoto)} ~~ P \\vdash (L, l:goto\\ l', p) \\longrightarrow (L, P(l'), l) \\] \\[ \\begin{array}{rc} {\\tt (pOp)} &amp;  \\begin{array}{c}         c_1 = L(s_1) ~~~ c_2 = L(s_2) ~~~ c_3 = c_1\\ op\\ c_2         \\\\ \\hline         P \\vdash (L, l: d \\leftarrow s_1\\ op\\ s_2, p) \\longrightarrow (L \\oplus (d,c_3), P(l+1), l)           \\end{array} \\\\ {\\tt (pIfn0)} &amp; \\begin{array}{c}      L(s) = 0      \\\\ \\hline      P \\vdash (L, l: ifn\\ s\\ goto\\ l', p) \\longrightarrow (L, P(l+1), l)      \\end{array} \\\\ {\\tt (pIfnNot0)} &amp; \\begin{array}{c}      L(s) \\neq  0      \\\\ \\hline      P \\vdash (L, l: ifn\\ s\\ goto\\ l', p) \\longrightarrow (L, P(l'), l)      \\end{array} \\end{array} \\] <p>All the existing rules are required some minor changes to accomodate the third component in the program context.  The adjustments are common, i.e. propogating the label of the current labeled instruction from the LHS to the RHS as the proceding label. Note that the above handle the cases in which the labeled instruction has no phi assignments.  In the presence of phi-assignments,  we need the following rules to guide the execution.</p> \\[ \\begin{array}{rc} {\\tt (pPhi1)} &amp;  \\begin{array}{c}         (L, l: []\\ i, p) \\longrightarrow (L, l: i, p)         \\end{array} \\\\ \\\\ {\\tt (pPhi2)} &amp;  \\begin{array}{c}         l_i = p\\ \\ \\ c_i = L(s_i) \\\\  j \\in [1,i-1]: l_j \\neq p          \\\\ \\hline         (L, l: d \\leftarrow phi(l_1:s_1,..,l_n:s_n); \\overline{\\phi}\\ i , p) \\longrightarrow (L\\oplus(d,c_i), l: \\overline{\\phi}\\ i, p)           \\end{array} \\end{array} \\] <p>The execution of the labeled instruction with phi assignments is defined by the \\((\\tt pPhi1)\\) and \\((\\tt pPhi2)\\) rules. </p> <ul> <li>The \\((\\tt pPhi1)\\) rule handles the base case where \\(\\overline{\\phi}\\) is an empty sequence, it proceeds to execute the following instruction \\(i\\) by using one of the earlier rules. </li> <li>The \\((\\tt pPhi2)\\) rule is applied when the sequence of phi-assignments is not empty. <ol> <li>We process the first one phi-assignment. By scanning the set of labels in the \\(phi()\\)'s operands from left to right, we identify the first matching label \\(l_i\\) and lookup the value of the associated variable/register \\(s_i\\), i.e. \\(c_i\\).</li> <li>Add the new entry \\((d,c_i)\\) to the local environment \\(L\\). </li> <li>Proceed by recursively processing the rest of the phi assignments with the updated \\(L \\oplus (d,c_i)\\).</li> </ol> </li> </ul> <p>Given \\(input = 1\\), excuting <code>SSA_PA1</code> yields the following derivation</p> <pre><code>P |- {(input,1)}, 1: x0 &lt;- input, undef ---&gt; # (pTempVar)\nP |- {(input,1), (x0,1)}, 2: s0 &lt;- 0, 1 ---&gt; # (pConst)\nP |- {(input,1), (x0,1), (s0,0)}, 3: c0 &lt;- 0, 2 ---&gt; # (pConst)\nP |- {(input,1), (x0,1), (s0,0), (c0,0)}, 4: s1 &lt;- phi(3:s0, 9:s2); c1 &lt;- phi(3:c0, 9:c2) t0 &lt;- c1 &lt; x0, 3 ---&gt; # (pPhi2)\nP |- {(input,1), (x0,1), (s0,0), (c0,0), (s1,0)}, 4: c1 &lt;- phi(3:c0, 9:c2) t0 &lt;- c1 &lt; x0, 3 ---&gt; # (pPhi2)\nP |- {(input,1), (x0,1), (s0,0), (c0,0), (s1,0), (c1,0) }, 4: [] t0 &lt;- c1 &lt; x0, 3 ---&gt; # (pPhi1)\nP |- {(input,1), (x0,1), (s0,0), (c0,0), (s1,0), (c1,0) }, 4: t0 &lt;- c1 &lt; x0, 3 ---&gt; # (pOp)\nP |- {(input,1), (x0,1), (s0,0), (c0,0), (s1,0), (c1,0), (t0, 1)}, 5: ifn t0 goto 9, 4 ---&gt; # (pIfn0)\nP |- {(input,1), (x0,1), (s0,0), (c0,0), (s1,0), (c1,0), (t0, 1)}, 6: s2 &lt;- c1 + s1, 5  ---&gt; # (pOp)\nP |- {(input,1), (x0,1), (s0,0), (c0,0), (s1,0), (c1,0), (t0, 1), (s2, 0)}, 7: c2 &lt;- c1 + 1, 8  ---&gt; # (pOp)\nP |- {(input,1), (x0,1), (s0,0), (c0,0), (s1,0), (c1,0), (t0, 1), (s2, 0), (c2, 1)}, 8: goto 4, 7  ---&gt; # (pGoto)\nP |- {(input,1), (x0,1), (s0,0), (c0,0), (s1,0), (c1,0), (t0, 1), (s2, 0), (c2, 1)}, 4: s1 &lt;- phi(3:s0, 9:s2); c1 &lt;- phi(3:c0, 9:c2) t0 &lt;- c1 &lt; x0, 8 ---&gt; # (pPhi2)\nP |- {(input,1), (x0,1), (s0,0), (c0,0), (s1,0), (c1,0), (t0, 1), (s2, 0), (c2, 1)}, 4: c1 &lt;- phi(3:c0, 9:c2) t0 &lt;- c1 &lt; x0, 8 ---&gt; # (pPhi2)\nP |- {(input,1), (x0,1), (s0,0), (c0,0), (s1,0), (c1,1), (t0, 1), (s2, 0), (c2, 1)}, 4: [] t0 &lt;- c1 &lt; x0, 8 ---&gt; # (pPhi1)\nP |- {(input,1), (x0,1), (s0,0), (c0,0), (s1,0), (c1,1), (t0, 1), (s2, 0), (c2, 1)}, 4: t0 &lt;- c1 &lt; x0, 8 ---&gt; # (pOp)\nP |- {(input,1), (x0,1), (s0,0), (c0,0), (s1,0), (c1,1), (t0, 0), (s2, 0), (c2, 1)}, 5: ifn t0 goto 9, 4 ---&gt; # (pIfn0)\nP |- {(input,1), (x0,1), (s0,0), (c0,0), (s1,0), (c1,1), (t0, 0), (s2, 0), (c2, 1)}, 9: rret &lt;- s1, 5 ---&gt; # (pTempVar)\nP |- {(input,1), (x0,1), (s0,0), (c0,0), (s1,0), (c1,1), (t0, 0), (s2, 0), (c2, 1), (rret, 0)}, 10: ret, 9 \n</code></pre>"},{"location":"notes/name_analysis/#minimality","title":"Minimality","text":"<p>One may argue that instead of generating <code>SSA_PA1</code>, one might generate the following static single assignment</p> <pre><code>// SSA_PA2\n1: x0 &lt;- input\n2: s0 &lt;- 0\n3: c0 &lt;- 0\n4: s1 &lt;- phi(3:s0, 8:s2)\n   c1 &lt;- phi(3:c0, 8:c2)\n   t0 &lt;- c1 &lt; x0\n5: ifn t0 goto 9\n6: s2 &lt;- c1 + s1\n7: c2 &lt;- c1 + 1\n8: goto 4\n9: s3 &lt;- phi(5:s1)\n   rret &lt;- s3\n10: ret\n</code></pre> <p>which will yield the same output. However we argue that <code>SSA_PA1</code> is preferred as it has the minimal number of phi assignments.</p>"},{"location":"notes/name_analysis/#ssa-construction-algorithm","title":"SSA Construction Algorithm","text":"<p>The defacto SSA construction algorithm that produces minimal SSA forms was developed by Cytron et al.</p> <p>https://doi.org/10.1145/115372.115320</p> <p>The main idea is to take the original program and identify the \"right\" locations to insert phi assignments so that the result is a minimal SSA form.</p>"},{"location":"notes/name_analysis/#control-flow-graph","title":"Control flow graph","text":"<p>We can model a Pseudo Assembly program using a graph, namely the contorl flow graph.</p> <p>For example, <code>PA1</code> can be represented as the following Control flow graph <code>Graph1_PA1</code></p> <pre><code>graph TD;\n    B1(\"1: x &lt;- input\n2: s &lt;- 0\n3: c &lt;- 0\")--&gt;B2;\n    B2--&gt;B3;\n    B2(\"4: t &lt;- c &lt; x \n5: ifn t goto 9\")--&gt;B4(\"9: rret &lt;- s\n10: ret\");\n    B3(\"6: s &lt;- c + s\n7: c &lt;- c + 1\n8: goto 4\")--&gt;B2;</code></pre> <p>For the ease of reasoning (though unnecessary), without losing the graph properties, we would consider an isomoprhic version of the above graph where the vertices with multiple instructions are further divided until each vertex contains only one instruction, let's call it <code>Graph2_PA1</code></p> <pre><code>graph TD;\n    V1(\"1: x &lt;- input\") --&gt; V2;\n    V2(\"2: s &lt;- 0\") --&gt; V3;\n    V3(\"3: c &lt;- 0\") --&gt; V4;\n    V4(\"4: t &lt;- c &lt; x\") --&gt; V5;\n    V5(\"5: ifn t goto 9\") --&gt; V9;\n    V9(\"9: rret &lt;- s\") --&gt; V10(\"10: ret\")\n    V5(\"5: ifn t goto 9\") --&gt; V6;\n    V6(\"6: s &lt;- c + s\") --&gt; V7;\n    V7(\"7: c &lt;- c + 1\") --&gt; V8;\n    V8(\"8: goto 4\") --&gt; V4;</code></pre> <p>Now we refer to the vertex in a control flow graph by the label.</p> <p>The technical trick is to apply some graph operation to identify the \"right\" locations for phi assignments from the CFG.</p>"},{"location":"notes/name_analysis/#identifying-the-right-locations","title":"Identifying the \"right\" locations","text":""},{"location":"notes/name_analysis/#definition-1-graph","title":"Definition 1 - Graph","text":"<p>Let \\(G\\) be a graph, \\(G = (V, E)\\), where \\(V\\) denotes the set of vertices and \\(E\\) denote a set of edges. Let \\(v_1 \\in V\\) and \\(v_2 \\in V\\), \\((v_1,v_2) \\in E\\) implies that exists an edge going from \\(v_1\\) to \\(v_2\\). </p> <p>Occassionally, we also refer to a vertex as a node in the graph.  For convenience, we also write </p> <ul> <li>\\(v \\in G\\) as the short-hand for \\(v \\in V \\wedge G = (V,E)\\) and </li> <li>\\((v_1, v_2) \\in G\\) as the short-hand for \\((v_1, v_2) \\in E \\wedge G = (V,E)\\).</li> </ul>"},{"location":"notes/name_analysis/#definition-2-path","title":"Definition 2 - Path","text":"<p>Let \\(v_1\\) and \\(v_2\\) be two vertices in a graph \\(G\\). We say a path from \\(v_1\\) to \\(v_2\\), written as \\(path(v_1,v_2)\\), exists iff </p> <ol> <li>\\(v_1 = v_2\\) or</li> <li>the set of edges \\(\\{(v_1, u_1), (u_1,u_2), ..., (u_n,v_2)\\} \\subseteq E\\) where \\(E\\) is the set of edges in \\(G\\).</li> </ol> <p>For convenience, some times we write \\(v_1,u_1,...,u_n,v_2\\) to denote a particular path from \\(v_1\\) to \\(v_2\\).</p>"},{"location":"notes/name_analysis/#definition-3-connectedness","title":"Definition 3 - Connectedness","text":"<p>Let \\(v_1\\) and \\(v_2\\) be two vertices in a graph \\(G\\). We say \\(v_1\\) and \\(v_2\\) are connected, written \\(connect(v_1,v_2)\\), iff </p> <ol> <li>\\(path(v_1, v_2)\\) or \\(path(v_2, v_1)\\) exists, or </li> <li>there exists \\(v_3\\) in \\(G\\) such that \\(connect(v_1,v_3)\\) and \\(connect(v_3, v_2)\\).</li> </ol>"},{"location":"notes/name_analysis/#definition-4-source-and-sink","title":"Definition 4 - Source and Sink","text":"<p>Let \\(v\\) be a vertex in a graph \\(G\\), we say \\(v\\) is a source vertex if there exists no entry \\((v',v) \\in E\\) where \\(E\\) is the set of edges in \\(G\\).</p> <p>Let \\(v\\) be a vertex in a graph \\(G\\), we say \\(v\\) is a sink vertex if there exists no entry \\((v, v') \\in E\\) where \\(E\\) is the set of edges in \\(G\\).</p>"},{"location":"notes/name_analysis/#assumption","title":"Assumption","text":"<p>Since we are dealing with SIMP program's CFGs, we assume that the set of graphs we are considering are </p> <ol> <li>Connected, i.e. for any \\(v_1, v_2\\) in \\(G\\), we have \\(connect(v_1,v_2)\\) </li> <li>Has only one source vertex, which means there is only one entry point to the program.</li> <li>Has only one sink vertex, which means there is only one return statement.</li> </ol>"},{"location":"notes/name_analysis/#definition-5-dominance-relation","title":"Definition 5 - Dominance Relation","text":"<p>Let \\(v_1\\) and \\(v_2\\) be two vertices in a graph \\(G\\). We say \\(v_1\\) dominates \\(v_2\\), written as \\(v_1 \\preceq v_2\\), iff for all path \\(v_0,...,v_2\\) where \\(v_0\\) is the source vertex, we find a prefix sequence \\(v_0,...,v_1\\) in \\(v_0,...,v_2\\). </p> <p>In other words, \\(v_1 \\preceq v_2\\) means whenever we execute the program from the start to location \\(v_2\\), we definitely pass through location \\(v_1\\). </p> <p>For instance, in the earlier control flow graph for <code>Graph2_PA1</code>, </p> <ul> <li>the vertex <code>1</code> dominates all vertices. </li> <li>the vertex <code>4</code> dominates itself, the vertices <code>5,6,7,8,9,10</code>.</li> </ul>"},{"location":"notes/name_analysis/#lemma-1-dominance-is-transitive","title":"Lemma 1 - Dominance is transitive","text":"<p>\\(v_1 \\preceq v_2\\) and \\(v_2 \\preceq v_3\\) implies that \\(v_1 \\preceq v_3\\).</p>"},{"location":"notes/name_analysis/#lemma-2-dominance-is-reflexive","title":"Lemma 2 - Dominance is reflexive","text":"<p>For any vertex \\(v\\), we have \\(v \\preceq v\\).</p>"},{"location":"notes/name_analysis/#definition-6-strict-dominance","title":"Definition 6 - Strict Dominance","text":"<p>We say \\(v_1\\) stricly domainates \\(v_2\\), written \\(v_1 \\prec v_2\\) iff \\(v_1 \\preceq v_2\\) and \\(v_1 \\neq v_2\\).</p>"},{"location":"notes/name_analysis/#definition-7-immediate-dominator","title":"Definition 7 - Immediate Dominator","text":"<p>We say \\(v_1\\) is the immediate dominator of \\(v_2\\), written \\(v_1 = idom(v_2)\\) iff \\(v_1 \\prec v_2\\) and not exists \\(v_3\\) such that \\(v_1 \\prec v_3\\) and \\(v_3 \\prec v_2\\).</p> <p>Note that \\(idom()\\) is a function, i.e. the immediate dominator of a vertex must be unique if it exists.</p>"},{"location":"notes/name_analysis/#dominator-tree","title":"Dominator Tree","text":"<p>Given the \\(idom()\\) function, we can construct a dominator tree from a control flow graph \\(G\\).</p> <ul> <li>Each vertex \\(v \\in G\\) forms a node in the dominator tree.</li> <li>For vertices \\(v_1, v_2 \\in G\\), \\(v_2\\) is a child of \\(v_1\\) if \\(v_1 = idom(v_2)\\).</li> </ul> <p>For example, from the CFG <code>Graph2_PA1</code>, we construct a dominator tree <code>Tree2_PA1</code>, as follows, </p> <pre><code>graph TD;\n    1 --&gt; 2;\n    2 --&gt; 3;\n    3 --&gt; 4;\n    4 --&gt; 5;\n    5 --&gt; 6;\n    6 --&gt; 7;\n    7 --&gt; 8;\n    5 --&gt; 9;\n    9 --&gt; 10;</code></pre> <p>Let \\(T\\) be a dominator tree, we write \\(child(v,T)\\) to denote the set of children of \\(v\\) in \\(T\\).</p>"},{"location":"notes/name_analysis/#definition-8-dominance-frontier","title":"Definition 8 - Dominance Frontier","text":"<p>Let \\(v\\) be vertex in a graph \\(G\\), we define the dominance frontier of \\(v\\) as $$ df(v, G) = { v_2 \\mid (v_1,v_2) \\in G \\wedge v \\preceq v_1 \\wedge \\neg(v \\prec v_2) } $$</p> <p>In other words, the dominance frontier of a vertex \\(v\\) is the set of vertices that are not dominated by \\(v\\) but their predecesors are (dominated by \\(v\\)). </p> <p>For instance, in our running example, the dominance frontier of  vertex <code>6</code> is the set containing vertex <code>4</code> This is because </p> <ul> <li>vertex <code>8</code> is one of the predecesors of the vertex <code>4</code> and </li> <li>vertex <code>8</code> is dominated by vertex <code>6</code>, but not the vertex <code>4</code> is not domainated by vertex <code>6</code>.</li> </ul> <p>Question: what is the dominance frontier of vertex <code>5</code>?</p>"},{"location":"notes/name_analysis/#computing-dominance-frontier","title":"Computing Dominance Frontier","text":"<p>The naive algorithm of computing dominance frontier of all ther vertices in a CFG takes \\(O(n^2)\\) where \\(n\\) is the number of vertices. </p> <p>Cytron et al proposed a more efficient algorithm to compute the dominance frontiers of all the vertices in a CFG. </p>"},{"location":"notes/name_analysis/#re-definining-dominance-frontier","title":"Re-definining Dominance Frontier","text":"<p>The main idea is to give a recursive definition to Dominance Frontier by making use of the dominator tree. </p> <p>Let \\(G\\) be a CFG, and \\(T\\) be the dominator tree of \\(G\\). We define  $$ df(v, G) = df_{local}(v, G) \\cup \\bigcup_{u \\in child(v,T)} df_{up}(u, G)  <sub>~</sub>(E1) $$ where </p> \\[ df_{local}(v, G) = \\{ w \\mid (v,w) \\in G \\wedge \\neg(v \\prec w)\\} ~~~(E2) \\] <p>and </p> \\[ df_{up}(v, G) = \\{ w \\mid w \\in df(v,G) \\wedge \\neg (idom(v) \\prec w)\\}~~~(E3) \\] <ul> <li>\\((E1)\\) says that the dominance frontier of a vertex \\(v\\) is the union of the local contribution \\(df_{local}(v,G)\\) and the (dominator tree) descendants' upward contribution \\(\\bigcup_{u \\in child(v,T)} df_{up}(u, G)\\)</li> <li>\\((E2)\\) defines the local dominance frontier of a vertex \\(v\\) by finding successors \\(w\\) of \\(v\\) (i.e. there is an edge from \\(v\\) to \\(w\\)) that are not dominated by \\(v\\).</li> <li>\\((E3)\\) defines the upward contributed frontier of a vertex \\(v\\), by finding vertices \\(w\\) in \\(v\\)'s dominance frontier, such that \\(w\\) is not dominated by \\(v\\)'s immediate dominator (i.e. \\(v\\)'s parent in the dominator tree).</li> </ul> <p>Cytron et al shows that \\((E1)\\)  defines the same result as Definition 6.</p>"},{"location":"notes/name_analysis/#dominance-frontier-algorithm","title":"Dominance frontier algorithm","text":"<p>As we can observe from the recursive definition, it is more efficient to compute the dominance frontiers by traversing the dominator tree \"bottom-up\", as we can reuse the dominance frontier of the child nodes (vertices) to compute the upward contribution of the parent node (vertex).</p> <p>The algorithm is structured as follows</p> <ol> <li>For each vertex \\(v\\) by traversing the dominator tree bottom up:<ol> <li>compute \\(df_{local}(v,G)\\)</li> <li>compute \\(\\bigcup_{u \\in child(v,T)}df_{up}(u, G)\\), which can be looked up from the a memoization table.</li> <li>save \\(df(v,G) = df_{local}(v,G) \\cup \\bigcup_{u \\in child(v,T)} df_{up}(u,G)\\) in the memoization table.</li> </ol> </li> </ol> <p>For instance, we make use of <code>Graph2_PA1</code> and <code>Tree2_PA1</code> to construct the following memoization table <code>Table2_PA1</code></p> vertex/node successors children idom \\(df_{local}\\) \\(df_{up}\\) df 10 {} {} 9 {} {} {} 9 {10} {10} 5 {} {} {} 8 {4} {} 7 {4} {4} {4} 7 {8} {8} 6 {} {4} {4} 6 {7} {7} 5 {} {4} {4} 5 {6,9} {6,9} 4 {} {4} {4} 4 {5} {5} 3 {} {} {4} 3 {4} {4} 2 {} {} {} 2 {3} {3} 1 {} {} {} 1 {2} {2} {} {} {} <p>From the above table, we conclude that variables that are updated in vertices <code>5,6,7,8</code> should be merged via phi-assignments at the entry point of vertex <code>4</code>.</p> <p>As highlighted in Cytron's paper, \\(df_{local}(x,G)\\) can be defined efficiently as \\(\\{ y \\mid  (x,y)\\in G \\wedge idom(y) \\neq x \\}\\)</p> <p>Furthermore, \\(df_{up}(u,x,G)\\) can be defined efficiently as \\(\\{y \\mid y \\in df(u) \\wedge idom(y) \\neq x \\}\\)</p> <p>Note that in Cytron's paper, they include two special vertices, <code>entry</code> the entry vertex, and <code>exit</code> as the exit, and <code>entry</code> dominates everything, and <code>exit</code> is only dominated by <code>entry</code>. The purpose is to handle langugage allowing multiple return statements.</p>"},{"location":"notes/name_analysis/#definition-9-iterative-dominance-frontier","title":"Definition 9 - Iterative Dominance Frontier","text":"<p>As pointed out by Cytron's work, if a variable \\(x\\) is updated in a program location (vertex) \\(v\\), a phi-assignment for this variable must be inserted in the dominance frontier of \\(v\\). However inserting a phi assignment at the dominance fronter of \\(v\\) introduces a new location of modifying the variable \\(x\\). This leads to some \"cascading effect\" in computing the phi-assignment locations. </p> <p>We extend the dominance frontier to handle a set of vertices.</p> <p>Let \\(S\\) denote a set of vertices of a graph \\(G\\). We define</p> \\[ df(S, G) = \\bigcup_{v\\in S} df(v, G) \\] <p>We define the iterative dominance frontier recursively as follows</p> \\[ \\begin{array}{l} df_1(S, G) = df(S, G) \\\\ df_n(S, G) = df(S \\cup df_{n-1}(S,G), G) \\end{array} \\] <p>It can be proven that there exists \\(k \\geq 1\\) where \\(df_{k}(S,G) = df_{k+1}(S,G)\\), i.e. the set is bounded. We use \\(df^+(S,G)\\) to denote the upper bound.</p> <p>It follows that if a variable \\(x\\) is modified in locations \\(S\\), then the set of phi-assignments to be inserted for \\(x\\) is \\(df^+(S,G)\\).</p>"},{"location":"notes/name_analysis/#ssa-construction-algorithm_1","title":"SSA construction algorithm","text":"<p>Given the control flow graph \\(G\\), the dominator tree \\(T\\), and the dominance frontier table \\(DFT\\), the SSA construction algorithm consists of two steps.</p> <ol> <li>insert phi assignments to the original program \\(P\\).</li> <li>rename variables to ensure the single assignment property.</li> </ol>"},{"location":"notes/name_analysis/#inserting-phi-assignments","title":"Inserting Phi assignments","text":"<p>Before inserting the phi assignments to \\(P\\), we need some intermediate data structure. </p> <ol> <li>A dictionary \\(E\\) that maps program labels (vertices in CFG) to a set of variables.  \\((l, S) \\in E\\)  implies that variables in \\(S\\) having phi-assignment to be inserted at the vertex label \\(l\\). \\(E\\) can be constructed from the \\(DFT\\) table using the \\(df^+(\\cdot,\\cdot)\\) operation.</li> </ol> <p>Input: the original program <code>P</code>, can be viewed as a list of labeled instructions. Output: the modified program <code>Q</code>. can be viewed as a list of labeled instructions.</p> <p>The phi-assignment insertion process can be described as follows,</p> <ol> <li><code>Q = List()</code></li> <li>for each <code>l:i</code> in <code>P</code><ol> <li>match <code>E.get(l)</code> with <ol> <li>case <code>None</code><ol> <li>add <code>l:i</code> to <code>Q</code></li> </ol> </li> <li>case <code>Some(xs)</code><ol> <li><code>phis = xs.map( x =&gt; x &lt;- phi( k:x | (k in pred(l,G)))</code></li> <li>if <code>phis</code> has more than 1 operand, add <code>l:phis i</code> to <code>Q</code></li> </ol> </li> </ol> </li> </ol> </li> </ol> <p>\\(pred(v, G)\\) retrieves the set of predecessors of vertex (label) in graph \\(G\\). </p> <p>For example, given <code>PA1</code>,</p> <ul> <li>variable \\(x\\) is modified at <code>1</code></li> <li>variable \\(s\\) is modified at <code>2,6</code></li> <li>variable \\(c\\) is modified at <code>3,7</code></li> <li>variable \\(t\\) is modified at <code>4</code></li> </ul> <p>We construct \\(E\\) by consulting the dominance frontier table <code>Table2_PA1</code>.</p> <p><pre><code>E = Map(\n    4 -&gt; Set(\"s\",\"c\", \"t\")\n)\n</code></pre> which says that in node/vertex <code>4</code>, we should insert the phi-assignments for variable <code>s</code> and <code>c</code>.</p> <p>Now we apply the above algorithm to <code>PA1</code> which generates</p> <pre><code>// PRE_SSA_PA1\n1: x &lt;- input\n2: s &lt;- 0\n3: c &lt;- 0\n4: s &lt;- phi(3:s, 8:s)\n   c &lt;- phi(3:c, 8:c)\n   t &lt;- c &lt; x\n5: ifn t goto 9\n6: s &lt;- c + s\n7: c &lt;- c + 1\n8: goto 4\n9: rret &lt;- s \n10: ret\n</code></pre> <p>Note that when we try to insert the phi assignment for <code>t</code> at <code>4</code>, we realize that there is only one operand. This is because <code>t</code> is not defined before label <code>4</code>. In this case we remove the phi assignment for <code>t</code>.</p>"},{"location":"notes/name_analysis/#renaming-variables","title":"Renaming Variables","text":"<p>Given an intermediate output like <code>PRE_SSA_PA1</code>, we need to rename the variable so that there is only one assignment for each variable.</p> <p>Inputs: </p> <ul> <li>a dictionary of stacks <code>K</code> where the keys are the variable names in the original PA program. e.g. <code>K(x)</code> returns the stack for variable <code>x</code>. </li> <li> <p>the input program in with phi assignment but oweing the variable renaming,  e.g. <code>PRE_SSA_PA1</code>. We view the program as a dictionary mapping labels to labeled instructions.</p> </li> <li> <p>For each variable <code>x</code> in the program, initialize <code>K(x) = Stack()</code>.</p> </li> <li>Let label <code>l</code> be the root of the dominator tree \\(T\\).</li> <li>Let <code>vars</code> be an empty list</li> <li>Match <code>P(l)</code> with<ol> <li>case <code>l: phis r &lt;- s</code><ol> <li><code>(phis', K, result_list) = processphi(phis, K, vars)</code></li> <li><code>s' = ren(K, s)</code></li> <li>set <code>Q(l)</code> to <code>l: phis' r &lt;- s'</code></li> </ol> </li> <li>case <code>l: phis r &lt;- s1 op s2</code><ol> <li><code>(phis', K, result_list) = processphi(phis, K, vars)</code></li> <li><code>s1' = ren(K, s1)</code></li> <li><code>s2' = ren(K, s2)</code></li> <li>set <code>Q(l)</code> to  <code>l: phis' r &lt;- s1' op s2'</code></li> </ol> </li> <li>case <code>l: phis x &lt;- s</code><ol> <li><code>(phis', K, result_list) = processphi(phis, K, vars)</code></li> <li><code>s' = ren(K, s)</code></li> <li><code>i = next(K,x)</code></li> <li>append <code>x</code> to <code>vars</code></li> <li>set <code>Q(l)</code> to <code>l: phis' x_i &lt;- s'</code></li> </ol> </li> <li>case <code>l: phis x &lt;- s1 op s2</code><ol> <li><code>(phis', K, result_list) = processphi(phis, K, vars)</code></li> <li><code>s1' = ren(K, s1)</code></li> <li><code>s2' = ren(K, s2)</code></li> <li><code>i = next(K,x)</code></li> <li>append <code>x</code> to <code>vars</code></li> <li>set <code>Q(l)</code> to  <code>l: phis' x_i &lt;- s1' op s2'</code></li> </ol> </li> <li>case <code>l: phis ifn t goto l'</code> <ol> <li><code>(phis', K, result_list) = processphi(phis, K, vars)</code></li> <li><code>t' = ren(K, t)</code></li> <li>set <code>Q(l)</code> to  <code>l: phis' ifn t' goto l'</code> </li> </ol> </li> <li>case <code>l: phis ret</code><ol> <li><code>(phis', K, result_list) = processphi(phis, K, vars)</code></li> <li>set <code>Q(l)</code> to <code>l: phis' ret</code></li> </ol> </li> <li>case <code>l: phis goto l'</code><ol> <li><code>(phis', K, result_list) = processphi(phis, K, vars)</code></li> <li>set <code>Q(l)</code> to <code>l: phis' goto l'</code></li> </ol> </li> </ol> </li> <li>For each successor <code>k</code> of <code>l</code> in the CFG \\(G\\)<ol> <li><code>R = if k in Q { Q } else { R }</code></li> <li>Pattern match <code>R(k)</code> <ol> <li>case <code>k: phis i</code><ol> <li>for each <code>x &lt;- phi(j:x', m:x'')</code> in <code>phis</code><ol> <li>if <code>K(origin(x))</code> is empty, do not add this phi assignment in the result list**.</li> <li>if <code>j == l</code>, <code>x &lt;- phi(j:ren(K,x'), m:x'')</code> into the result list</li> <li>if <code>m == l</code>, <code>x &lt;- phi(j:x', m:ren(K,x''))</code> into the result list</li> </ol> </li> <li>the result list is <code>phis'</code></li> <li>update <code>R(k)</code> to  <code>k: phis' i</code> </li> </ol> </li> <li>case <code>others</code>, no change</li> </ol> </li> </ol> </li> <li>Recursively apply step 3 to the children of <code>l</code> in the \\(T\\).</li> <li>For each <code>x</code> in <code>vars</code>, <code>K(x).pop()</code></li> </ul> <p>Where <code>ren(K, s)</code> is defined as         </p> <pre><code>ren(K,c) = c\nren(K, input) = input\nren(K, r) = r\nren(K, t) = K(t).peek() match \n    case None =&gt; error(\"variable use before being defined.\")\n    case Some(i) =&gt; t_i\n</code></pre> <p>and <code>next(K, x)</code> is defined as </p> <pre><code>next(K, x) = K(x).peek() match \n    case None =&gt; \n        K(x).push(1)\n        0\n    case Some(i) =&gt;\n        K(x).push(i+1)\n        i\n</code></pre> <p>and <code>processphi(phis, K)</code> is defined as  <pre><code>prcessphi(phis, K, vars) = \n    foreach x &lt;- phi(j:x', k:x'') in phis\n        i = K(x).peek() + 1\n        K(x).push(i)\n        append x to vars\n        put x_i &lt;- phi(j:x', k:x'') into result_list\n    return (result_list, K, vars)\n</code></pre></p> <p>and <code>stem(x)</code> returns the original version of <code>x</code> before renaming, e.g. <code>stem(x) = x</code> and <code>stem(x1) = x</code>. We assume there exists some book-keeping mechanism to keep track of that the fact that <code>x</code> is the origin form of <code>x_1</code>.</p> <p>Note on **: In Cytron's paper, all variables must be initialized in the starting vertex of the program. This is not the case in our context. A temp variable can be created to handle nested binary operation, it is might not be initialized. This can be fixed by skipping any phi-assignment of which one of the preceding branch has no such variable assigned. This is sound as this would means  * The phi-assignment is not needed, in case of while statement where the variable is introduced in the while body, or  * The phi-assignment is not fully initialized, in case of if-else where the variable is only introduced in one of the branch.</p> <p>We describe the application the algorithm to <code>PRE_SSA_PA1</code> (with the dominator tree <code>Tree2_PA1</code> and CFG <code>Graph1_PA1</code>) with the following table.</p> label P(l) Q(l) K P(succ(l)) Q(succ(l)) vars 1 <code>1:x&lt;-input</code> <code>1:x0&lt;-input</code> <code>{x:[0], s:[], c:[], t:[]}</code> <code>{1:{x}}</code> 2 <code>2:s&lt;-0</code> <code>2:s0&lt;-0</code> <code>{x:[0], s:[0], c:[], t:[]}</code> <code>{1:{x}, 2:{s}}</code> 3 <code>3:c&lt;-0</code> <code>3:c0&lt;-0</code> <code>{x:[0], s:[0], c:[0], t:[]}</code> <code>4:s&lt;-phi(3:s0,8:s);c&lt;-phi(3:c0,8:c);t&lt;-c&lt;x</code> <code>{1:{x}, 2:{s}. 3:{c}}</code> 4 <code>4:s&lt;-phi(3:s0,8:s);c&lt;-phi(3:c0,8:c);t&lt;-c&lt;x</code> <code>4:s1&lt;-phi(3:s0,8:s);c1&lt;-phi(3:c0,8:c);t0&lt;-c1&lt;x0</code> <code>{x:[0], s:[0,1], c:[0,1], t:[0]}</code> <code>{1:{x}, 2:{s}. 3:{c}, 4:{s,c,t}}</code> 5 <code>5:ifn t goto 9</code> <code>5:ifn t0 goto 9</code> <code>{x:[0], s:[0,1], c:[0,1], t:[0]}</code> <code>{1:{x}, 2:{s}. 3:{c}, 4:{s,c,t}}</code> 6 <code>6:s&lt;-c+s</code> <code>6:s2&lt;-c1+s1</code> <code>{x:[0], s:[0,1,2], c:[0,1], t:[0]}</code> <code>{1:{x}, 2:{s}. 3:{c}, 4:{s,c,t}, 6:{s}}</code> 7 <code>7:c&lt;-c+1</code> <code>7:c2&lt;-c1+1</code> <code>{x:[0], s:[0,1,2], c:[0,1,2], t:[0]}</code> <code>{1:{x}, 2:{s}. 3:{c}, 4:{s,c,t}, 6:{s}, 7:{c}}</code> 8 <code>8:goto 4</code> <code>8:goto 4</code> <code>{x:[0], s:[0,1,2], c:[0,1,2], t:[0]}</code> <code>4:s1&lt;-phi(3:s0,8:s2);c1&lt;-phi(3:c0,8:c2);t0&lt;-c1&lt;x0</code> 9 <code>9:rret&lt;-s</code> <code>9:rret&lt;-s1</code> <code>{x:[0], s:[0,1], c:[0,1], t:[0]}</code> <code>{1:{x}, 2:{s}. 3:{c}, 4:{s,c,t}}</code> 10 <code>10:ret</code> <code>10:ret</code> <code>{x:[0], s:[0,1], c:[0,1], t:[0]}</code> <code>{1:{x}, 2:{s}. 3:{c}, 4:{s,c,t}}</code> <ul> <li>The label column denotes the current label  being considered.</li> <li>The P(l) column denotes the input labeled instruction being considered.</li> <li>The Q(l) column denotes the output labeled instruction.</li> <li>The K column denotes the set of stacks after the current recursive call.</li> <li>The P(succ(l)) column denotes the modified successor intruction in P, (this applies only when the instruction is not yet available in Q)</li> <li>The Q(succ(l)) column denotes the modified successor instruction in Q.</li> <li>The vars column denotes a mapping of recursive call (indexed by the current label) to the set of variables' ids have been generated (which require popping at the end of the recursive call).</li> </ul> <p>The above derivation eventually yield <code>SSA_PA1</code>.</p> <p>Note that in case of a variable being use before initialized, <code>ren(K, t)</code> will raise an error. </p>"},{"location":"notes/name_analysis/#ssa-back-to-pseudo-assembly","title":"SSA back to Pseudo Assembly","text":"<p>To convert a SSA back to Pseudo Assembly, we have to \"resolve\" the phi-assignments to by moving the branch-dependent assignment back to the preceding labeled instruction. For instance, translating <code>SSA_PA1</code> back to PA while keeping the renamed variables, we have </p> <pre><code>// PA2\n1: x0 &lt;- input\n2: s0 &lt;- 0\n3: c0 &lt;- 0\n3: s1 &lt;- s0\n3: c1 &lt;- c0\n4: t0 &lt;- c1 &lt; x0\n5: ifn t0 goto 9\n6: s2 &lt;- c1 + s1\n7: c2 &lt;- c1 + 1\n8: s1 &lt;- s2\n8: c1 &lt;- c2\n8: goto 4\n9: rret &lt;- s1 \n10: ret\n</code></pre> <p>In the above we break the phi-assignments found in </p> <pre><code>4: s1 &lt;- phi(3:s0, 8:s2)\n   c1 &lt;- phi(3:c0, 8:c2)\n   t0 &lt;- c1 &lt; x0\n</code></pre> <p>into </p> <p><code>PhiFor3</code> <pre><code>s1 &lt;- s0 // for label 3\nc1 &lt;- c0\n</code></pre></p> <p>and </p> <p><code>PhiFor8</code> <pre><code>s1 &lt;- s2 // for label 8\nc1 &lt;- c2\n</code></pre></p> <p>We move <code>PhiFor3</code> to label 3</p> <pre><code>3:  c0 &lt;- 0\n3:  s1 &lt;- s0\n3:  c1 &lt;- c0\n</code></pre> <p>and <code>PhiFor8</code> to label 8</p> <pre><code>8: s1 &lt;- s2\n8: c1 &lt;- c2\n8: goto 4\n</code></pre> <p>The \"moving\" phi-assignment operation can be defined in the following algorithm.</p>"},{"location":"notes/name_analysis/#relocating-the-phi-assignments","title":"Relocating the phi-assignments","text":"<p>Input: a PA program \\(P\\) being viewed as a list of labeled instructions. Output: a PA program \\(Q\\) being viewed as a list of labeled instructions. (duplicate labels are allowed in \\(Q\\))</p> <ol> <li>For each \\(l: \\overline{\\phi}\\ i \\in P\\), append \\(l: i\\) to \\(Q\\).</li> <li>For each \\(l: \\overline{\\phi}\\ i\\).<ol> <li>For each <code>x = phi(l1:x1, l2:x2)</code> in \\(\\overline{\\phi}\\)<ol> <li>append <code>l1:x &lt;- x1</code> and <code>l2:x &lt;- x2</code> to \\(Q\\).</li> <li>note that the relocated assignment must be placed before the control flow transition from <code>l1</code> to <code>succ(l1)</code> (and <code>l2</code> to <code>succ(l2)</code>) </li> </ol> </li> </ol> </li> <li>Sort \\(Q\\) by labels using a stable sorting algorithm.</li> </ol> <p>Now since there are repeated labels in <code>PA2</code>, we need an extra relabelling step to convert <code>PA2</code> to <code>PA3</code></p> <pre><code>// PA3\n1: x0 &lt;- input\n2: s0 &lt;- 0\n3: c0 &lt;- 0\n4: s1 &lt;- s0\n5: c1 &lt;- c0\n6: t0 &lt;- c1 &lt; x0\n7: ifn t0 goto 11\n8: s2 &lt;- c1 + s1\n9: c2 &lt;- c1 + 1\n10: s1 &lt;- s2\n11: c1 &lt;- c2\n12: goto 4\n13: rret &lt;- s1 \n14: ret\n</code></pre>"},{"location":"notes/name_analysis/#relabelling","title":"Relabelling","text":"<p>This re-labeling step can be described in the following algorithm.</p> <p>Input: a PA program \\(P\\) being viewed as a list of labeled instructions. (duplicate labels are allowed in \\(P\\)) Output: a PA program \\(Q\\) being viewed as a list of labeled instructions.</p> <ol> <li>Initialize a counter <code>c = 1</code>,</li> <li>Initialize a mapping from old label to new label, <code>M = Map()</code>.</li> <li>Initialize \\(Q\\) as an empty list</li> <li>For each <code>l: i</code> \\(\\in P\\)<ol> <li><code>M = M + (l -&gt; c)</code></li> <li>incremeant <code>c</code> by 1</li> </ol> </li> <li>For each <code>l: i</code> \\(\\in P\\)<ol> <li>append <code>M(l): relabel(i, M)</code> to \\(Q\\)</li> </ol> </li> </ol> <p>where <code>relabel(i, M)</code> is defined as follows</p> <pre><code>relabel(ifn t goto l,M) = ifn t goto M(l)\nrelabel(goto l, M) = goto M(l)\nrelabel(i, M) = i\n</code></pre>"},{"location":"notes/name_analysis/#structured-ssa","title":"Structured SSA","text":"<p>Besides unstructured SSA, it is possible to construct SSA based on a structured program such as SSA. For instance, </p> <pre><code>x = input;\ns = 0;\nc = 0;\nwhile c &lt; x {\n    s = c + s;\n    c = c + 1;\n}\nreturn s;\n</code></pre> <p>Can be converted into a structured SSA </p> <pre><code>x1 = input;\ns1 = 0;\nc1 = 0;\njoin { s2 = phi(s1,s3); c2 = phi(c1,c3); } \nwhile c2 &lt; x1 {\n    s3 = c2 + s2;\n    c3 = c2 + 1;\n}\nreturn s2;\n</code></pre> <p>In the above SSA form, we have a <code>join ... while ...</code> loop.  The join clause encloses the phi assignments merging variable definitions coming from the statement preceding the join while loop and  also the body of the loop.  (Similarly we can introduce a <code>if ... else ... join ...</code> statement).</p> <p>Structured SSA allows us to </p> <ol> <li>conduct name analysis closer to the source language. </li> <li> <p>conduct flow insensitive analysis by incorporating the use-def information. In some cases we get same precision as the flow sensitive analysis. </p> </li> <li> <p>perform code obfuscation. </p> </li> </ol>"},{"location":"notes/name_analysis/#futher-readings","title":"Futher Readings","text":"<ul> <li>https://dl.acm.org/doi/10.1145/2955811.2955813</li> <li>https://dl.acm.org/doi/abs/10.1145/3605156.3606457</li> <li>https://dl.acm.org/doi/10.1145/202530.202532</li> </ul>"},{"location":"notes/schedule/","title":"Schedule","text":"Week Session 1 Session 2 Session 3 Assessment 1 Intro FP: Expression, Function, Conditional, Recursion Cohort Problem 1,  Homework 1 Homework 1 no submission required. Please refer to the markdown file for instructions. 2 FP: List, Pattern Matching FP: Algebraic Data Type Cohort Problem 2,  Homework 2 3 FP: Generics, GADT FP: Type Classes, Functor Cohort Problem 3,  Homework 2 (Cont'd) Homework 2 5% 4 FP: Applicative FP: Monad Cohort Problem 4,  Homework 3 5 Syntax Analysis: Lexing, Parsing Top-down Parsing Cohort Problem 5,  Homework 3 (Cont'd) Homework 3 5% 6 Parser-Combinator IR: Pseudo-Assembly Cohort Problem 5 Cont'd,  Homework 4,  Project Briefing Bottom-up Parsing for self-study. it won't show up in exams. 7 Homework 4 5% 8 1. Semantic Analysis 2. Dynamic Semantics Mid-term exam Cohort Problem 7 Mid-term 10% 9 Static Semantics for SIMP Static Semantics for Lambda Calculus Cohort Problem 8, Homework 5 Project Lab 1 10% 10 Name Analysis, SSA Lattice, Sign Analysis Cohort Problem 9: Name Analysis 11 Liveness Analysis Code Generation Cohort Problem 10: Sign Analysis Project Lab 2 10%,  Homework 5 5% 12 Information Flow Analysis Memory Management Cohort Problem 11 13 Guest Lecture Revision Project Lab 3 15% 14 Final Exam (19 Dec Thu 3:00PM-5:00PM) 30%"},{"location":"notes/semantic_analysis/","title":"50.054 - Semantic Analysis","text":""},{"location":"notes/semantic_analysis/#learning-outcomes","title":"Learning Outcomes","text":"<ol> <li>Articulate the meaning of program semantics</li> <li>List different types of program semantics.</li> <li>Explain the limitation of static analysis. </li> </ol>"},{"location":"notes/semantic_analysis/#what-is-program-semantic","title":"What is Program Semantic","text":"<p>In contrast to program syntax which defines the validity of a program, the program semantics define the behavior of a program.</p>"},{"location":"notes/semantic_analysis/#dynamic-semantics","title":"Dynamic Semantics","text":"<p>Dynamic Semantics defines the meaning and behaviors of the given program. The term \"behavior\" could mean</p> <ol> <li>How does the program get executed?</li> <li>What does the program compute / return?</li> </ol>"},{"location":"notes/semantic_analysis/#static-semantics","title":"Static Semantics","text":"<p>Static Semantics describes a set of properties that the given program holds. For example, a typing system (a kind of static semantics) ensures that a well-typed program is free of run-time type errors such as using a string variable in the context of an if condition expression, or adding a float value to a character value.</p>"},{"location":"notes/semantic_analysis/#semantics-analysis","title":"Semantics Analysis","text":"<p>Recall the compiler pipeline</p> <pre><code>graph LR\nA[Lexing] --&gt;B[Parsing] --&gt; C[Semantic Analysis] --&gt; D[Optimization] --&gt; E[Target Code Generation]</code></pre> <p>But in fact it could be</p> <pre><code>graph LR\nA[Lexing] --&gt;B[Parsing] --&gt; C[Semantic Analysis] --&gt; D[Optimization] --&gt; E[Target Code Generation]\nD --&gt; C</code></pre> <ul> <li>Lexing</li> <li>Input: Source file in String</li> <li>Output: A sequence of valid tokens according to the language specification (grammar)</li> <li>Parsing</li> <li>Input: Output from the Lexer</li> <li>Output: A parse tree representing parsed result according to the parse derivation</li> <li>Semantic Analysis</li> <li>Input: A parse tree or an internal representation<ul> <li>a source parse tree is considered an internal representation</li> </ul> </li> <li>Output:<ul> <li>if succeeds, a parse tree or an internal representation</li> <li>otherwise, an error report</li> </ul> </li> </ul>"},{"location":"notes/semantic_analysis/#goal-of-semantic-analysis","title":"Goal of Semantic Analysis","text":"<p>There mainly two goals of semantic analysis.</p>"},{"location":"notes/semantic_analysis/#optimization","title":"Optimization","text":"<pre><code>x = input;\ny = 0;\ns = 0;\nwhile (y &lt; x) { \n    y = y + 1;\n    t = s;  // t is not used.\n    s = s + y;  \n}\nreturn s;\n</code></pre>"},{"location":"notes/semantic_analysis/#fault-detection","title":"Fault Detection","text":"<pre><code>x = input; \n\nwhile (x &gt;= 0) {\n    x = x - 1;\n}\ny = Math.sqrt(x); // error, can't apply sqrt() to a negative number.\nreturn y;\n</code></pre>"},{"location":"notes/semantic_analysis/#dynamic-semantics-analysis","title":"Dynamic Semantics Analysis","text":"<p>Dynamic semantics analysis aims to find faults and ascertains quality by supplying actual inputs to the target programs. The following are some of the commony used techniques, (we have learned some of them in other modules).</p> <ol> <li>Testing</li> <li>Run-time verification - analyse the target programs with instrumentation by checking the logs and traces against its specification.</li> <li>Program Slicing - try to decompose a program into \"slices\", small units of codes, that exhibit the behaviors of interests.</li> </ol>"},{"location":"notes/semantic_analysis/#static-semantic-analysis","title":"Static Semantic Analysis","text":"<p>Static Semantic Analysis focuses on achieving the same goal as dynamic semantic analysis by analysing the given program without actually running it.</p> <ol> <li>Type checking and type inference</li> <li>Control flow analysis - to determine the control flow graph of a given program. It gets harder has higher order function and function pointers introduced.</li> <li>Data flow analysis - the goal is determine the possible values being held by a variable at a particular program location.</li> <li>Model checking - given a specification, to reason the program's correctness using a math model, e.g. logic constraints.</li> </ol> <p>The advantage is that we gain some generality of the results without worry about the limitation of code coverage. The disadvantage is that we often loose accuracy through approximation</p>"},{"location":"notes/semantic_analysis/#limitation-of-static-semantic-analysis","title":"Limitation of Static Semantic Analysis","text":"<p>It follow Rice's theorem that all non-trivial semantic properties of programs are undecidable. i.e. there exists no algorithm that can decide all semantic properties for all given programs.</p> <p>For example, assume we can find an algorithm that determine whether the variable <code>x</code> in the following function is positive or negative without executing it. </p> <pre><code>def f(path):\n  p = open(path, \"r\")\n  x = 1\n  if eval(p):\n    x = -1\n  return x\n</code></pre> <p>In the above program the analysis of <code>x</code>'s sign (positive or negative) is subject to whether <code>eval(p)</code> is <code>true</code> or <code>false</code>. If such an algorithm exists, as a side effect we can also statically detect whether the given program in <code>path</code> is terminating, which is of course undecidable. </p>"},{"location":"notes/sign_analysis_lattice/","title":"50.054 - Sign Analysis and Lattice Theory","text":""},{"location":"notes/sign_analysis_lattice/#learning-outcomes","title":"Learning Outcomes","text":"<ol> <li>Explain the objective of Sign Analysis</li> <li>Define Lattice and Complete Lattice</li> <li>Define Monotonic Functions</li> <li>Explain the fixed point theorem</li> <li>Apply the fixed pointed theorem to solve equation constraints of sign analysis</li> </ol>"},{"location":"notes/sign_analysis_lattice/#recap","title":"Recap","text":"<p>Recall that one of the goals of semantic analyses is to detect faults without executing the program.</p> <pre><code>// SIMP1\nx = input;\nwhile (x &gt;= 0) {\n    x = x - 1;\n}\ny = Math.sqrt(x); // error, can't apply sqrt() to a negative number\nreturn y;\n</code></pre> <p>Note that our current SIMP syntax does not support <code>&gt;=</code>. We could extend both SIMP and Pseudo Assembly to support a new binary operator <code>||</code> so that we can <code>x&gt;=0</code> into <code>(x &gt; 0) || (x == 0)</code></p> <p>Note that for In Pseudo Assembly we use <code>0</code> to encode <code>false</code> and <code>1</code> to encode <code>true</code>. Hence <code>||</code> can be encoded as <code>+</code>.</p> <p>To detect that the application of <code>sqrt(x)</code> is causing an error, we could apply the sign analysis.</p>"},{"location":"notes/sign_analysis_lattice/#sign-analysis","title":"Sign Analysis","text":"<p>Sign Analysis is a static analysis which statically determines the possible signs of integer variables at the end of a statement in a program. For example </p> <pre><code>// SIMP1\nx = input;        // x could be +, - or 0\nwhile (x &gt;= 0) {  // x could be +, - or 0 \n    x = x - 1;    // x could be +, - or 0\n}                 // x must be -\ny = Math.sqrt(x); // x must be -, y could be +, - or 0\nreturn y;         // x must be -, y could be +, - or 0\n</code></pre> <p>We put the comments as the results of the analysis. </p>"},{"location":"notes/sign_analysis_lattice/#can-we-turn-sign-analysis-into-a-type-inference-problem","title":"Can we turn Sign Analysis into a type inference problem?","text":"<p>The answer is yes, but it is rather imprecise. Let's consider a simple example.</p> <pre><code>// SIMP2\nx = 0;\nx = x + 1;\nreturn x;\n</code></pre> <p>Suppose we introduce 3 subtypes of the <code>Int</code> type, namely <code>Zero</code>, <code>PosInt</code> and <code>NegInt</code></p> <ol> <li>The first statement, we infer <code>x</code> has type <code>Zero</code>.</li> <li>The second statement, we infer <code>x</code> on the RHS, has type <code>Int</code>, the LHS <code>x</code> has type <code>Int</code>. </li> </ol> <p>Unification would fail when we try to combine the result of <code>(x : Zero)</code> and <code>(x : Int)</code>. It is also unsound to conclude that <code>Zero</code> is the final type.</p> <p>This is because the type inference algorithm is a flow-insensitive analysis, which does not take into account that the program is executed from top to bottom.</p>"},{"location":"notes/sign_analysis_lattice/#abstract-domain","title":"Abstract Domain","text":"<p>To analyse the sign property of the variables statically, we could model the sign property using a set of values instead of sub-typing. </p> <p>For example, we may use </p> <ul> <li>\\(\\{\\}\\) to denote the empty set</li> <li>\\(+\\) to denote the set of all positive integers</li> <li>\\(-\\) to denote the set of all ngative integers</li> <li>\\(\\{0\\}\\) to denote the set containing <code>0</code></li> <li>\\(+ \\cup - \\cup \\{0\\}\\) to denote all integers .</li> </ul> <p>For convenience, let's use \\(\\bot\\) to denote \\(\\{\\}\\), \\(\\top\\) to denote \\(+ \\cup - \\cup \\{0\\}\\) and \\(0\\) to denote \\(\\{0\\}\\).  These symbols are the abstract values of the sign property. </p> <p>Since they are sets of values, we can define the subset relation among them.</p> \\[ \\begin{array}{c} \\bot \\subseteq 0  \\\\  \\bot \\subseteq +  \\\\  \\bot \\subseteq -  \\\\  0 \\subseteq \\top \\\\  {+} \\subseteq \\top \\\\  {-} \\subseteq \\top  \\end{array} \\] <p>If we put each abstract domain values in a graph we have the following graph <code>Graph1</code></p> <pre><code>graph\n    A[\"\u22a4\"]---B[-]\n    A---C[0]\n    A---D[+]\n    B---E\n    C---E\n    D---E[\u22a5]</code></pre> <p>informally the above graph structure is called a lattice in math. </p> <p>We will discuss the formal details of lattice shortly. For now let's consider applying the above abstract domain to analyse the sign property of <code>SIMP2</code>. For the ease of implementation we conduct the sign analysis on the Pseudo Assembly instead of SIMP. (The design choice of using Pseudo Assembly is to better align with the project of this module, it is possible to apply the same technique to the SIMP programs directly.)</p> <pre><code>// PA2        // x -&gt; top\n1: x &lt;- 0     // x -&gt; 0\n2: x &lt;- x + 1 // x -&gt; 0 ++ + -&gt; +\n3: rret &lt;- x  // x -&gt; +\n4: ret\n</code></pre> <p>we can follow the flow of the program, before the program starts, we assign \\(\\top\\) to <code>x</code>, as <code>x</code> could be any value. After instruction 1, we deduce that <code>x</code> must be having the abstract value <code>0</code>, since we assign <code>0</code> to <code>x</code>. After instruction 2, we deduce that <code>x</code> has the abstract value <code>+</code> because we add (<code>++</code>) <code>1</code> to an abstract value <code>0</code>. (Note that the <code>0</code>, <code>1</code> and <code>++</code> in the comments are abstract values and abstract operator. Their overloaded definition will be discussed later in this unit.) For simplicity, we ignore the sign analysis for special variable <code>input</code> (which is always \\(\\top\\)) and the register <code>rret</code> (whose sign is not useful.)</p> <p>Let's consider another example </p> <pre><code>// PA3                // x -&gt; top, t -&gt; top\n1: x &lt;- 0             // x -&gt; 0, t -&gt; top\n2: t &lt;- input &lt; 0     // x -&gt; 0, t -&gt; top\n3: ifn t goto 6       // x -&gt; 0, t -&gt; top\n4: x &lt;- x + 1         // x -&gt; +, t -&gt; top\n5: goto 6             // x -&gt; +, t -&gt; top\n6: rret &lt;- x          // x -&gt; upperbound(+, 0) -&gt; top, t -&gt; top\n7: ret                \n</code></pre> <p>We start off by assigning \\(\\top\\) to <code>x</code>, then <code>0</code> to <code>x</code> at the instruction 1. At instruction 2, we assign the result of the boolean condition to <code>t</code> which could be 0 or 1 hence <code>top</code> is the abstract value associated with <code>t</code>. Instruction 3 is a conditional jump. Instruction 4 is the then-branch, we update <code>x</code>'s sign to <code>+</code>. Instruction 6 is the end of the if-else statement, where we need to merge the two possibility of <code>x</code>'s sign. If <code>t</code>'s value is 0, <code>x</code>'s sign is <code>0</code>, otherwise <code>x</code>'s sign is <code>+</code>. Hence we take the upperbound of <code>+</code>, <code>0</code> according to <code>Graph1</code> which is \\(\\top\\).</p> <p>Let's consider the formalism of the lattice and this approach we just presented. </p>"},{"location":"notes/sign_analysis_lattice/#lattice-theory","title":"Lattice Theory","text":""},{"location":"notes/sign_analysis_lattice/#definition-1-partial-order","title":"Definition 1 - Partial Order","text":"<p>A set \\(S\\) is a partial order iff there exists a binary relation \\(\\sqsubseteq\\) with the following condition.</p> <ol> <li>reflexivity: \\(\\forall x \\in S, x \\sqsubseteq x\\)</li> <li>transitivity: \\(\\forall x,y,z \\in S, x \\sqsubseteq y \\wedge y\\sqsubseteq z\\) implies \\(x \\sqsubseteq z\\). </li> <li>anti-symmetry: \\(\\forall x,y \\in S, x \\sqsubseteq y \\wedge y\\sqsubseteq x\\) implies \\(x = y\\).</li> </ol> <p>For instance, the set of abstract values in <code>Graph1</code> forms a partial order if we define \\(x \\sqsubseteq y\\) as \"\\(x\\) is at least as precise than \\(y\\)\", (i.e. \\(x\\) is the same or more precise than \\(y\\)). </p>"},{"location":"notes/sign_analysis_lattice/#definition-2-upper-bound","title":"Definition 2 - Upper Bound","text":"<p>Let \\(S\\) be a partial order, and \\(T \\subseteq S\\), \\(y \\in S\\). We say \\(y\\) is an upper bound of \\(T\\) (written as \\(T\\sqsubseteq y\\)) iff \\(\\forall x \\in T, x \\sqsubseteq y\\). </p>"},{"location":"notes/sign_analysis_lattice/#definition-3-least-upper-bound","title":"Definition 3 - Least Upper Bound","text":"<p>Let \\(S\\) be a partial order, and \\(T \\subseteq S\\), \\(y \\in S\\), We say \\(y\\) is the least upper bound of \\(T\\) (written as \\(y = \\bigsqcup T\\)) iff \\(\\forall z \\in S, T \\sqsubseteq z\\) implies \\(y \\sqsubseteq z\\).</p> <p>For example, in <code>Graph1</code>, 0 is an upper bound of \\(\\{\\bot\\}\\), but it is not a least upper bound. \\(\\top\\) is a least upper bound of \\(\\{+, - ,0, \\bot\\}\\).</p>"},{"location":"notes/sign_analysis_lattice/#definition-4-lower-bound","title":"Definition 4 - Lower Bound","text":"<p>Let \\(S\\) be a partial order, and \\(T \\subseteq S\\), \\(y \\in S\\). We say \\(y\\) is a lower bound of \\(T\\) (written as \\(y\\sqsubseteq T\\)) iff \\(\\forall x \\in T, y \\sqsubseteq x\\). </p>"},{"location":"notes/sign_analysis_lattice/#definition-5-greatest-lower-bound","title":"Definition 5 - Greatest Lower Bound","text":"<p>Let \\(S\\) be a partial order, and \\(T \\subseteq S\\), \\(y \\in S\\), We say \\(y\\) is the greatest lower bound of \\(T\\) (written as \\(y = {\\Large \\sqcap} T\\)) iff \\(\\forall z \\in S, z \\sqsubseteq T\\) implies \\(z \\sqsubseteq y\\).</p> <p>For example, in <code>Graph2</code>, 0 is a lower bound of \\(\\{\\top\\}\\), but it is not a greatest lower bound. \\(\\bot\\) is a greatest lower bound of \\(\\{+, - ,0, \\top\\}\\).</p>"},{"location":"notes/sign_analysis_lattice/#definition-6-join-and-meet","title":"Definition 6 - Join and Meet","text":"<p>Let \\(S\\) be a partial order, and \\(x, y \\in S\\). </p> <ol> <li>We define the join of \\(x\\) and \\(y\\) as \\(x \\sqcup y = \\bigsqcup \\{x, y\\}\\). </li> <li>We define the meet of \\(x\\) and \\(y\\) as \\(x \\sqcap y = {\\Large \\sqcap} \\{x, y\\}\\). </li> </ol>"},{"location":"notes/sign_analysis_lattice/#definition-7-lattice","title":"Definition 7 - Lattice","text":"<p>A partial order \\((S, \\sqsubseteq)\\) is a lattice iff \\(\\forall x, y\\in S\\), \\(x \\sqcup y\\) and \\(x \\sqcap y\\) exist.</p>"},{"location":"notes/sign_analysis_lattice/#definition-8-complete-lattice-and-semi-lattice","title":"Definition 8 - Complete Lattice and Semi-Lattice","text":"<p>A partial order \\((S, \\sqsubseteq)\\) is a complete lattice iff \\(\\forall X \\subseteq S\\), \\(\\bigsqcup X\\) and \\({\\Large \\sqcap} X\\) exist.</p> <p>A partial order \\((S, \\sqsubseteq)\\) is a join semilattice iff \\(\\forall X \\subseteq S\\), \\(\\bigsqcup X\\) exists.</p> <p>A partial order \\((S, \\sqsubseteq)\\) is a meet semilattice iff \\(\\forall X \\subseteq S\\), \\({\\Large \\sqcap} X\\) exists.</p> <p>For example the set of abstract values in <code>Graph1</code> and the \"as least as precise\" relation \\(\\sqsubseteq\\) form a complete lattice. </p> <p><code>Graph1</code> is the Hasse diagram of this complete lattice.</p>"},{"location":"notes/sign_analysis_lattice/#lemma-9","title":"Lemma 9","text":"<p>Let \\(S\\) be a non empty finite set and \\((S, \\sqsubseteq)\\) is a lattice, then \\((S, \\sqsubseteq)\\) is a complete lattice.</p> <p>In the next few subsections, we introduce a few commonly use lattices. </p>"},{"location":"notes/sign_analysis_lattice/#powerset-lattice","title":"Powerset Lattice","text":"<p>Let \\(A\\) be a set. We write \\({\\cal P}(A)\\) to denote the powerset of \\(A\\). Then \\(({\\cal P}(A), \\subseteq)\\) forms a complete lattice.  We call it powerset lattice. </p> <p>The above is valid because when we define \\(\\sqsubseteq = \\subseteq\\) and each abstract element in \\({\\cal P}(A)\\), we find that for any \\(T \\subseteq {\\cal P}(A)\\). \\(\\bigsqcup T = \\bigcup_{x \\in T} x\\) and \\({\\Large \\sqcap} T = \\bigcap_{x \\in T} x\\).</p> <p>Can you show that the power set of <code>{1,2,3,4}</code> and \\(\\subseteq\\) form a complete lattice? What is the \\(\\top\\) element and what is the \\(\\bot\\) element? Can you draw the diagaram?</p>"},{"location":"notes/sign_analysis_lattice/#product-lattice","title":"Product Lattice","text":"<p>Let \\(L_1,...,L_n\\) be complete lattices, then \\((L_1 \\times ... \\times  L_n)\\) is a complete lattice where the \\(\\sqsubseteq\\) is defined as </p> \\[ (x_1, ..., x_n) \\sqsubseteq (y_1, ..., y_n)\\ {\\tt iff}\\ \\forall i \\in [1,n], x_i \\sqsubseteq y_i \\] <p>We sometimes write \\(L^n\\) as a short-hand for \\((L_1 \\times ... \\times  L_n)\\).</p> <p>For example in <code>PA3</code>, to analyse the signs for variables we need two lattices, one for variable <code>x</code> and the other for variable <code>t</code>, which forms a product lattice. \\(Sign \\times Sign\\) where \\(Sign\\) is a complete lattice is defined as \\((\\{ \\bot, \\top, 0, + , -\\}, \\sqsubseteq)\\).</p> <pre><code>graph TD;\n    tt[\"(\u22a4,\u22a4)\"] --- t+[\"(\u22a4,+)\"]\n    tt[\"(\u22a4,\u22a4)\"] --- t0[\"(\u22a4,0)\"]\n    tt[\"(\u22a4,\u22a4)\"] --- tm[\"(\u22a4,-)\"]\n    tt[\"(\u22a4,\u22a4)\"] --- +t[\"(+,\u22a4)\"]\n    tt[\"(\u22a4,\u22a4)\"] --- 0t[\"(0,\u22a4)\"]\n    tt[\"(\u22a4,\u22a4)\"] --- mt[\"(-,\u22a4)\"]\n    t+[\"(\u22a4,+)\"] --- tb[\"(\u22a4,\u22a5)\"]\n    t+[\"(\u22a4,+)\"] --- ++[\"(+,+)\"]\n    t+[\"(\u22a4,+)\"] --- 0+[\"(0,+)\"]\n    t+[\"(\u22a4,+)\"] --- m+[\"(-,+)\"]\n    t0[\"(\u22a4,0)\"] --- tb[\"(\u22a4,\u22a5)\"]\n    t0[\"(\u22a4,0)\"] --- 00[\"(0,0)\"]\n    t0[\"(\u22a4,0)\"] --- +0[\"(+,0)\"]\n    t0[\"(\u22a4,0)\"] --- m0[\"(-,0)\"]\n    tm[\"(\u22a4,-)\"] --- tb[\"(\u22a4,\u22a5)\"]\n    tm[\"(\u22a4,-)\"] --- +m[\"(+,-)\"]\n    tm[\"(\u22a4,-)\"] --- 0m[\"(0,-)\"]\n    tm[\"(\u22a4,-)\"] --- mm[\"(-,-)\"]\n    +t[\"(+,\u22a4)\"] --- bt[\"(\u22a5,\u22a4)\"]\n    +t[\"(+,\u22a4)\"] --- ++[\"(+,+)\"]\n    +t[\"(+,\u22a4)\"] --- +0[\"(+,0)\"]\n    +t[\"(+,\u22a4)\"] --- +m[\"(+,-)\"]\n    0t[\"(0,\u22a4)\"] --- bt[\"(\u22a5,\u22a4)\"]\n    0t[\"(0,\u22a4)\"] --- 0+[\"(0,+)\"]\n    0t[\"(0,\u22a4)\"] --- 00[\"(0,0)\"]\n    0t[\"(0,\u22a4)\"] --- 0m[\"(0,-)\"]\n    mt[\"(-,\u22a4)\"] --- bt[\"(\u22a5,\u22a4)\"] \n    mt[\"(-,\u22a4)\"] --- m+[\"(-,+)\"] \n    mt[\"(-,\u22a4)\"] --- m0[\"(-,0)\"] \n    mt[\"(-,\u22a4)\"] --- mm[\"(-,-)\"] \n    ++[\"(+,+)\"] --- b+[\"(\u22a5,+)\"]\n    ++[\"(+,+)\"] --- +b[\"(+,\u22a5)\"]\n    0+[\"(0,+)\"] --- b+[\"(\u22a5,+)\"]\n    0+[\"(0,+)\"] --- 0b[\"(0,\u22a5)\"]\n    m+[\"(-,+)\"] --- b+[\"(\u22a5,+)\"]\n    m+[\"(-,+)\"] --- mb[\"(-,\u22a5)\"]\n    00[\"(0,0)\"] --- b0[\"(\u22a5,0)\"]\n    00[\"(0,0)\"] --- 0b[\"(0,\u22a5)\"]\n    +0[\"(+,0)\"] --- b0[\"(\u22a5,0)\"]\n    +0[\"(+,0)\"] --- +b[\"(+,\u22a5)\"]\n    m0[\"(-,0)\"] --- b0[\"(\u22a5,0)\"]\n    m0[\"(-,0)\"] --- mb[\"(-,\u22a5)\"]\n    +m[\"(+,-)\"] --- bm[\"(\u22a5,-)\"]\n    +m[\"(+,-)\"] --- +b[\"(+,\u22a5)\"]\n    0m[\"(0,-)\"] --- bm[\"(\u22a5,-)\"]\n    0m[\"(0,-)\"] --- 0b[\"(0,\u22a5)\"]\n    mm[\"(-,-)\"] --- bm[\"(\u22a5,-)\"] \n    mm[\"(-,-)\"] --- mb[\"(-,\u22a5)\"]\n    bt[\"(\u22a5,\u22a4)\"] --- b+[\"(\u22a5,+)\"]\n    bt[\"(\u22a5,\u22a4)\"] --- b0[\"(\u22a5,0)\"]\n    bt[\"(\u22a5,\u22a4)\"] --- bm[\"(\u22a5,-)\"]\n    b+[\"(\u22a5,+)\"] --- bb[\"(\u22a5,\u22a5)\"]\n    b0[\"(\u22a5,0)\"] --- bb[\"(\u22a5,\u22a5)\"]\n    bm[\"(\u22a5,-)\"] --- bb[\"(\u22a5,\u22a5)\"]\n    tb[\"(\u22a4,\u22a5)\"] --- +b[\"(+,\u22a5)\"]\n    tb[\"(\u22a4,\u22a5)\"] --- 0b[\"(0,\u22a5)\"]\n    tb[\"(\u22a4,\u22a5)\"] --- mb[\"(-,\u22a5)\"]\n    +b[\"(+,\u22a5)\"] --- bb[\"(\u22a5,\u22a5)\"]\n    0b[\"(0,\u22a5)\"] --- bb[\"(\u22a5,\u22a5)\"]\n    mb[\"(-,\u22a5)\"] --- bb[\"(\u22a5,\u22a5)\"]</code></pre>"},{"location":"notes/sign_analysis_lattice/#map-lattice","title":"Map Lattice","text":"<p>Let \\(L\\) be a complete lattice, \\(A\\) be a set. Let \\(A \\rightarrow L\\) denotes a set of functions </p> \\[ \\{ m \\mid x \\in A \\wedge m(x) \\in L \\} \\] <p>and the \\(\\sqsubseteq\\) relation among functions \\(m_1, m_2 \\in A \\rightarrow L\\) is defined as </p> \\[ m_1 \\sqsubseteq m_2\\ {\\tt iff}\\ \\forall x\\in A, m_1(x) \\sqsubseteq m_2(x) \\] <p>Then \\(A \\rightarrow L\\) is a complete lattice. </p> <p>Note that the term \"function\" used in this definition refers a math function. We could interpret it as a hash table or a Haskell <code>Data.Map.Map a l</code> object where elements of \\(a\\) are keys and elements of \\(l\\) are the values associated with the keys. </p> <p>Map lattice offers a compact alternative to lattices for sign analysis of variables in program like <code>PA3</code> when there are many variables. </p> <p>We can define a map lattice consisting of functions that map variables (<code>x</code> or <code>t</code>) to abstract values in the complete lattice of \\((\\{ \\bot, \\top, 0, + , -\\}, \\sqsubseteq)\\). </p> <p>For instance, one of the element \"functions\" in the above-mentioned map lattice could be </p> \\[ m_1 = [ x \\mapsto \\top, t \\mapsto + ]  \\] <p>another element function could be </p> \\[ m_2 = [ x \\mapsto \\top, t \\mapsto \\top ]  \\] <p>We conclude that \\(m_1\\sqsubseteq m_2\\). Let \\(Var\\) denote the set of all variables, and \\(Sign\\) denote the complete lattice \\((\\{ \\bot, \\top, 0, + , -\\}, \\sqsubseteq)\\). <code>m1</code> and <code>m2</code> are elements of the complete lattice \\(Var \\rightarrow Sign\\)</p> <pre><code>graph TD;\n    tt[\"(x\u2192\u22a4,t\u2192\u22a4)\"] --- t+[\"(x\u2192\u22a4,t\u2192+)\"]\n    tt[\"(x\u2192\u22a4,t\u2192\u22a4)\"] --- t0[\"(x\u2192\u22a4,t\u21920)\"]\n    tt[\"(x\u2192\u22a4,t\u2192\u22a4)\"] --- tm[\"(x\u2192\u22a4,t\u2192-)\"]\n    tt[\"(x\u2192\u22a4,t\u2192\u22a4)\"] --- +t[\"(x\u2192+,t\u2192\u22a4)\"]\n    tt[\"(x\u2192\u22a4,t\u2192\u22a4)\"] --- 0t[\"(x\u21920,t\u2192\u22a4)\"]\n    tt[\"(x\u2192\u22a4,t\u2192\u22a4)\"] --- mt[\"(x\u2192-,t\u2192\u22a4)\"]\n    t+[\"(x\u2192\u22a4,t\u2192+)\"] --- tb[\"(x\u2192\u22a4,t\u2192\u22a5)\"]\n    t+[\"(x\u2192\u22a4,t\u2192+)\"] --- ++[\"(x\u2192+,t\u2192+)\"]\n    t+[\"(x\u2192\u22a4,t\u2192+)\"] --- 0+[\"(x\u21920,t\u2192+)\"]\n    t+[\"(x\u2192\u22a4,t\u2192+)\"] --- m+[\"(x\u2192-,t\u2192+)\"]\n    t0[\"(x\u2192\u22a4,t\u21920)\"] --- tb[\"(x\u2192\u22a4,t\u2192\u22a5)\"]\n    t0[\"(x\u2192\u22a4,t\u21920)\"] --- 00[\"(x\u21920,t\u21920)\"]\n    t0[\"(x\u2192\u22a4,t\u21920)\"] --- +0[\"(x\u2192+,t\u21920)\"]\n    t0[\"(x\u2192\u22a4,t\u21920)\"] --- m0[\"(x\u2192-,t\u21920)\"]\n    tm[\"(x\u2192\u22a4,t\u2192-)\"] --- tb[\"(x\u2192\u22a4,t\u2192\u22a5)\"]\n    tm[\"(x\u2192\u22a4,t\u2192-)\"] --- +m[\"(x\u2192+,t\u2192-)\"]\n    tm[\"(x\u2192\u22a4,t\u2192-)\"] --- 0m[\"(x\u21920,t\u2192-)\"]\n    tm[\"(x\u2192\u22a4,t\u2192-)\"] --- mm[\"(x\u2192-,t\u2192-)\"]\n    +t[\"(x\u2192+,t\u2192\u22a4)\"] --- bt[\"(x\u2192\u22a5,t\u2192\u22a4)\"]\n    +t[\"(x\u2192+, t\u2192\u22a4)\"] --- ++[\"(x\u2192+, t\u2192+)\"]\n    +t[\"(x\u2192+, t\u2192\u22a4)\"] --- +0[\"(x\u2192+, t\u21920)\"]\n    +t[\"(x\u2192+, t\u2192\u22a4)\"] --- +m[\"(x\u2192+, t\u2192-)\"]\n    0t[\"(x\u21920, t\u2192\u22a4)\"] --- bt[\"(x\u2192\u22a5,t\u2192\u22a4)\"]\n    0t[\"(x\u21920, t\u2192\u22a4)\"] --- 0+[\"(x\u21920, t\u2192+)\"]\n    0t[\"(x\u21920, t\u2192\u22a4)\"] --- 00[\"(x\u21920, t\u21920)\"]\n    0t[\"(x\u21920, t\u2192\u22a4)\"] --- 0m[\"(x\u21920, t\u2192-)\"]\n    mt[\"(x\u2192-, t\u2192\u22a4)\"] --- bt[\"(x\u2192\u22a5, t\u2192\u22a4)\"] \n    mt[\"(x\u2192-, t\u2192\u22a4)\"] --- m+[\"(x\u2192-, t\u2192+)\"] \n    mt[\"(x\u2192-, t\u2192\u22a4)\"] --- m0[\"(x\u2192-, t\u21920)\"] \n    mt[\"(x\u2192-, t\u2192\u22a4)\"] --- mm[\"(x\u2192-, t\u2192-)\"] \n    ++[\"(x\u2192+, t\u2192+)\"] --- b+[\"(x\u2192\u22a5, t\u2192+)\"]\n    ++[\"(x\u2192+, t\u2192+)\"] --- +b[\"(x\u2192+, t\u2192\u22a5)\"]\n    0+[\"(x\u21920, t\u2192+)\"] --- b+[\"(x\u2192\u22a5, t\u2192+)\"]\n    0+[\"(x\u21920, t\u2192+)\"] --- 0b[\"(x\u21920, t\u2192\u22a5)\"]\n    m+[\"(x\u2192-, t\u2192+)\"] --- b+[\"(x\u2192\u22a5, t\u2192+)\"]\n    m+[\"(x\u2192-, t\u2192+)\"] --- mb[\"(x\u2192-, t\u2192\u22a5)\"]\n    00[\"(x\u21920, t\u21920)\"] --- b0[\"(x\u2192\u22a5, t\u21920)\"]\n    00[\"(x\u21920, t\u21920)\"] --- 0b[\"(x\u21920, t\u2192\u22a5)\"]\n    +0[\"(x\u2192+, t\u21920)\"] --- b0[\"(x\u2192\u22a5, t\u21920)\"]\n    +0[\"(x\u2192+, t\u21920)\"] --- +b[\"(x\u2192+, t\u2192\u22a5)\"]\n    m0[\"(x\u2192-, t\u21920)\"] --- b0[\"(x\u2192\u22a5, t\u21920)\"]\n    m0[\"(x\u2192-, t\u21920)\"] --- mb[\"(x\u2192-, t\u2192\u22a5)\"]\n    +m[\"(x\u2192+, t\u2192-)\"] --- bm[\"(x\u2192\u22a5, t\u2192-)\"]\n    +m[\"(x\u2192+, t\u2192-)\"] --- +b[\"(x\u2192+, t\u2192\u22a5)\"]\n    0m[\"(x\u21920, t\u2192-)\"] --- bm[\"(x\u2192\u22a5, t\u2192-)\"]\n    0m[\"(x\u21920, t\u2192-)\"] --- 0b[\"(x\u21920, t\u2192\u22a5)\"]\n    mm[\"(x\u2192-, t\u2192-)\"] --- bm[\"(x\u2192\u22a5, t\u2192-)\"] \n    mm[\"(x\u2192-, t\u2192-)\"] --- mb[\"(x\u2192-, t\u2192\u22a5)\"]\n    bt[\"(x\u2192\u22a5, t\u2192\u22a4)\"] --- b+[\"(x\u2192\u22a5,t\u2192+)\"]\n    bt[\"(x\u2192\u22a5, t\u2192\u22a4)\"] --- b0[\"(x\u2192\u22a5,t\u21920)\"]\n    bt[\"(x\u2192\u22a5, t\u2192\u22a4)\"] --- bm[\"(x\u2192\u22a5,t\u2192-)\"]\n    b+[\"(x\u2192\u22a5, t\u2192+)\"] --- bb[\"(x\u2192\u22a5, t\u2192\u22a5)\"]\n    b0[\"(x\u2192\u22a5, t\u21920)\"] --- bb[\"(x\u2192\u22a5, t\u2192\u22a5)\"]\n    bm[\"(x\u2192\u22a5, t\u2192-)\"] --- bb[\"(x\u2192\u22a5, t\u2192\u22a5)\"]\n    tb[\"(x\u2192\u22a4, t\u2192\u22a5)\"] --- +b[\"(x\u2192+,t\u2192\u22a5)\"]\n    tb[\"(x\u2192\u22a4, t\u2192\u22a5)\"] --- 0b[\"(x\u21920,t\u2192\u22a5)\"]\n    tb[\"(x\u2192\u22a4, t\u2192\u22a5)\"] --- mb[\"(x\u2192-,t\u2192\u22a5)\"]\n    +b[\"(x\u2192+, t\u2192\u22a5)\"] --- bb[\"(x\u2192\u22a5, t\u2192\u22a5)\"]\n    0b[\"(x\u21920, t\u2192\u22a5)\"] --- bb[\"(x\u2192\u22a5, t\u2192\u22a5)\"]\n    mb[\"(x\u2192-, t\u2192\u22a5)\"] --- bb[\"(x\u2192\u22a5, t\u2192\u22a5)\"]</code></pre>"},{"location":"notes/sign_analysis_lattice/#sign-analysis-with-lattice","title":"Sign analysis with Lattice","text":"<p>As we informally elaborated earlier, the sign analysis approach \"infer\" the signs of the variables based on the \"previous states\" set by the previous statements.</p> <pre><code>// PA2         // s0 = [x -&gt; top]\n1: x &lt;- 0      // s1 = s0[x -&gt; 0]\n2: x = x + 1   // s2 = s1[x  -&gt; s1(x) ++ +]\n3: rret &lt;- x   // s3 = s2\n4: ret\n</code></pre> <p>In the above, we analyse <code>SIMP2</code> program's sign by \"packaging\" the variable to sign bindings into some state variables, <code>s1</code>, <code>s2</code>, <code>s3</code> and <code>s4</code>. Each state variable is mapping from variable to the abstract values from \\(\\{\\top, \\bot, +, -, 0\\}\\). Since \\(\\{\\top, \\bot, +, -, 0\\}\\) is a lattice, the set of state variables is a map lattice. </p> <p>Note that we could also model the state variables as a tuple of lattice as a produce lattice.</p> <p>Next we would like to model the change of variable signs based on the previous instructions. We write <code>s[x -&gt; v]</code> to denote a new state <code>s'</code> which is nearly the same as <code>s</code> except that the mapping of variable <code>x</code> is changed to <code>v.</code> (In Haskell style syntax, assuming <code>s</code> is a <code>Data.Map.Map Var Sign</code> object, then <code>s[x-&gt;v]</code> is actually <code>Data.Map.insert x v s</code> in Haskell.)</p> <p>We write <code>s(x)</code> to denote a query of variable <code>x</code>'s value in state <code>s</code>. (In Haskell style syntax, it is <code>case Data.Map.lookup x s of { Just v -&gt; v }</code>)</p> <p>In the above example, we define <code>s2</code> based on <code>s1</code> by \"updating\" variable <code>x</code>'s sign to <code>0</code>. We update <code>x</code>'s sign in <code>s3</code> based on <code>s2</code> by querying <code>x</code>'s sign in <code>s2</code> and modifying it by increasing by <code>1</code>. We define the <code>++</code> abstract operator for abstract values \\(\\{\\top, \\bot, +, -, 0\\}\\) as follows</p> ++ \\(\\top\\) + - 0 \\(\\bot\\) \\(\\top\\) \\(\\top\\) \\(\\top\\) \\(\\top\\) \\(\\top\\) \\(\\bot\\) + \\(\\top\\) + \\(\\top\\) + \\(\\bot\\) - \\(\\top\\) \\(\\top\\) - - \\(\\bot\\) 0 \\(\\top\\) + - 0 \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) <p>Where the first column from the 2<sup>nd</sup> rows onwards are the left operand and the first row from the 2<sup>nd</sup> column onwards are the right operand. Similarly we can define the other abstract operators</p> -- \\(\\top\\) + - 0 \\(\\bot\\) \\(\\top\\) \\(\\top\\) \\(\\top\\) \\(\\top\\) \\(\\top\\) \\(\\bot\\) + \\(\\top\\) \\(\\top\\) + + \\(\\bot\\) - \\(\\top\\) - \\(\\top\\) - \\(\\bot\\) 0 \\(\\top\\) - + 0 \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) ** \\(\\top\\) + - 0 \\(\\bot\\) \\(\\top\\) \\(\\top\\) \\(\\top\\) \\(\\top\\) 0 \\(\\bot\\) + \\(\\top\\) + - 0 \\(\\bot\\) - \\(\\top\\) - + 0 \\(\\bot\\) 0 0 0 0 0 \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) &lt;&lt; \\(\\top\\) + - 0 \\(\\bot\\) \\(\\top\\) \\(\\top\\) \\(\\top\\) \\(\\top\\) \\(\\top\\) \\(\\bot\\) + \\(\\top\\) \\(\\top\\) 0 0 \\(\\bot\\) - \\(\\top\\) + \\(\\top\\) + \\(\\bot\\) 0 \\(\\top\\) + 0 0 \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) <p>Given the definitions of the abstract operators, our next task is to solve the equation among the state variable <code>s0</code>, <code>s1</code>, <code>s2</code> and <code>s3</code></p> <pre><code>s0 = [x -&gt; top]\ns1 = s0[x -&gt; 0]\ns2 = s1[x  -&gt; s1(x) ++ +]\ns3 = s2\n</code></pre> <p>Note that we can't use unification here as <code>x</code> is assocated with different sign abstract values at different states (instructions).</p> <p>Question: If we use SSA PA instead of PA, can the generated equations be solved using unification?</p> <p>To solve the set of equation constraints we could process the equations from top to bottom.</p> <pre><code>s0 = [x -&gt; top]\ns1 = [x -&gt; 0]\ns2 = [x -&gt; +]\ns3 = [x -&gt; +]\n</code></pre> <p>Then we can conclude that the sign of variable <code>x</code> at instruction 3 is positive. Note that all the states, <code>s0</code>, <code>s1</code>, <code>s2</code> and <code>s3</code> are elements in the map lattice \\(Var \\rightarrow Sign\\).</p> <p>However, we need a more general solver as the equation systems could be recursive in the presence of loops.  For example.</p> <pre><code>// PA4              // s0 = [x -&gt; top, y -&gt; top, t -&gt; top]\n1: x &lt;- input       // s1 = s0\n2: y &lt;- 0           // s2 = s1[y -&gt; 0]\n3: t &lt;- x &gt; 0       // s3 = upperbound(s2,s7)[t -&gt; top]\n4: ifn t goto 8     // s4 = s3\n5: y &lt;- y + 1       // s5 = s4[y -&gt; s4(y) ++ +]\n6: x &lt;- x - 1       // s6 = s5[x -&gt; s5(x) -- +]\n7: goto 3           // s7 = s6\n8: rret &lt;- y        // s8 = s4\n9: ret \n</code></pre> <p>In the above the <code>upperbound(s, t)</code> can be define as \\(s \\sqcup t\\), assuming \\(s\\) and \\(t\\) are elements of a complete lattice. </p> <p>Note that all the states in the above analysis are elements of \\(Var \\rightarrow Sign\\), hence \\(s \\sqcup t\\) can be defined as </p> \\[ [ x\\mapsto s(x) \\sqcup t(x) \\mid x \\in Var ] \\] <p>To solve equation systems like the above, we need some \"special\" functions that operates on lattices.</p>"},{"location":"notes/sign_analysis_lattice/#definition-10-monotonic-function","title":"Definition 10 - Monotonic Function","text":"<p>Let \\(L_1\\) and \\(L_2\\) be lattices, a function \\(f : L_1 \\longrightarrow L_2\\) is monotonic iff \\(\\forall x,y \\in L_1, x \\sqsubseteq y\\) implies \\(f(x) \\sqsubseteq f(y)\\).</p> <p>Note that the term \"function\" in the above is can be treated as the function/method that we define in a programl.</p> <p>For instance given the lattice described in <code>Graph1</code>, we define the following function </p> \\[ \\begin{array}{rcl} f_1(x) &amp; = &amp; \\top \\end{array} \\] <p>Function \\(f_1\\) is monotonic because </p> \\[ \\begin{array}{r} f_1(\\bot) = \\top \\\\ f_1(0) = \\top \\\\  f_1(+) = \\top \\\\  f_1(-) = \\top \\\\  f_1(\\top) = \\top \\end{array}  \\] <p>and \\(\\top \\sqsubseteq \\top\\)</p> <p>Let's consider another function \\(f_2\\) </p> \\[ \\begin{array}{rcl} f_2(x) &amp; = &amp; x \\sqcup +   \\end{array} \\] <p>is \\(f_2\\) monotonic? Recall \\(\\sqcup\\) computes the least upper bound of the operands</p> \\[ \\begin{array}{r} f_2(\\bot) = \\bot \\sqcup + = + \\\\ f_2(0) = 0 \\sqcup + = \\top \\\\  f_2(+) = + \\sqcup + = + \\\\  f_2(-) = - \\sqcup + = \\top \\\\  f_2(\\top) = \\top \\sqcup + = \\top  \\end{array}  \\] <p>Note that </p> \\[ \\begin{array}{r} \\bot \\sqsubseteq + \\sqsubseteq \\top\\\\  \\bot \\sqsubseteq 0 \\sqsubseteq \\top\\\\  \\bot \\sqsubseteq - \\sqsubseteq \\top \\end{array} \\] <p>when we apply \\(g\\) to all the abstract values in the above inequalities, we find that </p> \\[ \\begin{array}{r} f_2(\\bot) \\sqsubseteq f_2(+) \\sqsubseteq f_2(\\top)\\\\  f_2(\\bot) \\sqsubseteq f_2(0) \\sqsubseteq f_2(\\top)\\\\  f_2(\\bot) \\sqsubseteq f_2(-) \\sqsubseteq f_2(\\top) \\end{array} \\] <p>hold. Therefore \\(g\\) is monotonic. </p> <p>Let \\(L\\) be a lattice and \\(L_1 \\times ... \\times L_n\\) be a product lattice. It follows from Definition 10 that \\(f : L_1 \\times ... \\times L_n \\rightarrow L\\) is monotone iff \\(\\forall (v_1, ..., v_n) \\sqsubseteq (v_1', ..., v_n')\\) imples \\(f (v_1, ..., v_n) \\sqsubseteq f  (v_1', ..., v_n')\\)</p>"},{"location":"notes/sign_analysis_lattice/#lemma-11-constant-function-is-monotonic","title":"Lemma 11 - Constant Function is Monotonic.","text":"<p>Every constant function \\(f\\) is monotonic.</p>"},{"location":"notes/sign_analysis_lattice/#lemma-12-sqcup-and-sqcap-are-monotonic","title":"Lemma 12 - \\(\\sqcup\\) and \\(\\sqcap\\) are Monotonic.","text":"<p>Let's treat \\(\\sqcup\\) as a function \\(L \\times L \\rightarrow L\\), then \\(\\sqcup\\) is monotonic.</p> <p>Similar observation applies to \\(\\sqcap\\).</p>"},{"location":"notes/sign_analysis_lattice/#definition-13-fixed-point-and-least-fixed-point","title":"Definition 13 - Fixed Point and Least Fixed Point","text":"<p>Let \\(L\\) be a lattice and \\(f: L \\rightarrow L\\) is be function. We say \\(x \\in L\\) is a fixed point of \\(f\\) iff \\(x = f(x)\\). We say \\(x\\) is a least fixed point of \\(f\\) iff \\(\\forall y \\in L\\), \\(y\\) is a fixed point of \\(f\\) implies \\(x \\sqsubseteq y\\).</p> <p>For example, for function \\(f_1\\), \\(\\top\\) is a fixed point and also the least fixed point. For function \\(f_2\\), \\(+\\), \\(\\top\\) are the fixed points and \\(+\\) is the least fixed point.</p>"},{"location":"notes/sign_analysis_lattice/#theorem-14-fixed-point-theorem","title":"Theorem 14 - Fixed Point Theorem","text":"<p>Let \\(L\\) be a complete lattice with finite height, every monotonic  function \\(f\\) has a unique least fixed point point, namely \\({\\tt lfp}(f)\\), defined as </p> \\[ {\\tt lfp}(f) = \\bigsqcup_{i\\geq 0}f^i(\\bot) \\] <p>Where \\(f^n(x)\\) is a short hand for </p> \\[ \\overbrace{f(...(f(x)))}^{n\\ {\\tt times}} \\] <p>The height of a complete lattice is the length of the longest path from \\(\\top\\) to  \\(\\bot\\).</p> <p>The intution of this theorem is that if we start from the \\(\\bot\\) of the lattice and keep applying a monotonic function \\(f\\), we will reach a fixed point and it must be the only least fixed point.  The presence of \\(\\bigsqcup\\) in the definition above is find the common upper bound for all these applications. Note that the \\(f^{i}(\\bot) \\sqcup f^{i+1}(\\bot) = f^{i+1}(\\bot)\\) as \\(f\\) is monotonic. Eventually, we get rid of the \\(\\bigsqcup\\) in the result.</p> <p>For example, consider function \\(f_2\\). If we start from \\(\\bot\\) and apply \\(f_2\\) repetively, we reach \\(+\\) which is the least fixed point.</p>"},{"location":"notes/sign_analysis_lattice/#lemma-15-map-update-with-monotonic-function-is-monotonic","title":"Lemma 15 - Map update with monotonic function is Monotonic","text":"<p>Let \\(f : L_1 \\rightarrow (A \\rightarrow L_2)\\) be a monotonic function from a lattice \\(L_1\\) to a map lattice \\(A \\rightarrow L_2\\). Let \\(g: L_1 \\rightarrow L_2\\) be another monotonic function. Then \\(h(x) = f(x)[a \\mapsto g(x)]\\) is a monotonic function of \\(L_1 \\rightarrow (A \\rightarrow L_2)\\).</p> <p>To gain some intuition of this lemma, let's try to think in terms of Haskell. Recall that the map lattice is \\(A \\rightarrow L_2\\) can be treated as  <code>Map a l2</code> in Haskell style, and <code>l2</code> is a lattice. <code>f :: l1 -&gt; Map a l2</code> is a Haskell function that's monotonic, <code>g:: l1 -&gt; l2</code> is another Haskell function which is monotonic. Then we can conclude that </p> <p><pre><code>a :: A\na = ... --  a is an element of A, where A is a ground type.\nh :: l1 -&gt; Map a l2 \nh x = insert a (g x) (f x)\n</code></pre> <code>h</code> is also monotonic. </p> <p>Since \\(f\\) is monotonic, given \\(x \\sqsubseteq y\\), we have \\(f(x) \\sqsubseteq f(y)\\). It follows that \\(f(x)[ a\\mapsto g(x)] \\sqsubseteq f(y)[ a\\mapsto g(x)]\\). Since \\(g\\) is monotonic, we have \\(f(y)[ a\\mapsto g(x)] \\sqsubseteq f(y)[ a\\mapsto g(y)]\\).</p> <p>With the fixed point theoem and Lemma 15, we are ready to define a general solution to solve the equations sytems generated from the sign analysis.</p>"},{"location":"notes/sign_analysis_lattice/#naive-fixed-point-algorithm","title":"Naive Fixed Point Algorithm","text":"<p>input: a function <code>f</code>.</p> <ol> <li>initialize <code>x</code> as \\(\\bot\\)</li> <li>apply <code>f(x)</code> as <code>x1</code></li> <li>check <code>x1 == x</code> <ol> <li>if true return <code>x</code></li> <li>else, update <code>x = x1</code>, go back to step 2.</li> </ol> </li> </ol> <p>For instance, if we apply the above algorithm to the \\(f_2\\) with the lattice in <code>Graph1</code>, we have the following iterations.</p> <ol> <li>\\(x = \\bot, x_1 = f_2(x) = +\\)</li> <li>\\(x = +, x_1 = f_2(x) = +\\)</li> <li>fixed point is reached, return \\(x\\). </li> </ol>"},{"location":"notes/sign_analysis_lattice/#applying-naive-fixed-point-algorithm-to-sign-analysis-problem-of-pa2","title":"Applying Naive Fixed Point Algorithm to Sign Analysis Problem of <code>PA2</code>","text":"<p>Recall the set of equations generated from <code>PA2</code></p> <pre><code>s0 = [x -&gt; top]\ns1 = s0[x -&gt; 0]\ns2 = s1[x  -&gt; s1(x) ++ +]\ns3 = s2\n</code></pre> <p>and we use \\(Var\\) to denote the set of variables, in this case we have only one variable \\(x\\). and \\(Sign\\) to denote the sign lattice described in <code>Graph1</code>.</p> <p>We model the equation systems by defining one lattice for each equation, \\((Var \\rightarrow Sign)\\). In total. we have four map lattices, one for <code>s0</code>, one for <code>s1</code>, and etc. Then we \"package\" these four map lattices into a product lattice  \\(L = (Var \\rightarrow Sign)^4\\).  Since \\(Sign\\) is a complete lattice, so is \\(L\\).</p> <p>Next we want to define the monotonic function \\(f_3\\) that helps us to find least fixed point which will be the solution of the above equation systems.  The type of \\(f_3\\) should be \\(L \\rightarrow L\\), or </p> \\[((Var \\rightarrow Sign) \\times (Var \\rightarrow Sign) \\times (Var \\rightarrow Sign) \\times (Var \\rightarrow Sign)) \\rightarrow ((Var \\rightarrow Sign) \\times (Var \\rightarrow Sign) \\times (Var \\rightarrow Sign) \\times (Var \\rightarrow Sign))\\] <p>in its unabridge form.</p> <p>Reminder: Even though we write map lattice as \\(Var \\rightarrow Sign\\), but it is like a <code>Map[Var, Sign]</code>.</p> <p>Next we re-model the relations among <code>s0,s1,s2,s3</code> in above equation system in \\(f_3\\) as follows</p> \\[ f_3(s_0,s_1,s_2,s_3) = ([x \\mapsto \\top],s_0[x \\mapsto 0], s_1[x \\mapsto (s_0(x) {\\tt ++}\\ {\\tt +} )], s_2) \\] <p>Thanks to Lemma 15, \\(f_3\\) is monotonic.</p> <p>The last step is to apply the naive fixed point algorithm to \\(f_3\\) with \\(s_0 = s_1 = s_2 = s_3 = [x \\mapsto \\bot]\\) as the starting point.</p> <ol> <li>\\(s_0 = s_1 = s_2 = s_3 = [x \\mapsto \\bot]\\),  $$ \\begin{array}{rcl} f_3(s_0,s_1,s_2,s_3) &amp;  = &amp; ([x \\mapsto \\top], s_0[x \\mapsto 0], s_1[x \\mapsto (s_0(x) {\\tt ++} {\\tt +})], s_2) \\ &amp; = &amp; ([x \\mapsto \\top], [x \\mapsto 0], [x \\mapsto +], [x \\mapsto \\bot]) \\end{array} $$</li> <li> <p>\\(s_0 = [x \\mapsto \\top], s_1 =[x \\mapsto 0], s_2 = [x \\mapsto +], s_3 = [x \\mapsto \\bot]\\),  $$ \\begin{array}{rcl} f_3(s_0,s_1,s_2,s_3) &amp; = &amp; ([x \\mapsto \\top], s_0[x \\mapsto 0], s_1[x \\mapsto (s_0(x) {\\tt ++} {\\tt +})], s_2) \\ &amp; = &amp; ([x \\mapsto \\top], [x \\mapsto 0], [x \\mapsto +], [x \\mapsto +]) \\end{array} $$</p> </li> <li> <p>\\(s_0 = [x \\mapsto \\top], s_1 =[x \\mapsto 0], s_2 = [x \\mapsto +], s_3 = [x \\mapsto +]\\),  $$ \\begin{array}{rcl} f_3(s_0,s_1,s_2,s_3) &amp; = &amp; ([x \\mapsto \\top], s_0[x \\mapsto 0], s_1[x \\mapsto (s_0(x) {\\tt ++} {\\tt +})], s_2) \\ &amp; = &amp; ([x \\mapsto \\top], [x \\mapsto 0], [x \\mapsto +], [x \\mapsto +]) \\end{array} $$</p> </li> <li> <p>fixed point reached, the solution is \\(s_0 = [x \\mapsto \\top], s_1 =[x \\mapsto 0], s_2 = [x \\mapsto +], s_3 = [x \\mapsto +]\\).</p> </li> </ol>"},{"location":"notes/sign_analysis_lattice/#applying-naive-fixed-point-algorithm-to-sign-analysis-problem-of-pa4","title":"Applying Naive Fixed Point Algorithm to Sign Analysis Problem of <code>PA4</code>","text":"<p>Recall the set of equations generated from <code>PA4</code>'s sign analysis <pre><code>s0 = [x -&gt; top, y -&gt; top, t -&gt; top]\ns1 = s0\ns2 = s1[y -&gt; 0]\ns3 = upperbound(s2,s7)[t -&gt; top]\ns4 = s3\ns5 = s4[y -&gt; s4(y) ++ +]\ns6 = s5[x -&gt; s5(x) -- +]\ns7 = s6\ns8 = s4\n</code></pre></p> <p>We define a monotonic function \\(f_4 : (Var \\rightarrow Sign)^9 \\rightarrow (Var \\rightarrow Sign)^9\\) as follows</p> \\[ \\begin{array}{rcl} f_4(s_0, s_1, s_2, s_3, s_4, s_5, s_6, s_7, s_8, s_9) &amp; = &amp; \\left (     \\begin{array}{c}      [x \\mapsto \\top, y \\mapsto \\top, t \\mapsto \\top], \\\\       s_0, \\\\      s_1[y \\mapsto 0], \\\\      (s_2 \\sqcup s_7)[t \\mapsto \\top], \\\\      s_3, \\\\      s_4[y \\mapsto s_4(y) {\\tt ++} \\ {\\tt +}], \\\\      s_5[x \\mapsto s_5(x) {\\tt --} \\ {\\tt +}], \\\\      s_6, \\\\      s_4     \\end{array}      \\right ) \\end{array} \\] <ol> <li> <p>\\(s_0 = s_1 = s_2 = s_3 = s_4 = s_5 = s_6 = s_7 = s_8 = [x \\mapsto \\bot, y \\mapsto \\bot, t \\mapsto \\bot]\\),  $$ \\begin{array}{rcl} f_4(s_0, s_1, s_2, s_3, s_4, s_5, s_6, s_7, s_8, s_9) &amp; = &amp; \\left (     \\begin{array}{c}      [x \\mapsto \\top, y \\mapsto \\top, t \\mapsto \\top], \\     [x \\mapsto \\bot, y \\mapsto \\bot, t \\mapsto \\bot], \\      [x \\mapsto \\bot, y \\mapsto 0, t \\mapsto \\bot], \\      [x \\mapsto \\bot, y \\mapsto 0, t \\mapsto \\top], \\      [x \\mapsto \\bot, y \\mapsto \\bot, t \\mapsto \\bot], \\      [x \\mapsto \\bot, y \\mapsto \\bot, t \\mapsto \\bot], \\      [x \\mapsto \\bot, y \\mapsto \\bot, t \\mapsto \\bot], \\      [x \\mapsto \\bot, y \\mapsto \\bot, t \\mapsto \\bot], \\      [x \\mapsto \\bot, y \\mapsto \\bot, t \\mapsto \\bot]     \\end{array}      \\right ) \\end{array} $$</p> </li> <li> \\[  \\begin{array}{l}  s_0 = [x \\mapsto \\top, y \\mapsto \\top, t \\mapsto \\top], \\\\   s_1 = [x \\mapsto \\bot, y \\mapsto \\bot, t \\mapsto \\bot], \\\\  s_2 = [x \\mapsto \\bot, y \\mapsto 0, t \\mapsto \\bot], \\\\  s_3 = [x \\mapsto \\bot, y \\mapsto 0, t \\mapsto \\top], \\\\  s_4 = [x \\mapsto \\bot, y \\mapsto \\bot, t \\mapsto \\bot], \\\\  s_5 = [x \\mapsto \\bot, y \\mapsto \\bot, t \\mapsto \\bot], \\\\  s_6 = [x \\mapsto \\bot, y \\mapsto \\bot, t \\mapsto \\bot], \\\\  s_7 = [x \\mapsto \\bot, y \\mapsto \\bot, t \\mapsto \\bot], \\\\  s_8 = [x \\mapsto \\bot, y \\mapsto \\bot, t \\mapsto \\bot] \\end{array}  \\] </li> </ol> \\[ \\begin{array}{rcl} f_4(s_0, s_1, s_2, s_3, s_4, s_5, s_6, s_7, s_8, s_9) &amp; = &amp; \\left (     \\begin{array}{c}      [x \\mapsto \\top, y \\mapsto \\top, t \\mapsto \\top], \\\\       [x \\mapsto \\top, y \\mapsto \\top, t \\mapsto \\top], \\\\       [x \\mapsto \\bot, y \\mapsto 0, t \\mapsto \\bot], \\\\      [x \\mapsto \\bot, y \\mapsto 0, t \\mapsto \\top], \\\\     [x \\mapsto \\bot, y \\mapsto 0, t \\mapsto \\top], \\\\      [x \\mapsto \\bot, y \\mapsto \\bot, t \\mapsto \\bot], \\\\      [x \\mapsto \\bot, y \\mapsto \\bot, t \\mapsto \\bot], \\\\     [x \\mapsto \\bot, y \\mapsto \\bot, t \\mapsto \\bot], \\\\     [x \\mapsto \\bot, y \\mapsto \\bot, t \\mapsto \\bot]     \\end{array}      \\right ) \\end{array} \\] <ol> <li> \\[  \\begin{array}{l}  s_0 = [x \\mapsto \\top, y \\mapsto \\top, t \\mapsto \\top], \\\\   s_1 = [x \\mapsto \\top, y \\mapsto \\top, t \\mapsto \\top], \\\\   s_2 = [x \\mapsto \\bot, y \\mapsto 0, t \\mapsto \\bot], \\\\  s_3 = [x \\mapsto \\bot, y \\mapsto 0, t \\mapsto \\top], \\\\ s_4 = [x \\mapsto \\bot, y \\mapsto 0, t \\mapsto \\top], \\\\  s_5 = [x \\mapsto \\bot, y \\mapsto \\bot, t \\mapsto \\bot], \\\\  s_6 = [x \\mapsto \\bot, y \\mapsto \\bot, t \\mapsto \\bot], \\\\ s_7 = [x \\mapsto \\bot, y \\mapsto \\bot, t \\mapsto \\bot], \\\\ s_8 = [x \\mapsto \\bot, y \\mapsto \\bot, t \\mapsto \\bot] \\end{array}  \\] </li> </ol> \\[ \\begin{array}{rcl} f_4(s_0, s_1, s_2, s_3, s_4, s_5, s_6, s_7, s_8, s_9) &amp; = &amp; \\left (     \\begin{array}{c}      [x \\mapsto \\top, y \\mapsto \\top, t \\mapsto \\top], \\\\       [x \\mapsto \\top, y \\mapsto \\top, t \\mapsto \\top], \\\\       [x \\mapsto \\top, y \\mapsto 0, t \\mapsto \\top], \\\\      [x \\mapsto \\bot, y \\mapsto 0, t \\mapsto \\top], \\\\      [x \\mapsto \\bot, y \\mapsto 0, t \\mapsto \\top], \\\\     [x \\mapsto \\bot, y \\mapsto +, t \\mapsto \\top], \\\\       [x \\mapsto \\bot, y \\mapsto \\bot, t \\mapsto \\bot], \\\\      [x \\mapsto \\bot, y \\mapsto \\bot, t \\mapsto \\bot], \\\\      [x \\mapsto \\bot, y \\mapsto 0, t \\mapsto \\top]      \\end{array}      \\right ) \\end{array} \\] <ol> <li></li> </ol> \\[  \\begin{array}{l}  s_0 = [x \\mapsto \\top, y \\mapsto \\top, t \\mapsto \\top], \\\\   s_1 = [x \\mapsto \\top, y \\mapsto \\top, t \\mapsto \\top], \\\\   s_2 = [x \\mapsto \\top, y \\mapsto 0, t \\mapsto \\top], \\\\  s_3 = [x \\mapsto \\bot, y \\mapsto 0, t \\mapsto \\top], \\\\ s_4 = [x \\mapsto \\bot, y \\mapsto 0, t \\mapsto \\top], \\\\  s_5 = [x \\mapsto \\bot, y \\mapsto +, t \\mapsto \\top], \\\\  s_6 = [x \\mapsto \\bot, y \\mapsto \\bot, t \\mapsto \\bot], \\\\ s_7 = [x \\mapsto \\bot, y \\mapsto \\bot, t \\mapsto \\bot], \\\\ s_8 = [x \\mapsto \\bot, y \\mapsto 0, t \\mapsto \\top] \\end{array}  \\] \\[ \\begin{array}{rcl} f_4(s_0, s_1, s_2, s_3, s_4, s_5, s_6, s_7, s_8, s_9) &amp; = &amp; \\left (     \\begin{array}{c}      [x \\mapsto \\top, y \\mapsto \\top, t \\mapsto \\top], \\\\       [x \\mapsto \\top, y \\mapsto \\top, t \\mapsto \\top], \\\\      [x \\mapsto \\top, y \\mapsto 0, t \\mapsto \\top], \\\\      [x \\mapsto \\top, y \\mapsto 0, t \\mapsto \\top], \\\\     [x \\mapsto \\bot, y \\mapsto 0, t \\mapsto \\top], \\\\     [x \\mapsto \\bot, y \\mapsto +, t \\mapsto \\top], \\\\       [x \\mapsto \\bot, y \\mapsto +, t \\mapsto \\top], \\\\      [x \\mapsto \\bot, y \\mapsto \\bot, t \\mapsto \\bot], \\\\      [x \\mapsto \\bot, y \\mapsto 0, t \\mapsto \\top]     \\end{array}      \\right ) \\end{array} \\] <ol> <li></li> </ol> \\[ \\begin{array}{c}  s_0 = [x \\mapsto \\top, y \\mapsto \\top, t \\mapsto \\top], \\\\   s_1 = [x \\mapsto \\top, y \\mapsto \\top, t \\mapsto \\top], \\\\  s_2 = [x \\mapsto \\top, y \\mapsto 0, t \\mapsto \\top], \\\\  s_3 = [x \\mapsto \\top, y \\mapsto 0, t \\mapsto \\top], \\\\ s_4 = [x \\mapsto \\bot, y \\mapsto 0, t \\mapsto \\top], \\\\ s_5 = [x \\mapsto \\bot, y \\mapsto +, t \\mapsto \\top], \\\\   s_6 = [x \\mapsto \\bot, y \\mapsto +, t \\mapsto \\top], \\\\  s_7 = [x \\mapsto \\bot, y \\mapsto \\bot, t \\mapsto \\bot], \\\\  s_8 = [x \\mapsto \\bot, y \\mapsto 0, t \\mapsto \\top] \\end{array}  \\] \\[ \\begin{array}{rcl} f_4(s_0, s_1, s_2, s_3, s_4, s_5, s_6, s_7, s_8, s_9) &amp; = &amp; \\left (     \\begin{array}{c}      [x \\mapsto \\top, y \\mapsto \\top, t \\mapsto \\top], \\\\       [x \\mapsto \\top, y \\mapsto \\top, t \\mapsto \\top], \\\\      [x \\mapsto \\top, y \\mapsto 0, t \\mapsto \\top], \\\\      [x \\mapsto \\top, y \\mapsto 0, t \\mapsto \\top], \\\\     [x \\mapsto \\top, y \\mapsto 0, t \\mapsto \\top], \\\\      [x \\mapsto \\bot, y \\mapsto +, t \\mapsto \\top], \\\\      [x \\mapsto \\bot, y \\mapsto +, t \\mapsto \\top], \\\\      [x \\mapsto \\bot, y \\mapsto +, t \\mapsto \\top], \\\\      [x \\mapsto \\bot, y \\mapsto 0, t \\mapsto \\top]     \\end{array}      \\right ) \\end{array} \\] <ol> <li> \\[ \\begin{array}{c}  s_0 = [x \\mapsto \\top, y \\mapsto \\top, t \\mapsto \\top], \\\\   s_1 = [x \\mapsto \\top, y \\mapsto \\top, t \\mapsto \\top], \\\\  s_2 = [x \\mapsto \\top, y \\mapsto 0, t \\mapsto \\top], \\\\  s_3 = [x \\mapsto \\top, y \\mapsto 0, t \\mapsto \\top], \\\\ s_4 = [x \\mapsto \\top, y \\mapsto 0, t \\mapsto \\top], \\\\  s_5 = [x \\mapsto \\bot, y \\mapsto +, t \\mapsto \\top], \\\\  s_6 = [x \\mapsto \\bot, y \\mapsto +, t \\mapsto \\top], \\\\  s_7 = [x \\mapsto \\bot, y \\mapsto +, t \\mapsto \\top], \\\\  s_8 = [x \\mapsto \\bot, y \\mapsto 0, t \\mapsto \\top] \\end{array}  $$ $$ \\begin{array}{rcl} f_4(s_0, s_1, s_2, s_3, s_4, s_5, s_6, s_7, s_8, s_9) &amp; = &amp; \\left (     \\begin{array}{c}      [x \\mapsto \\top, y \\mapsto \\top, t \\mapsto \\top], \\\\       [x \\mapsto \\top, y \\mapsto \\top, t \\mapsto \\top], \\\\      [x \\mapsto \\top, y \\mapsto 0, t \\mapsto \\top], \\\\      [x \\mapsto \\top, y \\mapsto 0, t \\mapsto \\top], \\\\     [x \\mapsto \\top, y \\mapsto 0, t \\mapsto \\top], \\\\      [x \\mapsto \\top, y \\mapsto +, t \\mapsto \\top], \\\\          [x \\mapsto \\bot, y \\mapsto +, t \\mapsto \\top], \\\\      [x \\mapsto \\bot, y \\mapsto +, t \\mapsto \\top], \\\\      [x \\mapsto \\top, y \\mapsto 0, t \\mapsto \\top]     \\end{array}      \\right )     \\end{array} \\] </li> <li> \\[ \\begin{array}{c}  s_0 = [x \\mapsto \\top, y \\mapsto \\top, t \\mapsto \\top], \\\\   s_1 = [x \\mapsto \\top, y \\mapsto \\top, t \\mapsto \\top], \\\\  s_2 = [x \\mapsto \\top, y \\mapsto 0, t \\mapsto \\top], \\\\  s_3 = [x \\mapsto \\top, y \\mapsto 0, t \\mapsto \\top], \\\\ s_4 = [x \\mapsto \\top, y \\mapsto 0, t \\mapsto \\top], \\\\  s_5 = [x \\mapsto \\top, y \\mapsto +, t \\mapsto \\top], \\\\      s_6 = [x \\mapsto \\bot, y \\mapsto +, t \\mapsto \\top], \\\\  s_7 = [x \\mapsto \\bot, y \\mapsto +, t \\mapsto \\top], \\\\  s_8 = [x \\mapsto \\top, y \\mapsto 0, t \\mapsto \\top] \\end{array}  \\] </li> </ol> \\[ \\begin{array}{rcl} f_4(s_0, s_1, s_2, s_3, s_4, s_5, s_6, s_7, s_8, s_9) &amp; = &amp; \\left (     \\begin{array}{c}      [x \\mapsto \\top, y \\mapsto \\top, t \\mapsto \\top], \\\\       [x \\mapsto \\top, y \\mapsto \\top, t \\mapsto \\top], \\\\      [x \\mapsto \\top, y \\mapsto 0, t \\mapsto \\top], \\\\      [x \\mapsto \\top, y \\mapsto 0, t \\mapsto \\top], \\\\     [x \\mapsto \\top, y \\mapsto 0, t \\mapsto \\top], \\\\     [x \\mapsto \\top, y \\mapsto +, t \\mapsto \\top], \\\\     [x \\mapsto \\top, y \\mapsto +, t \\mapsto \\top], \\\\      [x \\mapsto \\bot, y \\mapsto +, t \\mapsto \\top], \\\\      [x \\mapsto \\top, y \\mapsto 0, t \\mapsto \\top]     \\end{array}      \\right )     \\end{array} \\] <p>8 .  $$ \\begin{array}{c}  s_0 = [x \\mapsto \\top, y \\mapsto \\top, t \\mapsto \\top], \\ s_1 = [x \\mapsto \\top, y \\mapsto \\top, t \\mapsto \\top], \\  s_2 = [x \\mapsto \\top, y \\mapsto 0, t \\mapsto \\top], \\  s_3 = [x \\mapsto \\top, y \\mapsto 0, t \\mapsto \\top], \\ s_4 = [x \\mapsto \\top, y \\mapsto 0, t \\mapsto \\top], \\  s_5 = [x \\mapsto \\top, y \\mapsto +, t \\mapsto \\top], \\    s_6 = [x \\mapsto \\top, y \\mapsto +, t \\mapsto \\top], \\  s_7 = [x \\mapsto \\bot, y \\mapsto +, t \\mapsto \\top], \\  s_8 = [x \\mapsto \\top, y \\mapsto 0, t \\mapsto \\top] \\end{array}  $$</p> \\[ \\begin{array}{rcl} f_4(s_0, s_1, s_2, s_3, s_4, s_5, s_6, s_7, s_8, s_9) &amp; = &amp; \\left (     \\begin{array}{c}      [x \\mapsto \\top, y \\mapsto \\top, t \\mapsto \\top], \\\\       [x \\mapsto \\top, y \\mapsto \\top, t \\mapsto \\top], \\\\      [x \\mapsto \\top, y \\mapsto 0, t \\mapsto \\top], \\\\      [x \\mapsto \\top, y \\mapsto 0, t \\mapsto \\top], \\\\     [x \\mapsto \\top, y \\mapsto 0, t \\mapsto \\top], \\\\     [x \\mapsto \\top, y \\mapsto +, t \\mapsto \\top], \\\\     [x \\mapsto \\top, y \\mapsto +, t \\mapsto \\top], \\\\      [x \\mapsto \\top, y \\mapsto +, t \\mapsto \\top], \\\\      [x \\mapsto \\top, y \\mapsto 0, t \\mapsto \\top]     \\end{array}      \\right )     \\end{array} \\] <p>If we apply \\(f_4\\) one more time to the above set of states, we get the same states. At this point, we reach the fixed point of the \\(f_4\\) functipn w.r.t the \\((Var \\rightarrow Sign)^9\\) lattice. </p>"},{"location":"notes/sign_analysis_lattice/#optimization","title":"Optimization","text":"<p>This naive fixed point algorithm works but not efficient, namely it blindly applies the \"update\" of a state \\(s_i\\) based on \\(s_{i-1}\\) though there is no changes to \\(s_{i-1}\\) in the last iteration. For example from step 7 to step 8, \\(s_3\\) is updated though there is no change to \\(s_2\\). </p> <p>A more efficient algorithm can be derived if we keep track of the dependencies among the states and perform the \"update of a state \\(s_i\\) if \\(s_i\\) is based on \\(s_{i-1}\\) and \\(s_{i-1}\\) has changed.</p>"},{"location":"notes/sign_analysis_lattice/#generalizing-the-monotone-constraints-for-sign-analysis","title":"Generalizing the monotone constraints for sign analysis","text":"<p>We would like to have a systematic way to define the monotone constraints (i.e. monotonic functions) for analyses like sign analysis.</p> <p>Let \\(v_i\\) denote a vertex in CFG. We write \\(pred(v_i)\\) to denote the set of predecesors of \\(v_i\\). let \\(s_i\\) denote the state variable of the vertex \\(v_i\\) in the CFG. We write \\(pred(s_i)\\) to denote the set of state variables of the predecessor of \\(v_i\\).</p> <p>For sign analysis,  we define the following helper function </p> \\[join(s) = \\bigsqcup pred(s)\\] <p>To avoid confusion, we write \\(src\\) to denote the source operands in PA instead of \\(s\\). Let \\(V\\) denotes the set of variables in the PA program's being analysed.</p> <p>The monotonic functions can be defined by the following cases.</p> <ul> <li>case \\(l == 0\\), \\(s_0 = \\lbrack x \\mapsto \\top \\mid x \\in V\\rbrack\\)</li> <li>case \\(l: t \\leftarrow src\\), \\(s_l = join(s_l) \\lbrack t \\mapsto join(s_l)(src)\\rbrack\\)</li> <li>case \\(l: t \\leftarrow src_1\\ op\\ src_2\\), \\(s_l = join(s_l) \\lbrack t \\mapsto (join(s_l)(src_1)\\ abs(op)\\ join(s_l)(src_2))\\rbrack\\)</li> <li>other cases: \\(s_l = join(s_l)\\)</li> </ul> <p>Let \\(m\\) be a map lattice object, and \\(src\\) be a PA source operand, the lookup operation \\(m(src)\\) for sign analysis is defined as follows </p> \\[ \\begin{array}{rcl} m(c) &amp; = &amp; \\left \\{          \\begin{array}{cc}         0 &amp; c == 0 \\\\         + &amp; c &gt; 0 \\\\         - &amp; c &lt; 0              \\end{array}          \\right . \\\\ \\\\ m(t) &amp; = &amp; \\left \\{         \\begin{array}{cc}         v &amp; t \\mapsto v \\in m \\\\          error &amp; otherwise         \\end{array}             \\right . \\\\ \\\\  m(r) &amp; = &amp; error \\end{array} \\] <p>Let \\(op\\) be PA operator, we define the abstraction operation \\(abs(op)\\) for sign analysis as follows, </p> \\[ \\begin{array}{rcl} abs(+) &amp; = &amp; ++\\\\ abs(-) &amp; = &amp; -- \\\\ abs(*) &amp; = &amp; ** \\\\ abs(&lt;) &amp; = &amp; &lt;&lt; \\\\  abs(==) &amp; = &amp; ===  \\end{array} \\] <p>We have seen the definitions of \\(++, --, **\\) and \\(&lt;&lt;\\)</p> <p>Question: can you define \\(===\\)?</p> <p>Question: the abstraction operations are pretty coarse (not accurate). For instance, <code>&lt;&lt;</code> and <code>===</code> should return either <code>0</code> or <code>1</code> hence \\(\\top\\) is too coarse. Can you define a lattice for sign analysis which offers better accuracy? </p> <p>Question: Convert <code>SIMP1</code> into a PA. Can we apply the sign analysis to find out that the <code>sqrt(x)</code> is definifely failing?</p>"},{"location":"notes/static_semantics/","title":"50.054 Static Semantics For SIMP","text":""},{"location":"notes/static_semantics/#learning-outcomes","title":"Learning Outcomes","text":"<ol> <li>Explain what static semantics is.</li> <li>Apply type checking rules to verify the type correctness property of a SIMP program.</li> <li>Explain the relation between type system and operational semantics.</li> <li>Apply type inference algorithm to generate a type environment given a SIMP program.</li> </ol>"},{"location":"notes/static_semantics/#what-is-static-semantics","title":"What is static semantics?","text":"<p>While dynamic semantics defines the run-time behavior of the given program, static semantics defines the compile-time properties of the given program.</p> <p>For example, a statically correct program, must satisfy some properties</p> <ol> <li>all uses of variables in it must be defined somewhere earlier. </li> <li>all the use of variables, the types must be matching with the expected type in the context.</li> <li>... </li> </ol> <p>Here is a statically correct SIMP program,</p> <pre><code>x = 0;\ny = input;\nif y &gt; x {\n    y = 0;\n}\nreturn y;\n</code></pre> <p>because it satifies the first two properties. </p> <p>The following program is not statically correct.</p> <pre><code>x = 0;\ny = input;\nif y + x { // type error\n    x = z; // the use of an undefined variable z\n}\nreturn x;\n</code></pre> <p>Static checking is to rule out the statically incorrect programs.</p>"},{"location":"notes/static_semantics/#type-checking-for-simp","title":"Type Checking for SIMP","text":"<p>We consider the type checking for SIMP programs.</p> <p>Recall the syntax rules for SIMP</p> \\[ \\begin{array}{rccl} (\\tt Statement) &amp; S &amp; ::= &amp; X = E ; \\mid return\\ X ; \\mid nop; \\mid if\\ E\\ \\{ \\overline{S} \\}\\ else\\ \\{ \\overline{S} \\} \\mid while\\ E\\ \\{ \\overline{S} \\} \\\\ (\\tt Expression) &amp; E &amp; ::= &amp; E\\ OP\\ E \\mid X \\mid C  \\mid (E) \\\\ (\\tt Statements) &amp; \\overline{S} &amp; ::= &amp; S \\mid S\\ \\overline{S} \\\\ (\\tt Operator) &amp; OP &amp; ::= &amp; + \\mid - \\mid * \\mid &lt; \\mid == \\\\  (\\tt Constant) &amp; C &amp; ::= &amp; 0 \\mid 1 \\mid 2 \\mid ... \\mid true \\mid false \\\\  (\\tt Variable) &amp; X &amp; ::= &amp; a \\mid b \\mid c \\mid d \\mid ... \\\\   {\\tt (Types)} &amp; T &amp; ::= &amp; int \\mid bool  \\\\   {\\tt (Type\\ Environments)} &amp; \\Gamma &amp; \\subseteq &amp; (X \\times T) \\end{array} \\] <p>We use the symbol \\(\\Gamma\\) to denote a type environments mapping SIMP variables to types. \\(T\\) to denote a type. We write \\(dom(\\Gamma)\\) to denote the domain of \\(\\Gamma\\), i.e. \\(\\{ X \\mid (x,T) \\in \\Gamma \\}\\). We assume for all \\(x \\in dom(\\Gamma)\\), there exists only one entry of \\((x,T) \\in \\Gamma\\).</p> <p>We define two different relations, </p> <ol> <li>\\(\\Gamma \\vdash E : T\\), which type-checks a SIMP expresion \\(E\\) against a type \\(T\\) under \\(\\Gamma\\).</li> <li>\\(\\Gamma \\vdash \\overline{S}\\), which type-checks a SIMP statement sequence \\(\\overline{S}\\) under \\(\\Gamma\\).</li> </ol>"},{"location":"notes/static_semantics/#type-checking-rules-for-simp-expressions","title":"Type checking rules for SIMP Expressions","text":"\\[ \\begin{array}{rc} {\\tt (tVar)} &amp; \\begin{array}{c}                 (X,T) \\in \\Gamma                 \\\\ \\hline                 \\Gamma \\vdash X : T                 \\end{array}  \\end{array} \\] <p>In the rule \\({\\tt (tVar)}\\), we type check the variable \\(X\\) having type \\(T\\) under the type environment \\(\\Gamma\\) if we can find the entry \\((X,T)\\) in \\(\\Gamma\\).</p> \\[ \\begin{array}{rc} {\\tt (tInt)} &amp; \\begin{array}{c}                 C\\ {\\tt is\\ an\\ integer}                 \\\\ \\hline                 \\Gamma \\vdash C : int                 \\end{array} \\\\ \\\\  {\\tt (tBool)} &amp; \\begin{array}{c}                 C \\in \\{true,false\\}                 \\\\ \\hline                 \\Gamma \\vdash C : bool                 \\end{array}  \\end{array} \\] <p>In the rule \\({\\tt (tInt)}\\), we type check an integer constant having type \\(int\\). Similarly, we type check a boolean constant having type \\(bool\\). </p> \\[ \\begin{array}{rc} {\\tt (tOp1)} &amp; \\begin{array}{c}                 \\Gamma \\vdash E_1:int \\ \\ \\ \\Gamma \\vdash E_2:int\\ \\ \\ OP \\in \\{ +, -, * \\}                 \\\\ \\hline                 \\Gamma \\vdash E_1\\ OP\\ E_2 : int                 \\end{array} \\\\ \\\\  {\\tt (tOp2)} &amp; \\begin{array}{c}                 \\Gamma \\vdash E_1:int \\ \\ \\ \\Gamma \\vdash E_2:int\\ \\ \\ OP \\in \\{ ==, &lt;\\}                 \\\\ \\hline                 \\Gamma \\vdash E_1\\ OP E_2 : bool                 \\end{array} \\\\ \\\\ {\\tt (tOp3)} &amp; \\begin{array}{c}                 \\Gamma \\vdash E_1:bool \\ \\ \\ \\Gamma \\vdash E_2:bool\\ \\ \\ OP \\in \\{ ==, &lt;\\}                 \\\\ \\hline                 \\Gamma \\vdash E_1\\ OP\\ E_2 : bool                 \\end{array}  \\end{array} \\] <p>In the rule \\({\\tt (tOp1)}\\), we type check an integer arithmetic operation having type \\(int\\), if both operands can be type-checked against \\(int\\). In the rule \\({\\tt (tOp2)}\\), we type check an integer comparison operation having type \\(bool\\), if both operands can be type-checked against \\(int\\). In the rule \\({\\tt (tOp3)}\\), we type check a boolean comparison operation having type \\(bool\\), if both operands can be type-checked against \\(bool\\).</p> \\[ \\begin{array}{rc} {\\tt (tParen)} &amp; \\begin{array}{c}                 \\Gamma \\vdash E :T                 \\\\ \\hline                 \\Gamma \\vdash (E) :T                 \\end{array} \\end{array} \\] <p>Lastly in rule \\({\\tt (tParen)}\\), we type check a parenthesized expression by type-checking the inner expression. </p>"},{"location":"notes/static_semantics/#type-checking-rules-for-simp-statements","title":"Type Checking rules for SIMP Statements","text":"<p>The typing rules for statement is in form of \\(\\Gamma \\vdash \\overline{S}\\) instead of  \\(\\Gamma \\vdash \\overline{S} : T\\), this is because  statements do not return a value (except for return statement, which returns a value for the entire program.)</p> \\[ \\begin{array}{rc} {\\tt (tSeq)} &amp; \\begin{array}{c}                 \\Gamma \\vdash S \\ \\ \\  \\Gamma \\vdash \\overline{S}                 \\\\ \\hline                 \\Gamma \\vdash S \\overline{S}                \\end{array} \\end{array} \\] <p>The \\({\\tt (tSeq)}\\) rule type checks a non empty sequence of statement \\(S \\overline{S}\\) under the type environment \\(\\Gamma\\).  It is typeable (a proof exists) iff if \\(S\\) is typeable under \\(\\Gamma\\) and \\(\\overline{S}\\) is typeable under \\(\\Gamma\\).</p> \\[ \\begin{array}{rc} {\\tt (tAssign)} &amp; \\begin{array}{c}                 \\Gamma \\vdash E : T \\ \\ \\  \\Gamma \\vdash X : T                 \\\\ \\hline                 \\Gamma \\vdash X = E                 \\end{array} \\end{array} \\] <p>The \\({\\tt (tAssign)}\\) rule type checks an assignment statement \\(X = E\\) under \\(\\Gamma\\). It is typeable if both \\(X\\) and \\(E\\) are typeable under \\(\\Gamma\\) respectively and their types agree.</p> \\[ \\begin{array}{rc}  {\\tt (tReturn)} &amp; \\begin{array}{c}                 \\Gamma \\vdash X : T                 \\\\ \\hline                 \\Gamma \\vdash return\\ X                 \\end{array} \\\\ \\\\  {\\tt (tNop)} &amp; \\Gamma \\vdash nop  \\end{array} \\] <p>The \\({\\tt (tReturn)}\\) rule type checks the return statement. It is typeable, if the variable \\(X\\) is typeable. The \\({\\tt (tNop)}\\) rule type checks the nop statement, which is always typeable.</p> \\[ \\begin{array}{rc}  {\\tt (tIf)} &amp; \\begin{array}{c}                 \\Gamma \\vdash E:bool \\ \\ \\ \\Gamma \\vdash \\overline{S_1} \\ \\ \\ \\Gamma \\vdash \\overline{S_2}                 \\\\ \\hline                 \\Gamma \\vdash if\\ E\\ \\{\\overline{S_1}\\}\\ else\\ \\{ \\overline{S_2} \\}                 \\end{array} \\\\ \\\\  {\\tt (tWhile)} &amp; \\begin{array}{c}                 \\Gamma \\vdash E:bool \\ \\ \\ \\Gamma \\vdash \\overline{S}                 \\\\ \\hline                 \\Gamma \\vdash while\\ E\\ \\{\\overline{S}\\}                 \\end{array}  \\end{array} \\] <p>The \\({\\tt (tIf)}\\) rule type checks the if-else statement, \\(if\\ E\\ \\{\\overline{S_1}\\}\\ else\\ \\{ \\overline{S_2} \\}\\).  It is typeable if \\(E\\) has type \\(bool\\) under \\(\\Gamma\\) and both then- and else- branches are typeable under the \\(\\Gamma\\). The \\({\\tt (tWhile)}\\) rule type checks the while statement in a similar way.</p> <p>We say that a SIMP program \\(\\overline{S}\\) is typeable under \\(\\Gamma\\), i.e. it type checks with \\(\\Gamma\\) iff \\(\\Gamma \\vdash \\overline{S}\\). On the other hand, we say that a SIMP program \\(\\overline{S}\\) is not typeable, i.e. it does not type check, iff there exists no \\(\\Gamma\\) such that \\(\\Gamma \\vdash \\overline{S}\\). </p> <p>Let \\(\\Gamma = \\{ (input, int), (x,int), (s,int) \\}\\), we consider the type checking derivation of </p> \\[x = input; s = 0; while\\ s&lt;x\\ \\{ s = s + 1;\\}\\ return\\ s;\\] <pre><code>                              \u0393 |- s:int (tVar)\n                              \u0393 |- 0:int (tInt) \n\u0393 |- input:int (tVar)         -----------------(tAssign)   [sub tree 1]\n\u0393 |- x:int (tVar)             \u0393 |- s=0  \n------------------(tAssign)   --------------------------------------(tSeq)\n\u0393 |- x=input;                 \u0393 |- s=0; while s&lt;x { s = s + 1;} return s; \n---------------------------------------------------------------------(tSeq)\n\u0393 |- x=input; s=0; while s&lt;x { s = s + 1;} return s;\n</code></pre> <p>Where [sub tree 1] is</p> <pre><code>                                          \u0393 |- 0:int (tInt)\n                                          \u0393 |- s:int (tVar)\n\u0393 |- s:int (tVar)                         -----------------(tOp1)\n\u0393 |- x:int (tVar)      \u0393 |-s:int (tVar)   \u0393 |- s+1:int \n--------------(tOp2)   -------------------------------(tAssign)\n\u0393 |- s&lt;x:bool          \u0393 |- s = s + 1                   \u0393 |- s:int (tVar)\n---------------------------------------------(tWhile)  ---------------(tReturn)\n\u0393 |- while s&lt;x { s = s + 1;}                            \u0393 |- return s\n--------------------------------------------------------------------(tSeq)\n\u0393 |- while s&lt;x { s = s + 1;} return s; \n</code></pre> <p>Note that the following two programs are not typeable.</p> <p><pre><code>// untypeable 1\nx = 1;\ny = 0;\nif x {\n    y = 0; \n} else {\n    y = 1;\n}\nreturn y;\n</code></pre> The above is untypeable because we use x of type <code>int</code> in a context where it is also expected as <code>bool</code>.</p> <pre><code>// untypeable 2\nx = input;\nif (x &gt; 1) {\n    y = true;\n} else {\n    y = 0;\n}\nreturn y;\n</code></pre> <p>The above is unteable because we can't find a type environment which has both <code>(y,int)</code> and <code>(y,bool)</code>.</p> <p>So far these two \"counter\" examples are bad programs. However we also note that our type system is too conservative.</p> <pre><code>// untypeable 3\nx = input;\nif (x &gt; 1) {\n    if ( x * x * x &lt; x * x) {\n        y = true;\n    } else {\n        y = 1;\n    }\n} else {\n    y = 0;\n}\nreturn y;\n</code></pre> <p>Even though we note that when <code>x &gt; 1</code>, we have <code>x * x * x &lt; x * x == false</code> hence the statement <code>y = true</code> is not executed. Our type system still rejects this program. We will discuss this issue in details in the upcoming units.</p> <p>Let's connect the type-checking rules for SIMP with it dynamic semantics.</p>"},{"location":"notes/static_semantics/#definition-1-type-and-value-environments-consistency","title":"Definition 1 - Type and Value Environments Consistency","text":"<p>We say \\(\\Gamma \\vdash \\Delta\\) iff for all \\((X,C) \\in \\Delta\\) we have \\((X,T) \\in \\Gamma\\) and \\(\\Gamma \\vdash C : T\\). </p> <p>It means the type environments and value environments are consistent.</p>"},{"location":"notes/static_semantics/#property-2-progress","title":"Property 2 - Progress","text":"<p>The following property says that a well typed SIMP program must not be stuck until it reaches the return statement.</p> <p>Let \\(\\overline{S}\\) be a SIMP statement sequence. Let \\(\\Gamma\\) be a type environment such that \\(\\Gamma \\vdash \\overline{S}\\). Then \\(\\overline{S}\\) is either  1. a return statement, or  1. a sequence of statements, and there exist \\(\\Delta\\), \\(\\Delta'\\) and \\(\\overline{S'}\\) such that \\(\\Gamma \\vdash \\Delta\\) and \\((\\Delta, \\overline{S}) \\longrightarrow (\\Delta', \\overline{S'})\\).</p>"},{"location":"notes/static_semantics/#property-3-preservation","title":"Property 3 - Preservation","text":"<p>The following property says that the evaluation of a SIMP program does not change its typeability.</p> <p>Let \\(\\Delta\\), \\(\\Delta'\\) be value environments. Let \\(\\overline{S}\\) and \\(\\overline{S'}\\) be SIMP statement sequences such that \\((\\Delta, \\overline{S}) \\longrightarrow (\\Delta', \\overline{S'})\\).  Let \\(\\Gamma\\) be a type environment such that \\(\\Gamma \\vdash \\Delta\\) and \\(\\Gamma \\vdash \\overline{S}\\). Then \\(\\Gamma \\vdash \\Delta'\\) and \\(\\Gamma \\vdash \\overline{S'}\\).</p>"},{"location":"notes/static_semantics/#what-is-type-inference","title":"What is Type Inference","text":"<p>Type inference is also known as type reconstruction is a static semantics analysis process that aims to reconstruct the missing (or omitted) typing info from the source programs. </p> <p>For example, given the Haskell function</p> <pre><code>f x = x + (1::Int)\n</code></pre> <p>the compiler is able to deduce that the type of <code>f</code> is <code>Int -&gt; Int</code>. </p> <p>Likewise for the following SIMP program</p> <p><pre><code>y = y + 1\n</code></pre> we can also deduce that <code>y</code> is a of type <code>int</code>.</p> <p>What we aim to achieve is a sound and systematic process to deduce the omitted type information.</p>"},{"location":"notes/static_semantics/#type-inference-for-simp-program","title":"Type inference for SIMP program","text":"<p>Given a SIMP program \\(\\overline{S}\\), the goal of type inference is to find the \"best\" type environment \\(\\Gamma\\) such that \\(\\Gamma \\vdash \\overline{S}\\).</p> <p>Given that \\(\\Gamma\\) is a set of variable to type mappings, the \"best\" can be defined as the smallest possible set that make \\(\\overline{S}\\) typeable. This is also called the most general solution.</p>"},{"location":"notes/static_semantics/#definition-most-general-type-envrionment","title":"Definition - Most general type (envrionment)","text":"<p>Let \\(\\Gamma\\) be type environment and \\(\\overline{S}\\) be a sequence of SIMP statements, such that \\(\\Gamma \\vdash \\overline{S}\\). \\(\\Gamma\\) is the most general type environment iff for all \\(\\Gamma'\\) such that \\(\\Gamma' \\vdash \\overline{S}\\) we have \\(\\Gamma \\subseteq \\Gamma'\\).</p>"},{"location":"notes/static_semantics/#type-inference-rules","title":"Type Inference Rules","text":"<p>We would like to design type inference process using a deduction system. First of all, let's introduce some extra meta syntax terms that serve as intermediate data structures.</p> \\[ \\begin{array}{rccl} {\\tt (Extended\\ Types)} &amp; \\hat{T} &amp; ::=  &amp;\\alpha \\mid T \\\\  {\\tt (Constraints)} &amp; \\kappa &amp; \\subseteq &amp; (\\hat{T} \\times \\hat{T}) \\\\  {\\tt (Type\\ Substitution)} &amp; \\Psi &amp; ::= &amp; [\\hat{T}/\\alpha] \\mid [] \\mid \\Psi \\circ \\Psi  \\end{array} \\] <p>Where \\(\\alpha\\) denotes a type variable. \\(\\kappa\\) define a set of pairs of extended types that are supposed to be equal, e.g. \\(\\{ (\\alpha, \\beta), (\\beta, int) \\}\\) means \\(\\alpha = \\beta \\wedge \\beta = int\\).</p> <p>Type substititution replace type variable to some other type. </p> \\[ \\begin{array}{rcll} \\lbrack\\rbrack\\hat{T} &amp; = &amp; \\hat{T} \\\\  \\lbrack\\hat{T}/\\alpha\\rbrack\\alpha &amp; = &amp; \\hat{T}  \\\\ \\lbrack\\hat{T}/\\alpha\\rbrack\\beta &amp; = &amp; \\beta &amp; if\\ \\alpha \\neq \\beta \\\\ \\lbrack\\hat{T}/\\alpha\\rbrack T &amp; = &amp; T  \\end{array} \\] <p>Type substiution can be compositional.</p> \\[ \\begin{array}{rcll}  (\\Psi_1 \\circ \\Psi_2) \\hat{T} &amp; = &amp; \\Psi_1(\\Psi_2(\\hat{T})) \\end{array} \\] <p>The SIMP type inference rules are defined in terms of a deduction system consists of two type of rule forms. </p>"},{"location":"notes/static_semantics/#type-inference-rules-for-simp-statements","title":"Type Inference Rules for SIMP statements","text":"<p>The type inference rules for SIMP statements are described in a form of \\(\\overline{S} \\vDash \\kappa\\), which reads give a sequence of statements \\(\\overline{S}\\), we generate a set of type constraints \\(\\kappa\\). </p> \\[ \\begin{array}{rc} {\\tt (tiNOP)} &amp; nop\\vDash \\{\\} \\\\ \\\\  {\\tt (tiReturn)} &amp; return\\ X \\vDash \\{\\}   \\end{array} \\] <p>The \\({\\tt (tiNOP)}\\) rule handles the \\(nop\\) statement, an empty constraint set is returned.  Similar observation applies to the return statement. </p> \\[ \\begin{array}{rc} {\\tt (tiSeq)} &amp; \\begin{array}{c}                  S \\vDash \\kappa_1 \\ \\ \\ \\ \\overline{S} \\vDash \\kappa_2                 \\\\ \\hline                 S \\overline{S} \\vDash \\kappa_1 \\cup \\kappa_2                  \\end{array}  \\end{array} \\] <p>The \\({\\tt (tiSeq)}\\) rule generates the type constraints of a sequence statement \\(S\\overline{S}\\). We can do so by first generate the constraints \\(\\kappa_1\\) from \\(S\\) and \\(\\kappa_2\\) from \\(\\overline{S}\\) and union \\(\\kappa_1\\) and \\(\\kappa_2\\).  </p> \\[ \\begin{array}{rc} {\\tt (tiAssign)} &amp;  \\begin{array}{c}                     E \\vDash \\hat{T}, \\kappa                      \\\\ \\hline                     X = E \\vDash \\{ (\\alpha_X, \\hat{T}) \\} \\cup \\kappa                      \\end{array} \\\\ \\\\ \\end{array} \\] <p>The inference rule for assignment statement requires the premise \\(E \\vDash \\hat{T}, \\kappa\\), the inference for the expression \\(E\\) returning the type of \\(E\\) and a constraint set \\(\\kappa\\), which will be discussed shortly. The \\({\\tt (tiAssign)}\\) rule \"calls\" the expression inference rule to generate the type \\(\\hat{T}\\) and the constraints \\(\\kappa\\), it prepends an entry \\((\\alpha_X,\\hat{T})\\) to \\(\\kappa\\) to ensure that \\(X\\)'s type and the type of the assignment's RHS must agree. </p> \\[ \\begin{array}{rc} {\\tt (tiIf)} &amp; \\begin{array}{c}                 E \\vDash \\hat{T_1},\\kappa_1 \\ \\ \\ \\overline{S_2} \\vDash \\kappa_2 \\ \\ \\ \\ \\overline{S_3} \\vDash \\kappa_3                 \\\\ \\hline                 if\\ E\\ \\{\\overline{S_2}\\}\\ else \\{\\overline{S_3}\\} \\vDash \\{(\\hat{T_1}, bool)\\} \\cup \\kappa_1 \\cup \\kappa_2 \\cup \\kappa_3                 \\end{array} \\\\ \\\\  \\end{array} \\] <p>The inference rule for if-else statatement first infers the type of the conditional expression \\(E\\)'s type has \\(\\hat{T_1}\\) and the constraints \\(\\kappa_1\\). \\(\\kappa_2\\) and \\(\\kappa_3\\) are the constraints inferred from the then- and else-branches. The final result is forming a union of \\(\\kappa_1\\), \\(\\kappa_2\\) and \\(\\kappa_3\\), in addition, requiring \\(E\\)'s type must be \\(bool\\). </p> \\[ \\begin{array}{rc} {\\tt (tiWhile)} &amp; \\begin{array}{c}                     E \\vDash \\hat{T_1}, \\kappa_1 \\ \\ \\ \\ \\overline{S_2} \\vDash \\kappa_2                     \\\\ \\hline                     while\\ E\\ \\{\\overline{S_2}\\} \\vDash \\{(\\hat{T_1}, bool)\\} \\cup \\kappa_1 \\cup \\kappa_2                   \\end{array}  \\end{array} \\] <p>The inference for while statement is very similar to if-else statement. We skip the explanation. </p>"},{"location":"notes/static_semantics/#type-inference-rules-for-simp-expressions","title":"Type Inference Rules for SIMP expressions","text":"<p>The type inference rules for the SIMP expressions are defined in a form of \\(E \\vDash \\hat{T}, \\kappa\\). </p> \\[ \\begin{array}{rc}  {\\tt (tiInt)} &amp; \\begin{array}{c}                 C\\ {\\tt is\\ an\\ integer}                 \\\\ \\hline                 C \\vDash int, \\{\\}                 \\end{array} \\\\ \\\\  {\\tt (tiBool)} &amp; \\begin{array}{c}                 C\\ \\in \\{true, false\\}                 \\\\ \\hline                 C \\vDash bool, \\{\\}                 \\end{array}  \\end{array} \\] <p>When the expression is an integer constant, we return \\(int\\) as the inferred type and an empty constraint set. Likewise for boolean constant, we return \\(bool\\) and \\(\\{\\}\\). </p> \\[ \\begin{array}{rc} {\\tt (tiVar)} &amp; X \\vDash \\alpha_X, \\{\\}  \\end{array} \\] <p>The \\({\\tt (tiVar)}\\) rule just generates a \"skolem\" type variable \\(\\alpha_X\\) which is specifically \"reserved\" for variable \\(X\\). A skolem type variable is a type variable that is free in the current context but it has a specific \"purpose\".</p> <p>For detailed explanation of skolem variable, refer to https://stackoverflow.com/questions/12719435/what-are-skolems and https://en.wikipedia.org/wiki/Skolem_normal_form.</p> \\[ \\begin{array}{rc} {\\tt (tiOp1)} &amp; \\begin{array}{c}                 OP \\in \\{+, -, *\\} \\ \\ \\ E_1 \\vDash \\hat{T_1}, \\kappa_1\\ \\ \\ \\ E_2 \\vDash \\hat{T_2}, \\kappa_2                 \\\\ \\hline                 E_1\\ OP\\ E_2 \\vDash int, \\{(\\hat{T_1}, int), (\\hat{T_2}, int)\\} \\cup \\kappa_1 \\cup \\kappa_2                 \\end{array} \\\\ \\\\  {\\tt (tiOp2)} &amp; \\begin{array}{c}                 OP \\in \\{&lt;, ==\\} \\ \\ \\ E_1 \\vDash \\hat{T_1}, \\kappa_1\\ \\ \\ \\ E_2 \\vDash \\hat{T_2}, \\kappa_2                 \\\\ \\hline                 E_1\\ OP\\ E_2 \\vDash bool, \\{(\\hat{T_1}, \\hat{T_2})\\} \\cup \\kappa_1 \\cup \\kappa_2                 \\end{array} \\end{array} \\] <p>The rules \\({\\tt (tiOp1)}\\) and \\({\\tt (tiOp2)}\\) infer the type of binary operation expressions. Note that they can be broken into 6 different rules to be syntax-directed. \\({\\tt (tiOp1)}\\) is applied when the operator is an arithmethic operation, the returned type is \\(int\\) and the inferred constraint set is the union of the constraints inferred from the operands plus the entries of enforcing both \\(\\hat{T_1}\\) and \\(\\hat{T_2}\\) are \\(int\\). \\({\\tt (tiOp2)}\\) supports the case where the operator is a boolean comparison. </p> \\[ \\begin{array}{rc} {\\tt (tiParen)} &amp; \\begin{array}{c}                   E \\vDash \\hat{T}, \\kappa                   \\\\ \\hline                   (E) \\vDash \\hat{T}, \\kappa                   \\end{array} \\end{array} \\] <p>The inference ruel for parenthesis expression is trivial, we infer the type from the inner expression.</p>"},{"location":"notes/static_semantics/#unification","title":"Unification","text":"<p>To solve the set of generated type constraints from the above inference rules, we need to use a unification algorithm. </p> \\[ \\begin{array}{rcl} mgu(int, int) &amp; = &amp; [] \\\\  mgu(bool, bool) &amp; = &amp; [] \\\\  mgu(\\alpha, \\hat{T}) &amp; = &amp; [\\hat{T}/\\alpha] \\\\  mgu(\\hat{T}, \\alpha) &amp; = &amp; [\\hat{T}/\\alpha] \\\\ \\end{array} \\] <p>The \\(mgu(\\cdot, \\cdot)\\) function generates a type substitution that unifies the two arguments. \\(mgu\\) is a short hand for most general unifier. Note that \\(mgu\\) function is a partial function, cases that are not mentioned in the above will result in a unification failure. </p> <p>At the moment \\(mgu\\) only unifies two extended types. We overload \\(mgu()\\) to apply to a set of constraints as follows</p> \\[ \\begin{array}{rcl} mgu(\\{\\}) &amp; = &amp; [] \\\\  mgu(\\{(\\hat{T_1}, \\hat{T_2})\\} \\cup \\kappa ) &amp; = &amp; let\\ \\Psi_1 = mgu(\\hat{T_1}, \\hat{T_2}) \\\\  &amp; &amp; \\ \\ \\ \\ \\ \\ \\kappa'  = \\Psi_1(\\kappa) \\\\  &amp; &amp; \\ \\ \\ \\ \\ \\ \\Psi_2   = mgu(\\kappa') \\\\  &amp; &amp; in\\  \\Psi_2 \\circ \\Psi_1   \\end{array} \\] <p>There are two cases.</p> <ol> <li>the constraint set is empty, we return the empty (identity) substitution.</li> <li>the constriant set is non-empty, we apply the first version of \\(mgu\\) to unify one entry \\((\\hat{T_1}, \\hat{T_2})\\), which yields a subsitution \\(\\Psi_1\\). We apply \\(\\Psi_1\\) to the rest of the constraints \\(\\kappa\\) to obtain \\(\\kappa'\\). Next we apply \\(mgu\\) to \\(\\kappa'\\) recursively to generate another type substitution \\(\\Psi_2\\). The final result is a composition of \\(\\Psi_2\\) with \\(\\Psi_1\\). </li> </ol> <p>Note that the choice of the particular entry \\((\\hat{T_1}, \\hat{T_2})\\) does not matter, the algorithm will always produce the same result when we apply the final subsitution to all the skolem type variable \\(\\alpha_X\\). We see that in an example shortly. </p>"},{"location":"notes/static_semantics/#an-example","title":"An Example","text":"<p>Consider the following SIMP program</p> <pre><code>x = input;          // (\u03b1_x, \u03b1_input)      \ny = 0;              // (\u03b1_y, int)\nwhile (y &lt; x) {     // (\u03b1_y, \u03b1_x)\n    y = y + 1;      // (\u03b1_y, int)\n}\n</code></pre> <p>For the ease of access we put the inferred constraint entry as comments next to the statements. The detail derivation of the inference algorithm is as follows</p> <pre><code>input|=\u03b1_input,{} (tiVar)\n-------------------------(tiAssign)    [subtree 1]\nx=input|={(\u03b1_x,\u03b1_input)}   \n-----------------------------------------------------------------------------(tiSeq)\nx=input; y=0; while (y&lt;x) { y=y+1; } return y; |= {(\u03b1_x,\u03b1_input),(a_y,int),(\u03b1_y,\u03b1_x)} \n</code></pre> <p>Where [subtree 1] is as follows</p> <pre><code>y|=\u03b1_y,{} (tiVar)\n0|=int,{} (tiInt)\n------------------(tiAssign)   [subtree 2]\ny=0|={(\u03b1_y,int)}\n--------------------------------------------------------(tiSeq)\ny=0; while (y&lt;x) { y=y+1; } return y; |= {(a_y,int),(\u03b1_y,\u03b1_x)} \n</code></pre> <p>Where [subtree 2] is as follows</p> <pre><code>                        y|=\u03b1_y,{} (tiVar)\n                        1|=int,{} (tiInt)\ny|=\u03b1_y,{} (tiVar)       --------------(tiOp1)\nx|=\u03b1_x,{} (tiVar)       y+1|=int,{(\u03b1_y,int)}\n--------------(tiOp2)  ----------------------(tiAssign)\ny&lt;x|=bool,{(\u03b1_y,\u03b1_x)}  y=y+1|= {(\u03b1_y,int)} \n---------------------------------------------(tiWhile) --------------(tiReturn)\nwhile (y&lt;x) { y=y+1; } |= {(\u03b1_y,\u03b1_x),(\u03b1_y,int)}        return y|= {}\n---------------------------------------------------------------------(tiSeq)\nwhile (y&lt;x) { y=y+1; } return y; |= {(\u03b1_y,\u03b1_x),(\u03b1_y,int)} \n</code></pre>"},{"location":"notes/static_semantics/#from-type-substitution-to-type-environment","title":"From Type Substitution to Type Environment","text":"<p>To derive the inferred type environment, we apply the type substitution to all the type variabales we created. </p> <p>Let \\(V(\\overline{S})\\) denote all the variables used in a SIMP program \\(\\overline{S}\\).</p> <p>Given a type substitution \\(\\Psi\\) obtained from the unification step, the type environment \\(\\Gamma\\) can be computed as follows,</p> \\[ \\Gamma = \\{ (X, \\Psi(\\alpha_X)) | X \\in V(\\overline{S}) \\} \\] <p>Recall that the set of constraints generated from the running example is </p> \\[ \\{(\\alpha_x,\\alpha_{input}),(\\alpha_{y},int),(\\alpha_{y},\\alpha_{x})\\}  \\]"},{"location":"notes/static_semantics/#unification-from-left-to-right","title":"Unification from left to right","text":"<p>Suppose the unification progress pick the entries from left to right</p> \\[ \\begin{array}{ll} mgu(\\{(\\underline{\\alpha_x,\\alpha_{input}}),(\\alpha_{y},int),(\\alpha_{y},\\alpha_{x})\\}) &amp; \\longrightarrow \\\\  let\\ \\Psi_1 = mgu(\\alpha_x,\\alpha_{input}) \\\\  \\ \\ \\ \\ \\ \\ \\kappa_1 = \\Psi_1\\{(\\alpha_{y},int),(\\alpha_{y},\\alpha_{x})\\} \\\\  \\ \\ \\ \\ \\ \\ \\Psi_2 = mgu(\\kappa_1) \\\\ in\\ \\Psi_2 \\circ \\Psi_1  &amp; \\longrightarrow \\\\  let\\ \\Psi_1 = [\\alpha_{input}/ \\alpha_x] \\\\  \\ \\ \\ \\ \\ \\ \\kappa_1 = \\Psi_1\\{(\\alpha_{y},int),(\\alpha_{y},\\alpha_{x})\\} \\\\  \\ \\ \\ \\ \\ \\ \\Psi_2 = mgu(\\kappa_1) \\\\ in\\ \\Psi_2 \\circ \\Psi_1  &amp; \\longrightarrow \\\\  let\\ \\Psi_1 = [\\alpha_{input}/ \\alpha_x] \\\\  \\ \\ \\ \\ \\ \\ \\kappa_1 = \\{(\\alpha_{y},int),(\\alpha_{y},\\alpha_{input})\\} \\\\  \\ \\ \\ \\ \\ \\ \\Psi_2 = mgu(\\kappa_1) \\\\ in\\ \\Psi_2 \\circ \\Psi_1 &amp; \\longrightarrow  \\end{array} \\] <p>Where derivation of \\(mgu(\\kappa_1)\\) </p> \\[ \\begin{array}{ll} mgu(\\{(\\underline{\\alpha_{y},int}),(\\alpha_{y},\\alpha_{input})\\}) &amp; \\longrightarrow \\\\  let\\ \\Psi_{21} = mgu(\\alpha_{y},int) \\\\ \\ \\ \\ \\ \\ \\ \\kappa_2 = \\Psi_{21}\\{(\\alpha_{y},\\alpha_{input})\\} \\\\  \\ \\ \\ \\ \\ \\ \\Psi_{22} = mgu(\\kappa_2) \\\\  in\\ \\Psi_{22} \\circ \\Psi_{21} &amp; \\longrightarrow \\\\ let\\ \\Psi_{21} = [int/\\alpha_{y}] \\\\ \\ \\ \\ \\ \\ \\ \\kappa_2 = \\Psi_{21}\\{(\\alpha_{y},\\alpha_{input})\\} \\\\  \\ \\ \\ \\ \\ \\ \\Psi_{22} = mgu(\\kappa_2) \\\\  in\\ \\Psi_{22} \\circ \\Psi_{21} &amp; \\longrightarrow \\\\ let\\ \\Psi_{21} = [int/\\alpha_{y}] \\\\ \\ \\ \\ \\ \\ \\ \\kappa_2 = \\{(int,\\alpha_{input})\\} \\\\  \\ \\ \\ \\ \\ \\ \\Psi_{22} = mgu(\\kappa_2) \\\\  in\\ \\Psi_{22} \\circ \\Psi_{21} &amp; \\longrightarrow \\\\ let\\ \\Psi_{21} = [int/\\alpha_{y}] \\\\ \\ \\ \\ \\ \\ \\ \\kappa_2 = \\{(int,\\alpha_{input})\\} \\\\  \\ \\ \\ \\ \\ \\ \\Psi_{22} = [int/\\alpha_{input}] \\\\  in\\ \\Psi_{22} \\circ \\Psi_{21} &amp; \\longrightarrow \\\\ [int/\\alpha_{input}] \\circ [int/\\alpha_{y}] \\end{array} \\] <p>Hence the final result is </p> \\[  [int/\\alpha_{input}] \\circ [int/\\alpha_{y}] \\circ [\\alpha_{input}/ \\alpha_x] \\] <p>We apply this type substitution to all the variables in the program.</p> \\[ \\begin{array}{rl} ([int/\\alpha_{input}] \\circ [int/\\alpha_{y}] \\circ [\\alpha_{input}/ \\alpha_x])\\alpha_{input} &amp; = \\\\   ([int/\\alpha_{input}] \\circ [int/\\alpha_{y}])\\alpha_{input} &amp; = \\\\   [int/\\alpha_{input}]  \\alpha_{input} &amp; = \\\\   int \\\\ \\\\  ([int/\\alpha_{input}] \\circ [int/\\alpha_{y}] \\circ [\\alpha_{input}/ \\alpha_x])\\alpha_{x} &amp; = \\\\   ([int/\\alpha_{input}] \\circ [int/\\alpha_{y}])\\alpha_{input} &amp; = \\\\   [int/\\alpha_{input}]  \\alpha_{input} &amp; = \\\\   int \\\\ \\\\  ([int/\\alpha_{input}] \\circ [int/\\alpha_{y}] \\circ [\\alpha_{input}/ \\alpha_x])\\alpha_{y} &amp; = \\\\   ([int/\\alpha_{input}] \\circ [int/\\alpha_{y}]) \\alpha_{y} &amp; = \\\\   [int/\\alpha_{input}] int &amp; = \\\\   int \\\\ \\\\  \\end{array} \\] <p>So we have computed the inferred type environment</p> \\[ \\Gamma = \\{(input, int), (x, int), (y,int) \\} \\]"},{"location":"notes/static_semantics/#unification-from-right-to-left","title":"Unification from right to left","text":"<p>Now let's consider a different of order of applying the \\(mgu\\) function to the constraint set. Instead of going from left to right, we solve the constraints from right to left. </p> \\[ \\begin{array}{ll} mgu(\\{(\\alpha_x,\\alpha_{input}),(\\alpha_{y},int),(\\underline{\\alpha_{y},\\alpha_{x}})\\}) &amp; \\longrightarrow \\\\  let\\ \\Psi_1 = mgu(\\alpha_{y},\\alpha_{x}) \\\\  \\ \\ \\ \\ \\ \\ \\kappa_1 = \\Psi_1\\{(\\alpha_x,\\alpha_{input}),(\\alpha_{y},int)\\} \\\\  \\ \\ \\ \\ \\ \\ \\Psi_2 = mgu(\\kappa_1) \\\\ in\\ \\Psi_2 \\circ \\Psi_1  &amp; \\longrightarrow \\\\  let\\ \\Psi_1 = [\\alpha_{x}/ \\alpha_y] \\\\  \\ \\ \\ \\ \\ \\ \\kappa_1 = \\Psi_1\\{(\\alpha_x,\\alpha_{input}),(\\alpha_{y},int)\\} \\\\  \\ \\ \\ \\ \\ \\ \\Psi_2 = mgu(\\kappa_1) \\\\ in\\ \\Psi_2 \\circ \\Psi_1  &amp; \\longrightarrow \\\\  let\\ \\Psi_1 = [\\alpha_{x}/ \\alpha_y] \\\\  \\ \\ \\ \\ \\ \\ \\kappa_1 = \\{(\\alpha_x,\\alpha_{input}),(\\alpha_{x},int)\\} \\\\  \\ \\ \\ \\ \\ \\ \\Psi_2 = mgu(\\kappa_1) \\\\ in\\ \\Psi_2 \\circ \\Psi_1 &amp; \\longrightarrow  \\end{array} \\] <p>Where derivation of \\(mgu(\\kappa_1)\\) </p> \\[ \\begin{array}{ll} mgu(\\{(\\alpha_x,\\alpha_{input}),(\\underline{\\alpha_{x},int})\\}) &amp; \\longrightarrow \\\\  let\\ \\Psi_{21} = mgu(\\alpha_{x},int) \\\\ \\ \\ \\ \\ \\ \\ \\kappa_2 = \\Psi_{21}\\{(\\alpha_{x},\\alpha_{input})\\} \\\\  \\ \\ \\ \\ \\ \\ \\Psi_{22} = mgu(\\kappa_2) \\\\  in\\ \\Psi_{22} \\circ \\Psi_{21} &amp; \\longrightarrow \\\\ let\\ \\Psi_{21} = [int/\\alpha_{x}] \\\\ \\ \\ \\ \\ \\ \\ \\kappa_2 = \\Psi_{21}\\{(\\alpha_{x},\\alpha_{input})\\} \\\\  \\ \\ \\ \\ \\ \\ \\Psi_{22} = mgu(\\kappa_2) \\\\  in\\ \\Psi_{22} \\circ \\Psi_{21} &amp; \\longrightarrow \\\\ let\\ \\Psi_{21} = [int/\\alpha_{x}] \\\\ \\ \\ \\ \\ \\ \\ \\kappa_2 = \\{(int,\\alpha_{input})\\} \\\\  \\ \\ \\ \\ \\ \\ \\Psi_{22} = mgu(\\kappa_2) \\\\  in\\ \\Psi_{22} \\circ \\Psi_{21} &amp; \\longrightarrow \\\\ let\\ \\Psi_{21} = [int/\\alpha_{x}] \\\\ \\ \\ \\ \\ \\ \\ \\kappa_2 = \\{(int,\\alpha_{input})\\} \\\\  \\ \\ \\ \\ \\ \\ \\Psi_{22} = [int/\\alpha_{input}] \\\\  in\\ \\Psi_{22} \\circ \\Psi_{21} &amp; \\longrightarrow \\\\ [int/\\alpha_{input}] \\circ [int/\\alpha_{x}] \\end{array} \\] <p>Hence the final result is </p> \\[  [int/\\alpha_{input}] \\circ [int/\\alpha_{x}] \\circ [\\alpha_{x}/ \\alpha_y] \\] <p>We apply this type substitution to all the variables in the program.</p> \\[ \\begin{array}{rl} ([int/\\alpha_{input}] \\circ [int/\\alpha_{x}] \\circ [\\alpha_{x}/ \\alpha_y])\\alpha_{input} &amp; = \\\\   ([int/\\alpha_{input}] \\circ [int/\\alpha_{x}])\\alpha_{input} &amp; = \\\\   [int/\\alpha_{input}]  \\alpha_{input} &amp; = \\\\   int \\\\ \\\\  ([int/\\alpha_{input}] \\circ [int/\\alpha_{x}] \\circ [\\alpha_{x}/ \\alpha_y])\\alpha_{x} &amp; = \\\\   ([int/\\alpha_{input}] \\circ [int/\\alpha_{x}])\\alpha_{x} &amp; = \\\\   [int/\\alpha_{input}]  int &amp; = \\\\   int \\\\ \\\\  ([int/\\alpha_{input}] \\circ [int/\\alpha_{x}] \\circ [\\alpha_{x}/ \\alpha_y])\\alpha_{y} &amp; = \\\\   ([int/\\alpha_{input}] \\circ [int/\\alpha_{x}]) \\alpha_{x} &amp; = \\\\   [int/\\alpha_{input}] int &amp; = \\\\   int \\\\ \\\\  \\end{array} \\] <p>So we have computed the inferred the same type environment</p> \\[ \\Gamma = \\{(input, int), (x, int), (y,int) \\} \\] <p>In face regardless the order of picking entries from the constraint sets, we compute the same \\(\\Gamma\\).  </p> <p>If you have time, you can try another order.</p>"},{"location":"notes/static_semantics/#inputs-type","title":"Input's type","text":"<p>In our running example, our inference algorithm is able to infer the program's input type i.e. \\(\\alpha_{input}\\).</p> <p>This is not always possible. Let's consider the following program.</p> <pre><code>x = input;          // (\u03b1_x, \u03b1_input)      \ny = 0;              // (\u03b1_y, int)\nwhile (y &lt; 3) {     // (\u03b1_y, int)\n    y = y + 1;      // (\u03b1_y, int)\n}\n</code></pre> <p>In the genereated constraints, our algorithm can construct the subtitution </p> \\[[\\alpha_{input}/\\alpha_x] \\circ [int/\\alpha_y]\\] <p>Which fails to \"ground\" type variables \\(\\alpha_{input}\\) and \\(\\alpha_x\\). </p> <p>We may argue that this is an ill-defined program as <code>input</code> and <code>x</code> are not used in the rest of the program, which should be rejected if we employ some name analysis, (which we will learn in the upcoming lesson). Hence we simply reject this kind of programs. </p> <p>Alternatively, we can preset the type of the program, which is a common practice for many program languages. When generating the set of constraint \\(\\kappa\\), we manually add an entry \\((\\alpha_{input}, int)\\) assuming the input's type is expected to be \\(int\\). </p>"},{"location":"notes/static_semantics/#uninitialized-variable","title":"Uninitialized Variable","text":"<p>There is another situatoin in which the inference algorithm fails to ground all the type variables.</p> <p><pre><code>x = z;              // (\u03b1_x, \u03b1_z)      \ny = 0;              // (\u03b1_y, int)\nwhile (y &lt; 3) {     // (\u03b1_y, int)\n    y = y + 1;      // (\u03b1_y, int)\n}\n</code></pre> in this case, we can't ground \\(\\alpha_x\\) and \\(\\alpha_z\\) as <code>z</code> is not initialized before use. In this case we argue that such a program should be rejected either by the type inference or the name analysis.</p>"},{"location":"notes/static_semantics/#property-4-type-inference-soundness","title":"Property 4: Type Inference Soundness","text":"<p>The following property states that the type environment generated from a SIMP program by the type inference algorithm is able to type check the SIMP program.</p> <p>Let \\(\\overline{S}\\) be a SIMP program and \\(\\Gamma\\) is a type environment inferred using the described inference algorithm. Then \\(\\Gamma \\vdash \\overline{S}\\). </p>"},{"location":"notes/static_semantics/#property-5-principality","title":"Property 5: Principality","text":"<p>The following property states that the type environment generated from a SIMP program by the type inference algorithm is a principal type environment.</p> <p>Let \\(\\overline{S}\\) be a SIMP program and \\(\\Gamma\\) is a type environment inferred using the described inference algorithm. Then \\(\\Gamma\\) is the most general type environment that can type-check \\(\\overline{S}\\). </p>"},{"location":"notes/static_semantics_2/","title":"50.054 Static Semantics for Lambda Calculus","text":""},{"location":"notes/static_semantics_2/#learning-outcomes","title":"Learning Outcomes","text":"<ol> <li>Apply type checking algorithm to type check a simply typed lambda calculus expression.</li> <li>Apply Hindley Milner algorithm to type check lambda calculus expressions.</li> <li>Apply Algorithm W to infer type for lambda calculus.</li> </ol>"},{"location":"notes/static_semantics_2/#type-checking-for-lambda-calculus","title":"Type Checking for Lambda Calculus","text":"<p>To illustrate the proocess of type checking for lambda calculus, we consider adding types and type annotations to the lambda calculus language. </p> <p>Recall the lambda calculus syntax, with the following adaptation</p> \\[ \\begin{array}{rccl}  {\\tt (Lambda\\ Terms)} &amp; t &amp; ::= &amp; x \\mid \\lambda x:T.t \\mid t\\ t \\mid let\\ x:T =\\ t\\ in\\ t \\mid  if\\ t\\ then\\ t\\ else\\ t \\mid t\\ op\\ t \\mid c \\mid fix\\ t \\\\  {\\tt (Builtin\\ Operators)} &amp; op &amp; ::= &amp; + \\mid - \\mid * \\mid / \\mid\\ == \\\\  {\\tt (Builtin\\ Constants)} &amp; c &amp; ::= &amp; 0 \\mid 1 \\mid ... \\mid true \\mid false \\\\  {\\tt (Types)} &amp; T &amp; ::= &amp; int \\mid bool \\mid T \\rightarrow T \\\\   {\\tt (Type\\ Environments)} &amp; \\Gamma &amp; \\subseteq &amp; (x \\times T) \\end{array} \\] <p>The difference is that the lambda abstraction \\(\\lambda x:T.t\\) now carries a type annotation of the lambda-bound variable. (Similar observation applies to let-binding) \\(T\\) is a type symbol which can be \\(int\\) or \\(bool\\) or a function type \\(T \\rightarrow T\\). The \\(\\rightarrow\\) type operator is right associative, i.e. \\(T_1 \\rightarrow T_2 \\rightarrow T_3\\) is parsed as \\(T_1 \\rightarrow (T_2 \\rightarrow T_3)\\). Let's call this extended version of lambda calculus as Simply Typed Lambda Calculus.</p> <p>Note that all the existing definitions for dynamic semantics of lambda calculus can be brought-forward (and extended) to support Simply Typed Lambda Calculus. We omit the details.</p> <p>We formalize the type-checking process in a relation \\(\\Gamma \\vdash t : T\\), where \\(\\Gamma\\) is a mapping from variables to types. We write \\(dom(\\Gamma)\\) to denote the domain of \\(\\Gamma\\), i.e. \\(\\{ X \\mid (x,T) \\in \\Gamma \\}\\). We assume for all \\(x \\in dom(\\Gamma)\\), there exists only one entry of \\((x,T) \\in \\Gamma\\).</p> <p>Since \\(\\Gamma \\vdash t : T\\) is relation, what type-checking attempts to verify is the following. Given a type environment \\(\\Gamma\\) and lambda term \\(t\\) and a type \\(T\\), \\(t\\) can be given a type \\(T\\) under \\(\\Gamma\\).</p> \\[ \\begin{array}{cc} {\\tt (lctInt)} &amp; \\begin{array}{c} \\\\                       c\\ {\\tt is\\ an\\ integer}                       \\\\ \\hline                       \\Gamma \\vdash c : int                       \\end{array} \\\\ \\\\   {\\tt (lctBool)} &amp; \\begin{array}{c}                        c\\in \\{ true, false\\}                       \\\\ \\hline                       \\Gamma \\vdash c : bool                       \\end{array} \\end{array} \\] <p>The rule \\({\\tt (lctInt)}\\) checks whether the given constant value is an integer. The rule \\({\\tt (lctBool)}\\) checks whether the given constant value is a boolean.</p> \\[ \\begin{array}{cc} {\\tt (lctVar)} &amp; \\begin{array}{c}                 (x, T) \\in \\Gamma  \\\\                \\hline                \\Gamma \\vdash x : T                 \\end{array}  \\end{array} \\] <p>In rule \\({\\tt (lctVar)}\\), we type check a variable \\(x\\) against a type \\(T\\), which is only valid where \\((x,T)\\) can be found in the type environment \\(\\Gamma\\).</p> \\[ \\begin{array}{cc} {\\tt (lctLam)} &amp; \\begin{array}{c}                \\Gamma \\oplus (x, T) \\vdash t : T'  \\\\                \\hline                \\Gamma \\vdash \\lambda x : T.t :T \\rightarrow T'                 \\end{array}  \\end{array} \\] <p>In rule \\({\\tt (lctLam)}\\), we type check a lambda abstraction against a type \\(T\\rightarrow T'\\). This is only valid if the body of the lambda expression \\(t\\) has type \\(T'\\) under the extended type environment \\(\\Gamma \\oplus (x, T)\\).</p> \\[ \\begin{array}{cc} {\\tt (lctApp)} &amp; \\begin{array}{c}                \\Gamma \\vdash t_1 : T_1 \\rightarrow T_2 \\ \\ \\ \\                \\Gamma \\vdash t_2 : T_1 \\\\                \\hline                \\Gamma \\vdash  t_1\\ t_2 :T_2                 \\end{array}  \\end{array} \\] <p>In rule \\({\\tt (lctApp)}\\), we type check a function application, applying \\(t_1\\) to \\(t_2\\), against a type \\(T_2\\). This is only valid if \\(t_1\\) is having type \\(T_1 \\rightarrow T_2\\) and \\(t_2\\) is having type \\(T_1\\).</p> \\[ \\begin{array}{cc} {\\tt (lctLet)} &amp; \\begin{array}{c}                \\Gamma \\vdash t_1 : T_1 \\ \\ \\ \\                \\Gamma \\oplus (x, T_1) \\vdash t_2 : T_2 \\\\                \\hline                \\Gamma \\vdash  let\\ x:T_1 = t_1\\ in\\ t_2 :T_2                 \\end{array}  \\end{array} \\] <p>In rule \\({\\tt (lctLet)}\\), we type check a let binding, \\(let\\ x:T_1 = t_1\\ in\\ t_2\\) against type \\(T_2\\). This is only valid if \\(t_1\\) has type \\(T_1\\) and \\(t_2\\) has type \\(T_2\\) under the extended environment  \\(\\Gamma \\oplus (x, T_1)\\).</p> \\[ \\begin{array}{cc} {\\tt (lctIf)} &amp; \\begin{array}{c}                \\Gamma \\vdash t_1 : bool \\ \\ \\ \\Gamma \\vdash t_2 : T \\ \\ \\ \\ \\Gamma \\vdash t_3 : T \\\\                \\hline                \\Gamma \\vdash  if\\ t_1\\ then\\ t_2\\ else\\ t_3 : T                 \\end{array} \\end{array} \\] <p>In rule \\({\\tt (lctIf)}\\), we type check a if-then-else expression against type \\(T\\). This is only valid if  \\(t_1\\) has type \\(bool\\) and both \\(t_1\\) and \\(t_2\\) have type \\(T\\).</p> \\[ \\begin{array}{cc} {\\tt (lctOp1)} &amp; \\begin{array}{c}                \\Gamma \\vdash t_1 : int \\ \\ \\ \\Gamma \\vdash t_2 : int \\ \\ \\ op\\in\\{+,-,*,/\\} \\\\                \\hline                \\Gamma \\vdash  t_1\\ op\\ t_2 : int                 \\end{array} \\\\ \\\\  {\\tt (lctOp2)} &amp; \\begin{array}{c}                \\Gamma \\vdash t_1 : int \\ \\ \\ \\Gamma \\vdash t_2 : int \\\\                \\hline                \\Gamma \\vdash  t_1\\ ==\\ t_2 : bool                 \\end{array} \\\\ \\\\  {\\tt (lctOp3)} &amp; \\begin{array}{c}                \\Gamma \\vdash t_1 : bool \\ \\ \\ \\Gamma \\vdash t_2 : bool \\\\                \\hline                \\Gamma \\vdash  t_1\\ ==\\ t_2 : bool                 \\end{array} \\\\ \\\\  \\end{array} \\] <p>The above three rules type check the binary operations. \\({\\tt (lctOp1)}\\) handles the case where the \\(op\\) is an arithmatic operation, which requires both operands having type \\(int\\). \\({\\tt (lctOp2)}\\) and \\({\\tt (lctOp3)}\\) handle the case where \\(op\\) is the equality test. In this case, the types of the operands must agree.</p> \\[ \\begin{array}{cc} {\\tt (lctFix)} &amp; \\begin{array}{c}                 \\Gamma \\vdash t : (T_1 \\rightarrow T_2) \\rightarrow T_1 \\rightarrow T_2                 \\\\  \\hline                 \\Gamma \\vdash fix\\ t:T_1 \\rightarrow T_2                \\end{array}  \\end{array} \\] <p>The last rule \\({\\tt (lctFix)}\\) type checks the fix operator application against the type \\(T_1 \\rightarrow T_2\\). We enforce that the argument \\(t\\) must be a fixed point function of type \\((T_1 \\rightarrow T_2) \\rightarrow T_1 \\rightarrow T_2\\).</p> <p>For example, we would like to type check the following simply typed lambda term.</p> <p>$$ fix (\\lambda f:int\\rightarrow int.(\\lambda x:int. (if x == 0 then 1 else (f (x-1))* x))) $$ against the type \\(int \\rightarrow int\\)</p> <p>We added the optional parantheses for readability. </p> <p>We find the the following type checking derivation (proof tree).</p> <p>Let <code>\u0393</code> be the initial type environment.</p> <pre><code>\u0393\u2295(f:int-&gt;int)\u2295(x:int)|- x:int (lctVar)\n\u0393\u2295(f:int-&gt;int)\u2295(x:int)|- 0:int (lctInt)\n---------------------------------------(lctOp2)  [sub tree 1]   [sub tree 2]\n\u0393\u2295(f:int-&gt;int)\u2295(x:int)|- x == 0: bool\n------------------------------------------------------------------------------- (lctIf)\n\u0393\u2295(f:int-&gt;int)\u2295(x:int)|-if x == 0 then 1 else (f (x-1))*x:int\n--------------------------------------------------------------------(lctLam)\n\u0393\u2295(f:int-&gt;int)|-\u03bbx:int.(if x == 0 then 1 else (f (x-1))*x):int-&gt;int\n--------------------------------------------------------------------------------(lctLam)\n\u0393 |- \u03bbf:int-&gt;int.(\u03bbx:int.(if x == 0 then 1 else (f (x-1))*x)):(int-&gt;int)-&gt;int-&gt;int\n---------------------------------------------------------------------------------(lctFix)\n\u0393 |- fix (\u03bbf:int-&gt;int.(\u03bbx:int.(if x == 0 then 1 else (f (x-1))*x))):int-&gt;int\n</code></pre> <p>Let <code>\u03931=\u0393\u2295(f:int-&gt;int)\u2295(x:int)</code> Where [sub tree 1] is </p> <pre><code>\u03931|- 1:int (lctInt)\n</code></pre> <p>and [sub tree 2] is  <pre><code>                           \u03931|-x:int (lctVar) \n                           \u03931|-1:int (lctInt)\n                           -----------------(lctOp1)\n\u03931|- f:int-&gt;int (lctVar)   \u03931|- x-1:int \n-------------------------------------------------(lctApp)  \n\u03931|- f (x-1):int                                           \u03931 |- x:int (lctVar)\n-------------------------------------------------------------------------(lctOp1)\n\u03931|- (f (x-1))*x:int\n</code></pre></p> <p>Another (counter) example which shows that we can't type check the following program </p> \\[ let\\ x:int = 1\\ in\\ (if\\ x\\ then\\ x\\ else\\ 0) \\] <p>against the type \\(int\\).</p> <pre><code>                   fail, no proof exists\n                   ---------------------- \n                   \u0393\u2295(x:int)|- x:bool\n                   ----------------------------------(lctIf)\n\u0393|-1:int (lctInt)  \u0393\u2295(x:int)|-if x then x else 0:int\n--------------------------------------------------------(lctLet)\n\u0393|- let x:int = 1 in (if x then x else 0):int\n</code></pre>"},{"location":"notes/static_semantics_2/#property-1-uniqueness","title":"Property 1 - Uniqueness","text":"<p>The following property states that if a lambda term is typable, its type must be unique.</p> <p>Let \\(t\\) be a simply typed lambda calculus term. Let \\(\\Gamma\\) be a type environment such that for all \\(x \\in fv(t)\\), \\(x \\in dom(\\Gamma)\\). Let \\(T\\) and \\(T'\\) be types such that \\(\\Gamma \\vdash t : T\\) and \\(\\Gamma \\vdash t:T'\\). Then \\(T\\) and \\(T'\\) must be the same.</p> <p>Where \\(dom(\\Gamma)\\) refers to the domain of \\(\\Gamma\\), i.e. all the variables being mapped.</p>"},{"location":"notes/static_semantics_2/#property-2-progress","title":"Property 2 - Progress","text":"<p>The second property states that if a closed lambda term is typeable under the empty type environment, it must be runnable and not getting stuck.</p> <p>Let \\(t\\) be a simply typed lambda calculus term such that \\(fv(t) = \\{\\}\\).  Let \\(T\\) be a type such that \\(\\{\\} \\vdash t : T\\). Then \\(t\\) is either a value or there exists some \\(t'\\) such that \\(t \\longrightarrow t'\\).</p>"},{"location":"notes/static_semantics_2/#property-3-preservation","title":"Property 3 - Preservation","text":"<p>The third property states that the type of a lambda term does not change over evaluation.</p> <p>Let \\(t\\) and \\(t'\\) be simply typed lambda calculus terms such that \\(t \\longrightarrow t'\\). Let \\(T\\) be a type and \\(\\Gamma\\) be a type environment such that \\(\\Gamma \\vdash t:T\\). Then \\(\\Gamma \\vdash t':T\\).</p>"},{"location":"notes/static_semantics_2/#issue-with-let-binding","title":"Issue with let-binding","text":"<p>The current type checking rules for Simply-typed Lambda Calculus fails to type check the following lambda calculus term.</p> \\[ \\begin{array}{l} let\\ f = \\lambda x:\\alpha.x \\\\ in\\ let\\ g = \\lambda x:int.\\lambda y:bool.x \\\\  \\ \\ \\ \\ in\\ (g\\ (f\\ 1)\\ (f\\ true)) \\end{array} \\] <p>Where \\(\\alpha\\) denotes some generic type. This is due to the fact that we can only give one type to <code>f</code>, either \\(Int \\rightarrow Int\\) or \\(Bool \\rightarrow Bool\\) but not both.</p> <p>To type check the above program we need to get rid of the type annotations to the let binding (as well as lambda abstraction). This leads us to the Hindley-Milner Type System.</p>"},{"location":"notes/static_semantics_2/#hindley-milner-type-system","title":"Hindley Milner Type System","text":"<p>We define the lambda calculus syntax for Hindley Milner Type System as follows</p> \\[ \\begin{array}{rccl}  {\\tt (Lambda\\ Terms)} &amp; t &amp; ::= &amp; x \\mid \\lambda x.t \\mid t\\ t \\mid let\\ x =\\ t\\ in\\ t \\mid  if\\ t\\ then\\ t\\ else\\ t \\mid t\\ op\\ t \\mid c \\mid fix\\ t \\\\  {\\tt (Builtin\\ Operators)} &amp; op &amp; ::= &amp; + \\mid - \\mid * \\mid / \\mid\\ == \\\\  {\\tt (Builtin\\ Constants)} &amp; c &amp; ::= &amp; 0 \\mid 1 \\mid ... \\mid true \\mid false \\\\  {\\tt (Types)} &amp; T &amp; ::= &amp; int \\mid bool \\mid T \\rightarrow T \\mid \\alpha \\\\   {\\tt (Type Scheme)} &amp; \\sigma &amp; ::= &amp; \\forall \\alpha. \\sigma \\mid T \\\\   {\\tt (Type\\ Environments)} &amp; \\Gamma &amp; \\subseteq &amp; (x \\times \\sigma ) \\\\  {\\tt (Type\\ Substitution)} &amp; \\Psi &amp; ::= &amp; [T/\\alpha] \\mid [] \\mid \\Psi \\circ \\Psi  \\end{array} \\] <p>In the above grammar rules, we remove the type annotations from the lambda abstraction and let binding. Our type inference algorithm should be able to recover them.  We add the type variable directly to the type \\(T\\) rule instead of introducing the \\(\\hat{T}\\) rule for conciseness. We introduce a type scheme term \\(\\sigma\\) which is required for polymorphic types.  </p> <p>We describe the Hindley Milner Type Checking rules as follows</p> \\[ \\begin{array}{rc} {\\tt (hmInt)} &amp; \\begin{array}{c} \\\\                       c\\ {\\tt is\\ an\\ integer}                       \\\\ \\hline                       \\Gamma \\vdash c : int                       \\end{array} \\\\ \\\\  {\\tt (hmBool)} &amp; \\begin{array}{c}                        c\\in \\{ true, false\\}                       \\\\ \\hline                       \\Gamma \\vdash c : bool                       \\end{array} \\end{array} \\] <p>The rules for constants remain unchanged. </p> \\[ \\begin{array}{rc} {\\tt (hmVar)} &amp; \\begin{array}{c}                 (x,\\sigma) \\in \\Gamma                 \\\\ \\hline                 \\Gamma \\vdash x : \\sigma                 \\end{array}  \\end{array} \\] <p>The rule for variable is adjusted to use type signatures instead of types.</p> \\[ \\begin{array}{rc} {\\tt (hmLam)} &amp; \\begin{array}{c}                \\Gamma \\oplus (x, T) \\vdash t : T'  \\\\                \\hline                \\Gamma \\vdash \\lambda x.t :T\\rightarrow T'                 \\end{array} \\\\ \\\\  {\\tt (hmApp)} &amp; \\begin{array}{c}                \\Gamma \\vdash t_1 : T_1 \\rightarrow T_2 \\ \\ \\ \\                \\Gamma \\vdash t_2 : T_1 \\\\                \\hline                \\Gamma \\vdash  t_1\\ t_2 :T_2                 \\end{array}  \\end{array} \\] <p>In rule \\({\\tt (hmLam)}\\) we type check the lambda abstraction against \\(T\\rightarrow T'\\). It is largely the same as the \\({\\tt (lctLam)}\\) rule for simply typed lambda calculus, except that there is no type annotation to the lambda bound variable \\(x\\). The rule \\({\\tt (hmApp)}\\) is exactly the same as \\({\\tt (lctApp)}\\). </p> \\[ \\begin{array}{rc} {\\tt (hmFix)} &amp; \\begin{array}{c}                 (fix,\\forall \\alpha. (\\alpha\\rightarrow \\alpha)\\rightarrow \\alpha)\\in \\Gamma                 \\\\ \\hline                  \\Gamma \\vdash fix:\\forall \\alpha. (\\alpha\\rightarrow \\alpha) \\rightarrow \\alpha                 \\end{array} \\end{array} \\] <p>To type check the \\(fix\\) operator, we assume that \\(fix\\) is predefined in the language library and its type is given in the initial type environment \\(\\Gamma_{init}\\).</p> \\[ \\begin{array}{rc} {\\tt (hmIf)} &amp; \\begin{array}{c}                 \\Gamma \\vdash t_1 : bool \\ \\ \\                  \\Gamma \\vdash t_2 : \\sigma \\ \\ \\                  \\Gamma \\vdash t_3 : \\sigma                  \\\\ \\hline                 \\Gamma \\vdash if\\ t_1\\ \\{ t_2\\}\\ else \\{ t_3 \\}: \\sigma                \\end{array} \\\\ \\\\  \\end{array} \\] <p>We made minor adjustment to the rule handling if-else expression, by replacing \\(T\\) with \\(\\sigma\\).</p> \\[ \\begin{array}{rc} {\\tt (hmOp1)} &amp; \\begin{array}{c}                \\Gamma \\vdash t_1 : int \\ \\ \\ \\Gamma \\vdash t_2 : int \\ \\ \\ op\\in\\{+,-,*,/\\} \\\\                \\hline                \\Gamma \\vdash  t_1\\ op\\ t_2 : int                 \\end{array} \\\\ \\\\  {\\tt (hmOp2)} &amp; \\begin{array}{c}                \\Gamma \\vdash t_1 : int \\ \\ \\ \\Gamma \\vdash t_2 : int \\\\                \\hline                \\Gamma \\vdash  t_1\\ ==\\ t_2 : bool                 \\end{array} \\\\ \\\\  {\\tt (hmOp3)} &amp; \\begin{array}{c}                \\Gamma \\vdash t_1 : bool \\ \\ \\ \\Gamma \\vdash t_2 : bool \\\\                \\hline                \\Gamma \\vdash  t_1\\ ==\\ t_2 : bool                 \\end{array} \\\\ \\\\   \\end{array} \\] <p>The type checking rules for binary operation remain unchanged.</p> \\[ \\begin{array}{rc} {\\tt (hmLet)} &amp; \\begin{array}{c}                \\Gamma \\vdash t_1 : \\sigma_1 \\ \\ \\ \\                \\Gamma \\oplus (x, \\sigma_1) \\vdash t_2 : T_2 \\\\                \\hline                \\Gamma \\vdash  let\\ x = t_1\\ in\\ t_2 :T_2                 \\end{array} \\\\ \\\\  {\\tt (hmInst)} &amp; \\begin{array}{c}                 \\Gamma \\vdash t : \\sigma_1 \\ \\ \\ \\ \\sigma_1 \\sqsubseteq \\sigma_2                 \\\\ \\hline                 \\Gamma \\vdash t : \\sigma_2                 \\end{array} \\\\ \\\\  {\\tt (hmGen)} &amp; \\begin{array}{c}                 \\Gamma \\vdash t : \\sigma \\ \\ \\ \\ \\alpha \\not\\in ftv(\\Gamma)                 \\\\ \\hline                  \\Gamma \\vdash t : \\forall \\alpha.\\sigma                 \\end{array} \\end{array} \\] <p>In the rule \\({\\tt (hmLet)}\\), we first type check \\(t_1\\) againt \\(\\sigma_1\\), which is a type scheme, which allows \\(t_1\\) to have a generic type. Under the extended type environment \\(\\Gamma \\oplus (x, \\sigma_1)\\) we type-check \\(t_2\\). </p> <p>For the \\({\\tt (hmLet)}\\) rule to work as intended, we need two more rules, namely, \\({\\tt (hmInst)}\\) and \\({\\tt (hmGen)}\\). In rule \\({\\tt (hmInst)}\\) we allow a term \\(t\\) to be type-checked against \\(\\sigma_2\\), provided we can type check it against \\(\\sigma_1\\) and \\(\\sigma_1 \\sqsubseteq \\sigma_2\\).</p>"},{"location":"notes/static_semantics_2/#definition-type-instances","title":"Definition - Type Instances","text":"<p>Let \\(\\sigma_1\\) and \\(\\sigma_2\\) be type schemes. We say \\(\\sigma_1 \\sqsubseteq \\sigma_2\\) iff \\(\\sigma_1 = \\forall \\alpha. \\sigma_1'\\) and there exists a type subsitution \\(\\Psi\\) such that \\(\\Psi(\\sigma_1') = \\sigma_2\\).</p> <p>In otherwords, we say \\(\\sigma_1\\) is more general that \\(\\sigma_2\\) and \\(\\sigma_2\\) is a type instance of \\(\\sigma_1\\).</p> <p>Finally the rule \\({\\tt (hmGen)}\\) generalizes existing type to type schemes. In this rule, if a term \\(t\\) can be type-checked against a type scheme \\(\\sigma\\), then \\(t\\) can also be type-checked against \\(\\forall \\alpha.\\sigma\\) if \\(\\alpha\\) is not a free type variable in \\(\\Gamma\\).</p> <p>The type variable function \\(ftv()\\) can be defined similar to the \\(fv()\\) function we introduced for lambda caculus. </p> \\[ \\begin{array}{rcl} ftv(\\alpha) &amp; = &amp; \\{\\alpha \\} \\\\  ftv(int) &amp; = &amp; \\{ \\} \\\\ ftv(bool) &amp; = &amp; \\{ \\} \\\\ ftv(T_1 \\rightarrow T_2) &amp; = &amp; ftv(T_1) \\cup ftv(T_2) \\\\ ftv(\\forall \\alpha.\\sigma) &amp; = &amp; ftv(\\sigma) - \\{ \\alpha \\}  \\end{array} \\] <p>\\(ftv()\\) is also overloaded to extra free type variables from a type environment.</p> \\[ \\begin{array}{rcl} ftv(\\Gamma) &amp; = &amp; \\{ \\alpha \\mid (x,\\sigma) \\in \\Gamma \\wedge \\alpha \\in ftv(\\sigma) \\} \\end{array} \\] <p>The application of a type substitution can be defined as </p> \\[ \\begin{array}{rcll} [] \\sigma &amp; = &amp; \\sigma \\\\  [T/\\alpha] int &amp; = &amp; int \\\\ [T/\\alpha] bool &amp; = &amp; bool \\\\  [T/\\alpha] \\alpha &amp; = &amp; T \\\\ [T/\\alpha] \\beta &amp; = &amp; \\beta &amp; \\beta \\neq \\alpha \\\\  [T/\\alpha] T_1 \\rightarrow T_2 &amp; = &amp; ([T/\\alpha] T_1) \\rightarrow ([T/\\alpha] T_2) \\\\  [T/\\alpha] \\forall \\beta. \\sigma &amp; = &amp; \\forall \\beta. ([T/\\alpha]\\sigma) &amp; \\beta \\neq \\alpha \\wedge \\beta \\not \\in ftv(T) \\\\  (\\Psi_1 \\circ \\Psi_2)\\sigma &amp; = &amp; \\Psi_1 (\\Psi_2 (\\sigma)) \\end{array} \\] <p>In case of applying a type subtitution to a type scheme, we need to check whether the quantified type variable \\(\\beta\\) is in conflict with the type substitution. In case of conflict, a renaming operation simiilar to \\(\\alpha\\) renaming will be applied to \\(\\forall \\beta. \\sigma\\). </p>"},{"location":"notes/static_semantics_2/#example","title":"Example","text":"<p>Let's consider the type-checking derivation of our running (counter) example. </p> <p>Let <code>\u0393 = {}</code> and <code>\u03931 = {(f,\u2200\u03b1.\u03b1-&gt;\u03b1)}</code>.</p> <pre><code>                           -------------------(hmVar)\n                           \u03931\u2295(x,\u03b2)\u2295(y,\u03b3)|-x:\u03b2 \n                           --------------------(hmLam)\n                           \u03931\u2295(x,\u03b2)|-\u03bby.x:\u03b3-&gt;\u03b2\n------------(hmVar)        -------------------(hmLam)\n\u0393\u2295(x,\u03b1)|-x:\u03b1               \u03931|-\u03bbx.\u03bby.x:\u03b2-&gt;\u03b3-&gt;\u03b2   \u03b3,\u03b2\u2209ftv(\u03931)\n------------(hmLam)        --------------------------(hmGen)\n\u0393|-\u03bbx.x:\u03b1-&gt;\u03b1    \u03b1\u2209ftv(\u0393)   \u03931|-\u03bbx.\u03bby.x:\u2200\u03b2.\u2200\u03b3.\u03b2-&gt;\u03b3-&gt;\u03b2          [subtree 1]\n-----------------(hmGen)   -------------------------------------------(hmLet)\n\u0393|-\u03bbx.x:\u2200\u03b1.\u03b1-&gt;\u03b1            \u03931|-let g = \u03bbx.\u03bby.x in (g (f 1) (f true)):int    \n------------------------------------------------------------------- (hmLet)\n\u0393|-let f = \u03bbx.x in (let g = \u03bbx.\u03bby.x in (g (f 1) (f true)):int\n</code></pre> <p>Let <code>\u03932 = {(f,\u2200\u03b1.\u03b1-&gt;\u03b1), (g,\u2200\u03b2.\u2200\u03b3.\u03b2-&gt;\u03b3-&gt;\u03b2)}</code>, we find [subtree 1] is as follows</p> <pre><code>--------------------(hmVar)\n\u03932|-g:\u2200\u03b2.\u2200\u03b3.\u03b2-&gt;\u03b3-&gt;\u03b2     \u2200\u03b2.\u2200\u03b3.\u03b2-&gt;\u03b3-&gt;\u03b2 \u2291 \u2200\u03b3.int-&gt;\u03b3-&gt;int\n----------------------------------(hmInst)\n\u03932|-g:\u2200\u03b3.int-&gt;\u03b3-&gt;int                       [subtree 3]\n-----------------------------------------------(hmApp)\n\u03932|-g (f 1):\u2200\u03b3.\u03b3-&gt;int                      \u2200\u03b3.\u03b3-&gt;int \u2291 bool-&gt;int \n-------------------------------------------------(hmInst)    \n\u03932|-g (f 1):bool-&gt;int                                   [subtree 2]\n---------------------------------------------------------------(hmApp)\n\u03932|-g (f 1) (f true):int\n</code></pre> <p>Where [subtree 2] is as follows</p> <pre><code>--------------(hmVar)\n\u03932|-f:\u2200\u03b1.\u03b1-&gt;\u03b1 \u2200\u03b1.\u03b1-&gt;\u03b1 \u2291 bool-&gt;bool\n-------------------(hmInst)       ----------------(hmBool)\n\u03932|-f:bool-&gt;bool                  \u03932|-true:bool\n----------------------------------------------------(hmApp)\n\u03932|-f true:bool\n</code></pre> <p>Where [subtree 3] is as follows</p> <pre><code>--------------(hmVar)\n\u03932|-f:\u2200\u03b1.\u03b1-&gt;\u03b1 \u2200\u03b1.\u03b1-&gt;\u03b1 \u2291 int-&gt;int\n-------------------(hmInst)       ----------------(hmInt)\n\u03932|-f:int-&gt;int                    \u03932|-1:int\n---------------------------------------------------(hmApp)\n\u03932|-f 1:int\n</code></pre> <p>As we can observe, through the use of rules of \\({\\tt (hmGen)}\\) and \\({\\tt (hmVar)}\\), we are able to give let-bound variables <code>f</code> and <code>g</code> some generic types (AKA parametric polymorphic types). Through rules \\({\\tt (hmApp)}\\) and \\({\\tt (hmInst)}\\) we are able to \"instantiate\" these polymoprhic types to the appropriate monomorphic types depending on the contexts. </p> <p>Note that the goal of Hindley Milner type system is to store the most general (or principal) type (scheme) of a lambda term in the type environment, (especially the program variables and function names), so that when an application is being type-checked, we are able to instantiate a specific type based on the context, as we observe that it is always an combo of \\({\\tt (hmVar)}\\) rule followed by \\({\\tt (hmInst)}\\) rule.</p>"},{"location":"notes/static_semantics_2/#property-4-uniqueness","title":"Property 4 - Uniqueness","text":"<p>The following property states that if a lambda term is typable, its type scheme must be unique modulo type variable renaming.</p> <p>Let \\(t\\) be a lambda calculus term. Let \\(\\Gamma\\) be a type environment such that for all \\(x \\in fv(t)\\), \\(x \\in dom(\\Gamma)\\). Let \\(\\sigma\\) and \\(\\sigma'\\) be type schemes such that \\(\\Gamma \\vdash t : \\sigma\\) and \\(\\Gamma \\vdash t:\\sigma'\\). Then \\(\\sigma\\) and \\(\\sigma'\\) must be the same modulo type variable renaming.</p> <p>For instance, we say type schemes \\(\\forall \\alpha.\\alpha \\rightarrow int\\) and \\(\\forall \\beta.\\beta \\rightarrow int\\) are the same modulo type variable renaming. But type schemes \\(\\forall \\alpha.\\alpha \\rightarrow bool\\) and \\(\\forall \\beta.\\beta \\rightarrow int\\) are not the same.</p>"},{"location":"notes/static_semantics_2/#property-5-progress","title":"Property 5 - Progress","text":"<p>The Progress property is valid for Hindley Milner type checking.</p> <p>Let \\(t\\) be a lambda calculus term such that \\(fv(t) = \\{\\}\\).  Let \\(\\sigma\\) be a type scheme such that \\(\\Gamma_{init} \\vdash t : \\sigma\\). Then \\(t\\) is either a value or there exists some \\(t'\\) such that \\(t \\longrightarrow t'\\).</p>"},{"location":"notes/static_semantics_2/#property-6-preservation","title":"Property 6 - Preservation","text":"<p>The Presevation property is also held for Hindley Milner type checking.</p> <p>Let \\(t\\) and \\(t'\\) be lambda calculus terms such that \\(t \\longrightarrow t'\\). Let \\(\\sigma\\) be a type scheme and \\(\\Gamma\\) be a type environment such that \\(\\Gamma \\vdash t:\\sigma\\). Then \\(\\Gamma \\vdash t':\\sigma\\).</p>"},{"location":"notes/static_semantics_2/#type-inference-for-lambda-calculus","title":"Type Inference for Lambda Calculus","text":"<p>To infer the type environment as well as the type for lambda calculus term, we need an algorithm called Algorithm W.</p> <p>The algorithm is described in a deduction rule system of shape \\(\\Gamma, t \\vDash T, \\Psi\\), which reads as given input type environment \\(\\Gamma\\) and a lambda term \\(t\\), the algorithm infers the type \\(T\\) and type substitution \\(\\Psi\\).</p> \\[ \\begin{array}{rc} {\\tt (wInt)} &amp; \\begin{array}{c}                 c\\ {\\tt is\\ an\\ integer}                  \\\\ \\hline                 \\Gamma, c \\vDash int, []                 \\end{array} \\\\ \\\\ {\\tt (wBool)} &amp; \\begin{array}{c}                 c\\in \\{true,false \\}                  \\\\ \\hline                 \\Gamma, c \\vDash bool, []                 \\end{array} \\end{array} \\] <p>The rules for integer and boolean constants are straight forward. We omit the explanation.</p> \\[ \\begin{array}{rc} {\\tt (wVar)} &amp; \\begin{array}{c}                 (x,\\sigma) \\in \\Gamma \\ \\ \\ inst(\\sigma) = T                 \\\\ \\hline                 \\Gamma, x \\vDash T, []                 \\end{array} \\\\ \\\\ {\\tt (wFix)} &amp; \\begin{array}{c}                 (fix,\\forall \\alpha. (\\alpha\\rightarrow \\alpha)\\rightarrow \\alpha) \\in \\Gamma \\ \\ \\ inst(\\forall \\alpha. (\\alpha\\rightarrow \\alpha)\\rightarrow \\alpha) = T                 \\\\ \\hline                 \\Gamma, fix \\vDash T, []                 \\end{array} \\end{array} \\] <p>The rule \\({\\tt (wVar)}\\) infers the type for a variable by looking it up from the input type environment \\(\\Gamma\\). Same observation applies to \\({\\tt (wFix)}\\) since we assume that \\(fix\\) is pre-defined in the initial type environment \\(\\Gamma_{init}\\), which serves as the starting input.</p> \\[ \\begin{array}{rc} {\\tt (wLam)} &amp; \\begin{array}{c}                 \\alpha_1 = newvar \\ \\ \\ \\Gamma \\oplus (x,\\alpha_1), t \\vDash T, \\Psi                 \\\\ \\hline                 \\Gamma, \\lambda x.t \\vDash : \\Psi(\\alpha_1 \\rightarrow T ), \\Psi                 \\end{array} \\end{array} \\] <p>The rule \\({\\tt (wLam)}\\) infers the type for a lambda abstraction by \"spawning\" a fresh skolem type variable \\(\\alpha_1\\) which is reserved for the lambda bound variable \\(x\\). Under the extended type environment \\(\\Gamma \\oplus (x,\\alpha_1)\\) it infers the body of the lambda extraction \\(t\\) to have type \\(T\\) and the type substitution \\(\\Psi\\). The inferred type of the entire lambda abstraction is therefore \\(\\Psi(\\alpha_1 \\rightarrow T)\\). The reason is that while infering the type for the lambda body, we might obtain substitution that grounds \\(\\alpha_1\\). For instance \\(\\lambda x. x + 1\\) will ground \\(x\\)'s skolem type variable to \\(int\\).</p> \\[ \\begin{array}{rc} {\\tt (wApp)} &amp; \\begin{array}{c}                 \\Gamma, t_1 \\vDash T_1, \\Psi_1\\ \\ \\ \\ \\Psi_1(\\Gamma), t_2 \\vDash T_2, \\Psi_2\\ \\ \\\\ \\alpha_3 = newvar\\ \\ \\ \\Psi_3 = mgu(\\Psi_2(T_1), T_2 \\rightarrow \\alpha_3)                  \\\\ \\hline                 \\Gamma, (t_1\\ t_2) \\vDash \\Psi_3(\\alpha_3), \\Psi_3 \\circ \\Psi_2 \\circ \\Psi_1                 \\end{array} \\end{array} \\] <p>The rule \\({\\tt (wApp)}\\) infers the type for a function application \\(t_1\\ t_2\\). We first apply the inference recursively to \\(t_1\\), producing a type \\(T_1\\) and a type substitution \\(\\Psi_1\\). Next we apply \\(\\Psi_1\\) to \\(\\Gamma\\) hoping to ground some of the type variables inside and use it to infer \\(t_2\\)'s type as \\(T_2\\) with a subsitution \\(\\Psi_2\\). To denote the type of the application, we generate a fresh skolem type variable \\(\\alpha_3\\) reserved for this term. We perform a unification between \\(\\Psi_2(T_1)\\) (hoping \\(\\Psi_2\\) will ground some more type variables in \\(T_1\\)), and \\(T_2 \\rightarrow \\alpha_3\\). If the unifcation is successful, it will result in another type substitution \\(\\Psi_3\\). \\(\\Psi_3\\) can potentially ground the type variable \\(\\alpha_3\\). At last we return \\(\\Psi_3(\\alpha_3)\\) as the inferred type and composing all three substitutions as the resulted substitution.</p> \\[ \\begin{array}{rc} {\\tt (wLet)} &amp; \\begin{array}{c}                 \\Gamma, t_1 \\vDash T_1, \\Psi_1 \\\\ \\Psi_1(\\Gamma) \\oplus (x, gen(\\Psi_1(\\Gamma), T_1)), t_2 \\vDash T_2, \\Psi_2                 \\\\ \\hline                 \\Gamma, let\\ x=t_1\\ in\\ t_2 \\vDash T_2, \\Psi_2 \\circ \\Psi_1              \\end{array} \\end{array} \\] <p>The \\({\\tt (wLet)}\\) rule infers a type for the let binding. We first infer the type \\(T_1\\) and type substitutions \\(\\Psi_1\\). By applying \\(\\Psi_1\\) to \\(\\Gamma\\) we hope to ground some type variables in \\(\\Gamma\\). We apply a helper function \\(gen\\) to generalize \\(T_1\\) w.r.t \\(\\Psi_1(\\Gamma)\\), and use it as the type for \\(x\\) to infer \\(t_2\\) type. Finally, we return \\(T_2\\) as the inferred type and \\(\\Psi_2 \\circ \\Psi_1\\) as the type substitutions. </p> \\[ \\begin{array}{rc} {\\tt (wOp1)} &amp; \\begin{array}{c}                 op \\in \\{+,-,*,/\\} \\\\                  \\Gamma, t_1 \\vDash T_1, \\Psi_1 \\ \\ \\ \\Psi_1(\\Gamma), t_2 \\vDash T_2, \\Psi_2 \\\\                  mgu(\\Psi_2(T_1), T_2, int) = \\Psi_3                    \\\\ \\hline                  \\Gamma, t_1\\ op\\ t_2 \\vDash int, \\Psi_3 \\circ \\Psi_2 \\circ \\Psi_1                  \\end{array} \\\\ \\\\  {\\tt (wOp2)} &amp; \\begin{array}{c}                 \\Gamma, t_1 \\vDash T_1, \\Psi_1 \\ \\ \\ \\Psi_1(\\Gamma), t_2 \\vDash T_2, \\Psi_2 \\\\                  mgu(\\Psi_2(T_1), T_2) = \\Psi_3                    \\\\ \\hline                  \\Gamma, t_1\\ ==\\ t_2 \\vDash bool, \\Psi_3 \\circ \\Psi_2 \\circ \\Psi_1                  \\end{array} \\end{array} \\] <p>The rule \\({\\tt (wOp1)}\\) handles the type inference for arithmetic binary operation. The result type must be \\(int\\). In the premises, we infer the type of the left operand \\(t_1\\) to be \\(T_1\\) with a type substitution \\(\\Psi_1\\). We apply \\(\\Psi_1\\) to \\(\\Gamma\\) hoping to ground some type variables. We continue to infer the right operand \\(t_2\\) with a type \\(T_2\\) and \\(\\Psi_2\\). Finally we need to unify  \\(\\Psi_2(T_1)\\), \\(T_2\\) and \\(int\\) to form \\(\\Psi_3\\). Note that we don't need to apply \\(\\Psi_1\\) to \\(T_2\\) during the unification, because \\(T_2\\) is infered from \\(\\Psi_1(\\Gamma)\\), i.e. type variables in \\(T_2\\) is either already in the domain of \\(\\Psi_1(\\Gamma)\\), or it is enirely fresh, i.e. not in \\(T_1\\) and \\(\\Psi_1\\). We return \\(\\Psi_3 \\circ \\Psi_2 \\circ \\Psi_1\\) as the final substitution. </p> <p>In rule \\({\\tt (wOp2)}\\), the binary operator is an equality check. It works similar to the rule \\({\\tt (wOp1)}\\) except that we return \\(bool\\) as the result type, and we do not include \\(int\\) as the additional operand when unifying the the types of \\(\\Psi_2(T_1)\\) and \\(T_2\\). </p> \\[ \\begin{array}{rc} {\\tt (wIf)} &amp; \\begin{array}{c}                 \\Gamma, t_1 \\vDash T_1, \\Psi_1\\ \\ \\                 \\Psi_1' = mgu(bool, T_1) \\circ \\Psi_1 \\\\                 \\Psi_1'(\\Gamma),t_2 \\vDash T_2, \\Psi_2 \\ \\ \\                 \\Psi_1'(\\Gamma),t_3 \\vDash T_3, \\Psi_3 \\\\                 \\Psi_4 = mgu(\\Psi_3(T_2), \\Psi_2(T_3))                  \\\\ \\hline                 \\Gamma, if\\ t_1\\ then\\ t_2\\ else\\ t_3 \\vDash \\Psi_4(\\Psi_3(T_2)),  \\Psi_4 \\circ \\Psi_3 \\circ \\Psi_2 \\circ \\Psi_1'               \\end{array} \\end{array} \\] <p>In the rule \\({\\tt (wIf)}\\), we infer the type of \\(if\\ t_1\\ then\\ t_2\\ else\\ t_3\\). In the premises, we first infer the type of \\(t_1\\) to be type \\(T_1\\) and type subsitution \\(\\Psi_1\\). Since \\(t_1\\) is used as a condition expression, we define a refined substitution \\(\\Psi_1'\\) by unifing \\(bool\\) with \\(T_1\\) and composing the result with \\(\\Psi_1\\). We then apply \\(\\Psi_1'\\) to \\(\\Gamma\\) and infer \\(t_2\\) and \\(t_3\\).  Finally we unify the returned types from both branches, i.e. \\(\\Psi_3(T_2)\\) and \\(\\Psi_2(T_3)\\). Note that we have to cross apply the type substitutions to ground some type variables. We return \\(\\Psi_4(\\Psi_2(T_2))\\) as the overall inferred type and \\(\\Psi_4 \\circ \\Psi_3 \\circ \\Psi_2 \\circ \\Psi_1'\\) as the overall type substitution. </p>"},{"location":"notes/static_semantics_2/#helper-functions","title":"Helper functions","text":"<p>We find the list of helper functions defined in Algorithm W.</p>"},{"location":"notes/static_semantics_2/#type-substitution","title":"Type Substitution","text":"\\[ \\begin{array}{rcl} \\Psi(\\Gamma)  &amp;= &amp; \\{ (x,\\Psi(\\sigma)) \\mid (x,\\sigma) \\in \\Gamma \\} \\end{array} \\]"},{"location":"notes/static_semantics_2/#type-instantiation","title":"Type Instantiation","text":"\\[ \\begin{array}{rcl} inst(T) &amp; = &amp; T \\\\ inst(\\forall \\alpha.\\sigma) &amp; = &amp; \\lbrack\\beta_1/\\alpha\\rbrack(inst(\\sigma))\\ where\\ \\beta_1=newvar \\\\ \\end{array} \\] <p>The type instantation function instantiate a type scheme. In case of a simple type \\(T\\), it returns \\(T\\). In case it is a polymorphic type scheme \\(\\forall \\alpha.\\sigma\\), we generate a new skolem type variable \\(\\beta_1\\) and replace all the occurances of \\(\\alpha\\) in \\(inst(\\sigma)\\). In some literature, these skolem type variables are called the unification type variables as they are created for the purpose of unification.</p>"},{"location":"notes/static_semantics_2/#type-generalization","title":"Type Generalization","text":"\\[ \\begin{array}{rcl} gen(\\Gamma, T) &amp; = &amp; \\forall \\overline{\\alpha}.T\\ \\ where\\ \\overline{\\alpha} = ftv(T) - ftv(\\Gamma) \\end{array} \\] <p>The type generation function turns a type \\(T\\) into a type scheme if there exists some free type variable in \\(T\\) but not in \\(ftv(\\Gamma)\\), i.e. skolem variables. </p>"},{"location":"notes/static_semantics_2/#type-unification","title":"Type Unification","text":"\\[ \\begin{array}{rcl} mgu(\\alpha, T) &amp; = &amp; [T/\\alpha] \\\\  mgu(T, \\alpha) &amp; = &amp; [T/\\alpha] \\\\  mgu(int, int) &amp; = &amp; [] \\\\  mgu(bool, bool) &amp; = &amp; [] \\\\  mgu(T_1 \\rightarrow T_2 , T_3\\rightarrow T_4) &amp; = &amp; let\\ \\Psi_1 = mgu(T_1, T_3)\\ \\\\  &amp;  &amp; in\\ \\ let\\ \\Psi_2 = mgu(\\Psi_1(T_2), \\Psi_1(T_4)) \\\\ &amp;  &amp; \\ \\ \\ \\ \\ in\\ \\Psi_2 \\circ \\Psi_1 \\end{array} \\] <p>The type unification process is similar to the one described for SIMP program type inference, except that we included an extra case for function type unification. In the event of unifying two function types \\(T_1 \\rightarrow T_2\\) and \\(T_3 \\rightarrow T_4\\), we first unify the argument types \\(T_1\\) and \\(T_3\\) then apply the result to \\(T_2\\) and \\(T_4\\) and unify them.</p>"},{"location":"notes/static_semantics_2/#examples","title":"Examples","text":"<p>Let's consider some examples </p>"},{"location":"notes/static_semantics_2/#example-1-lambda-xx","title":"Example 1 \\(\\lambda x.x\\)","text":"<p>Let \\(\\Gamma = \\{(fix,\\forall \\alpha. (\\alpha \\rightarrow \\alpha) \\rightarrow \\alpha)\\}\\)</p> <pre><code>           (x,\u03b11)\u2208\u0393\u2295(x,\u03b11)  inst(\u03b11)=\u03b11\n           ----------------------------(wVar)\n\u03b11=newvar  \u0393\u2295(x,\u03b11),x|=\u03b11,[]\n------------------------------------------(wLam)\n\u0393,\u03bbx.x|= \u03b11-&gt;\u03b11, []\n</code></pre>"},{"location":"notes/static_semantics_2/#example-2-lambda-xlambda-yx","title":"Example 2 \\(\\lambda x.\\lambda y.x\\)","text":"<pre><code>                     (x,\u03b21)\u2208\u0393\u2295(x,\u03b21)\u2295(y,\u03b31) inst(\u03b21)=\u03b21\n                     --------------------------------(wVar)\n           \u03b31=newvar \u0393\u2295(x,\u03b21)\u2295(y,\u03b31),x|= \u03b21,[]\n           --------------------------------------(wLam)\n\u03b21=newvar  \u0393\u2295(x,\u03b21),\u03bby.x|=\u03b31-&gt;\u03b21,[]\n-------------------------------------------------(wLam)\n\u0393,\u03bbx.\u03bby.x|= \u03b21-&gt;\u03b31-&gt;\u03b21,[]\n</code></pre>"},{"location":"notes/static_semantics_2/#example-3-let-flambda-xx-in-let-glambda-xlambda-yx-in-g-f-1-f-true","title":"Example 3 \\(let\\ f=\\lambda x.x\\ in\\ (let\\ g=\\lambda x.\\lambda y.x\\ in\\ g\\ (f\\ 1)\\ (f\\ true))\\)","text":"<pre><code>[Example 1]\n-------------------\n\u0393,\u03bbx.x|= \u03b11-&gt;\u03b11, []   gen(\u0393,\u03b11-&gt;\u03b11)=\u2200\u03b1.\u03b1-&gt;\u03b1  [subtree 1]\n------------------------------------------------------------------(wLet)\n\u0393,let f=\u03bbx.x in (let g=\u03bbx.\u03bby.x in g (f 1) (f true))|= int, \u03a83\u25cb[bool/\u03b32,int/\u03b41]\n</code></pre> <p>Let <code>\u03931 =\u0393\u2295(f,\u2200\u03b1.\u03b1-&gt;\u03b1)</code>, where [subtree 1] is </p> <pre><code>[Example 2]\n--------------------------   \n\u03931,\u03bbx.\u03bby.x|= \u03b21-&gt;\u03b31-&gt;\u03b21,[]  gen(\u03931,\u03b21-&gt;\u03b31-&gt;\u03b21)=\u2200\u03b2.\u2200\u03b3.\u03b2-&gt;\u03b3-&gt;\u03b2 [subtree 2] \n-------------------------------------------------------------------------------(wLet)\n\u03931,let g=\u03bbx.\u03bby.x in g (f 1) (f true)|=int, \u03a83\u25cb[bool/\u03b32,int/\u03b41]\u25cb[]\n</code></pre> <p>Let <code>\u03932 =\u0393\u2295(f,\u2200\u03b1.\u03b1-&gt;\u03b1)\u2295(g,\u2200\u03b2.\u2200\u03b3.\u03b2-&gt;\u03b3-&gt;\u03b2)</code>, where [subtree 2] is</p> <pre><code>[subtree 3]  [subtree 5] \u03b41=newvar  mgu(\u03b32-&gt;int,bool-&gt;\u03b41)=[bool/\u03b32,int/\u03b41]\n--------------------------------------------------------------------------(wApp)\n\u03932, g (f 1) (f true)|= [bool/\u03b32,int/\u03b41](\u03b41), \u03a83\u25cb[bool/\u03b32,int/\u03b41]\n</code></pre> <p>Where [subtree 3] is</p> <pre><code>(g,\u2200\u03b2.\u2200\u03b3.\u03b2-&gt;\u03b3-&gt;\u03b2)\u2208\u03932                 \u03b51=newvar\ninst(\u2200\u03b2.\u2200\u03b3.\u03b2-&gt;\u03b3-&gt;\u03b2)=\u03b22-&gt;\u03b32-&gt;\u03b22       mgu(\u03b22-&gt;\u03b32-&gt;\u03b22,int-&gt;\u03b51)=[int/\u03b22,\u03b32-&gt;int/\u03b51]\n--------------------------(wVar)    \n\u03932, g|=\u03b22-&gt;\u03b32-&gt;\u03b22, []        [subtree 4]\n---------------------------------------------------------------------(wApp)\n\u03932, g (f 1)|= [int/\u03b22,\u03b32-&gt;int/\u03b51](\u03b51),[int/\u03b22,\u03b32-&gt;int/\u03b51]\u25cb[int/\u03b61,int/\u03b12]\n</code></pre> <p>Where [subtree 4] is </p> <pre><code>(f,\u2200\u03b1.\u03b1-&gt;\u03b1)\u2208\u03932 \ninst(\u2200\u03b1.\u03b1-&gt;\u03b1)=\u03b12-&gt;\u03b12                      \u03b61=newvar\n-----------------(wVar) ------------(wInt)    \n\u03932, f|=\u03b12-&gt;\u03b12,[]        \u03932,1|=int,[]      mgu(\u03b12-&gt;\u03b12,int-&gt;\u03b61)=[int/\u03b61,int/\u03b12]\n---------------------------------------------------------------------(wApp)\n[](\u03932),f 1|= [int/\u03b61,int/\u03b12](\u03b61), [int/\u03b61,int/\u03b12]\n</code></pre> <p>Let <code>\u03a83=[int/\u03b22,\u03b32-&gt;int/\u03b51]\u25cb[int/\u03b61,int/\u03b12]</code>, note that <code>\u03a83(\u03932) =\u03932</code>,  where [subtree 5] is </p> <pre><code>(f,\u2200\u03b1.\u03b1-&gt;\u03b1)\u2208\u03932\ninst(\u2200\u03b1.\u03b1-&gt;\u03b1)=\u03b13-&gt;\u03b13                      \u03b71=newvar\n----------------(wVar) ----------(wBool) \n\u03932,f|=\u03b13-&gt;\u03b13, []       \u03932,true|=bool,[]   mgu(\u03b13-&gt;\u03b13,bool-&gt;\u03b71)=[bool/\u03b13,bool/\u03b71]\n-----------------------------------------------------------------------[wApp]\n\u03932,f true|=[bool/\u03b13,bool/\u03b71](\u03b13),[bool/\u03b13,bool/\u03b71]\n</code></pre>"},{"location":"notes/static_semantics_2/#property-7-type-inference-soundness","title":"Property 7: Type Inference Soundness","text":"<p>The following property states that the type and subsitution generated by Algorithm W is able to type check the lambda calculus term in Hindley Milners' type system.</p> <p>Let \\(t\\) be a lambda calculus term and \\(\\Gamma_{init}\\) is a initial type environment and \\(\\Gamma_{init}, t \\vDash T, \\Psi\\). Then \\(\\Gamma \\vdash t:gen(\\Gamma_{init},\\Psi(T))\\). </p>"},{"location":"notes/static_semantics_2/#property-8-principality","title":"Property 8: Principality","text":"<p>The following property states that the type generated by Algorithm W is the principal type.</p> <p>Let \\(t\\) be a lambda calculus term and \\(\\Gamma_{init}\\) is a initial type environment and \\(\\Gamma_{init}, t \\vDash T, \\Psi\\). Then \\(gen(\\Gamma_{init}, \\Psi(T))\\) is the most general type scheme to type check \\(t\\). </p>"},{"location":"notes/syntax_analysis/","title":"50.054 - Syntax Analysis","text":""},{"location":"notes/syntax_analysis/#learning-outcome","title":"Learning Outcome","text":"<p>By the end of this lesson, you should be able to</p> <ol> <li>Describe the roles and functionalities of lexers and parsers in a compiler pipeline</li> <li>Describe the difference between top-down parsing and bottom-up parsing</li> <li>Apply left-recursion elimination and left-factoring</li> <li>Construct a <code>LL(1)</code> predictive parsing table</li> <li>Explain first-first conflicts and first-follow conflicts</li> </ol>"},{"location":"notes/syntax_analysis/#a-compiler-pipeline","title":"A compiler pipeline","text":"<pre><code>graph LR\nA[Lexing] --&gt;B[Parsing] --&gt; C[Semantic Analysis] --&gt; D[Optimization] --&gt; E[Target Code Generation]</code></pre> <ul> <li>Lexing</li> <li>Input: Source file in String</li> <li>Output: A sequence of valid tokens according to the language specification (grammar)</li> <li>Parsing</li> <li>Input: Output from the Lexer</li> <li>Output: A parse tree representing parsed result according to the parse derivation</li> </ul> <p>A parse tree can be considered the first intermediate representation (IR).</p>"},{"location":"notes/syntax_analysis/#language-grammar-and-rules","title":"Language, Grammar and Rules","text":""},{"location":"notes/syntax_analysis/#what-is-a-language","title":"What is a language?","text":"<p>A language is a set of strings.</p>"},{"location":"notes/syntax_analysis/#what-is-a-grammar","title":"What is a grammar?","text":"<p>A grammar is a specification of a language, including the set of valid words (vocabulary) and the set of possible structures formed by the words.</p> <p>One common way to define a grammar is by defining a set of production rules.</p>"},{"location":"notes/syntax_analysis/#a-running-example","title":"A running example","text":"<p>Let's consider the language of JSON. Though JSON has no operational semantics, i.e. it's not executable, it serves a good subject for syntax analysis.</p> <p>The grammar rule for JSON is as follows</p> <pre><code>&lt;&lt;Grammar 1&gt;&gt;\n(JSON) J ::= i | 's' | [] | [IS] | {NS}\n(Items) IS ::= J,IS | J\n(Named Objects) NS ::= N,NS | N\n(Named Object) N ::= 's':J\n</code></pre> <p>In the above, the grammar consists of four production rules. Each production rule is of form</p> <pre><code>(Name) LHS ::= RHS \n</code></pre> <p>Sometimes, we omit the Name. Terms in upper case, are the non-terminals, and terms in lower case, and symbol terms are the terminals.</p> <p>For each production rule, the LHS is always a non-terminal. On the RHS, we have alternatives separated by <code>|</code>. Each alternatives consists of terminals, non-terminals and mixture of both.</p> <p>For instance, the production rule <code>(JSON)</code> states that a JSON non-terminal <code>J</code> is either an <code>i</code> (an integer), a <code>'s'</code> (a quoted string), an empty list <code>[]</code>, an non-empty list <code>[IS]</code> and an object <code>{NS}</code>.  </p> <p>A production rule with multiple alternatives, can be rewritten into multiple production rules without alternatives. For instance, the <code>(JSON)</code> production rule can be rewritten as follows,</p> <pre><code>J ::= i\nJ ::= 's'\nJ ::= []\nJ ::= [IS]\nJ ::= {NS}\n</code></pre> <p>For each grammar, we expect the LHS of the first production rule is the starting symbol.</p>"},{"location":"notes/syntax_analysis/#lexing","title":"Lexing","text":"<p>Input: Source file in string</p> <p>Output: A sequence of valid tokens according to the language specification (grammar)</p> <p>The purpose of a lexer is to scan through the input source file to ensure the text is constructed as a sequence of valid tokens specified by the syntax rules of the source langugage. The focus is on token-level. The inter-token constraint validation is performed in the next step, Parsing.</p> <p>Sometimes, a lexer is omitted, as the token validation task can be handled in the parser.</p>"},{"location":"notes/syntax_analysis/#lexical-tokens","title":"Lexical Tokens","text":"<p>The set of tokens of a grammar is basically all the terminals. In this JSON grammar example,</p> <pre><code>{i, s, ', [, ], {, }, :, \\, }\n</code></pre> <p>and white spaces are the Lexical Tokens of the language.</p> <p>If we are to represent it using Haskell data types, we could use the following algebraic data type:</p> <pre><code>data LToken = \n    IntTok Int |\n    StrTok String | \n    SQuote | \n    LBracket | \n    RBracket | \n    LBrace |\n    RBrace | \n    Colon  |\n    Comma   \n    deriving (Show, Eq) \n</code></pre> <p>Note that in the above, we find that <code>IntTok</code> and <code>StrTok</code> have semantic components (i.e. the underlying values.) The rest of the tokens  do not.</p> <p>Given the input</p> <pre><code>{'k1':1,'k2':[]}\n</code></pre> <p>the  lexer function <code>lex :: String -&gt; [LToken]</code> should return</p> <pre><code>[LBrace,SQuote,StrTok \"k1\" ,SQuote,Colon, IntTok 1,Comma,SQuote,StrTok \"k2\",SQuote,Colon,LBracket, RBracket,RBrace]\n</code></pre> <p>One could argue that we cheat by assuming the integer and string data types are available as builtin terminals. In case we don't have integer and string as builtin terminals, we could expand the grammar as follows:</p> <pre><code>&lt;&lt;Grammar 2&gt;&gt;\n(JSON) J ::= I | 'STR' | [] | [IS] | {NS}\n(Items) IS ::= J,IS | J\n(Named Objects) NS ::= N,NS | N\n(Named Object) N ::= 'STR':J\n(Integer) I ::= dI | d\n(String) STR ::= aSTR | a\n</code></pre> <p>where <code>d</code> denotes a single digit and <code>a</code> denotes a single ascii character.</p> <p>For the rest of this lesson, we will stick with the first formulation in which we have integer and string terminals builtin, which is common for modern languages.</p>"},{"location":"notes/syntax_analysis/#implementing-a-lexer-using-regular-expression","title":"Implementing a Lexer using Regular Expression","text":"<p>Perhaps one easier way to implement a lexer is to make use of regular expression.</p>"},{"location":"notes/syntax_analysis/#a-simple-example-of-using-regex","title":"A simple example of using Regex","text":"<p>We can specify a regex pattern as follows. </p> <pre><code>import Text.Regex.TDFA (=~~)\ndate = \"^([[:digit:]]{4})-([[:digit:]]{2})-([[:digit:]]{2})$\"\n</code></pre> <p>Next we can perform a match against the above regex pattern using the <code>(=~~)</code> function.</p> <pre><code>(=~~) \"2024-06-01\" date :: Maybe (String,String,String,[String])\n-- yields Just (\"\",\"2024-06-01\",\"\",[\"2024\",\"06\",\"01\"])\n</code></pre> <p>We could develop a simple lexer using the above trick. First we define the pattern for reach token.</p> <pre><code>integer = \"^([[:digit:]]+)(.*)\"\nstring  = \"^([^']*)(.*)\"\nsquote  = \"^(')(.*)\"\nlbracket = \"^(\\\\[)(.*)\"\nrbracket = \"^(\\\\])(.*)\"\nlbrace = \"^(\\\\{)(.*)\"\nrbrace = \"^(\\\\})(.*)\"\ncolon = \"^(:)(.*)\"\ncomma = \"^(,)(.*)\"\n</code></pre> <p>For each token, we have two sub patterns, the first sub-pattern capture the token, and second sub-pattern captures the remaining input, so that we can pass it to the next iteration.</p> <p>Next we define the following function which tries to extract a token from the begining of the input string, and return the rest if a match is found, otherwise, an error is returned.</p> <pre><code>type Error = String\n\n\n(=~=) :: String -&gt; String -&gt; Maybe (String,String)\n(=~=) input regex = case input =~~ regex of \n    Nothing -&gt; Nothing \n    Just (_::String,_::String,_::String,[tokStr,rest]) -&gt; Just (tokStr, rest)\n    Just _ -&gt; Nothing\n\nlexOne :: String -&gt; Either Error (LToken, String)\nlexOne input =\n    case input =~= integer of\n        Just (i,rest) -&gt; Right (IntTok (read i), rest)\n        Nothing -&gt; case input =~= squote of\n            Just (_,rest) -&gt; Right (SQuote, rest)\n            Nothing -&gt; case input =~= lbracket of \n                Just (_,rest) -&gt; Right (LBracket, rest) \n                Nothing -&gt; case input =~= rbracket of \n                    Just (_, rest) -&gt; Right (RBracket, rest)\n                    Nothing -&gt; case input =~= lbrace of \n                        Just (_, rest) -&gt; Right (LBrace, rest)\n                        Nothing -&gt; case input =~= rbrace of\n                            Just (_, rest) -&gt; Right (RBrace, rest)\n                            Nothing -&gt; case input =~= colon of\n                                Just (_, rest) -&gt; Right (Colon, rest) \n                                Nothing -&gt; case input =~= comma of \n                                    Just (_, rest) -&gt; Right (Comma, rest)\n                                    Nothing -&gt; case input =~= string of\n                                        Just (s,rest) -&gt; Right (StrTok s, rest)\n                                        Nothing -&gt; Left \"lexOne failed.\"\n</code></pre> <p>Note that the order of the Haskell patterns is important, since there is some overlapping cases from the above definition (e.g. the regex pattern <code>string</code> and the rest except for <code>squote</code>).</p> <p>Lastly we define the top level <code>lex</code> function by calling <code>lex_one</code> in a recursive function.</p> <pre><code>lex :: String -&gt; Either Error [LToken] \nlex src = go src []\n    where go :: String -&gt; [LToken] -&gt; Either Error [LToken] \n          go []  acc = Right acc \n          go xs acc = case lexOne xs of\n            Left err -&gt; Left err\n            Right (tok, rest) -&gt; go rest (acc ++ [tok])\n</code></pre>"},{"location":"notes/syntax_analysis/#implementing-a-lexer-using-a-parser","title":"Implementing a Lexer using a Parser","text":"<p>In general, parsers are capable of handling context free grammar, which is a super-set of the regular grammars. (A grammar that can be expressed as a regular expression is a regular grammar.)</p> <p>Hence it is possible to implement a lexer using a parser, which we are going to discuss in the cohort problems.</p>"},{"location":"notes/syntax_analysis/#parsing","title":"Parsing","text":"<p>Input: Output from the Lexer</p> <p>Output: A parse tree representing parsed result according to the parse derivation</p> <p>Why tree representation?</p> <ol> <li>Firstly, a tree representation allows efficient access to the sub parts of the source code and intuitive transformation.</li> <li>Secondly, a tree representation captures the relationship between the LHS non-terminals and their RHS in the production rules.</li> </ol>"},{"location":"notes/syntax_analysis/#parsing-derivation","title":"Parsing Derivation","text":"<p>Given an input list of tokens, we could \"walk\" through the production rules starting from the starting non-terminal to find the part that is \"matching with\" the RHS.</p> <p>Consider the JSON grammar in its unabridged form,</p> <pre><code>(1) J ::= i\n(2) J ::= 's'\n(3) J ::= []\n(4) J ::= [IS]\n(5) J ::= {NS}\n(6) IS ::= J,IS\n(7) IS ::= J \n(8) NS ::= N,NS\n(9) NS ::= N\n(10) N ::= 's':J\n</code></pre> <p>We take the output from our lexer example as the input, with some simplification by removing the Haskell data constructors</p> <pre><code>{ , ' , k1 , ' , : , 1 , , , ' , k2 , : , [ , ] , }\n</code></pre> <p>For each token, we attempt to search for a matched rule by scanning the set of production rules from top to bottom.</p>  Rule   Parse tree   Symbols   Input   (5)   graph   J--&gt;LB[\"{\"]   J--&gt;NS   J--&gt;RB[\"}\"]  {  NS }  {  ' k  1 ' : 1 , ' k 2 ' : [ ] }   graph   J--&gt;LB[\"{\"]   J--&gt;NS   J--&gt;RB[\"}\"]     NS }    ' k  1 ' : 1 , ' k 2 ' : [ ] }   (8)   graph   J--&gt;LB[\"{\"]   J--&gt;NS   J--&gt;RB[\"}\"]   NS--&gt;N   NS--&gt;,   NS--&gt;NS2[\"NS\"]   N,NS }    ' k  1 ' : 1 , ' k 2 ' : [ ] }   ... (for the steps skipped, please refer to syntax_analysis_annex.md)   (3)   graph   J--&gt;LB[\"{\"]   J--&gt;NS   J--&gt;RB[\"}\"]   NS--&gt;N   NS--&gt;,   NS--&gt;NS2[\"NS\"]   N--&gt;LQ[']   N--&gt;S1(\"s(k1)\")   N--&gt;RQ[']   N--&gt;:   N--&gt;J2[J]   J2--&gt;i[\"i(1)\"]   NS2--&gt;N2[N]   N2--&gt;LQ2[']   N2--&gt;S2[\"s(k2)\"]   N2--&gt;RQ2[']   N2--&gt;CL2[\":\"]   N2--&gt;J3[J]   J3--&gt;LSQ[\"[\"]   J3--&gt;RSQ[\"]\"];  [ ] } [ ] }  graph   J--&gt;LB[\"{\"]   J--&gt;NS   J--&gt;RB[\"}\"]   NS--&gt;N   NS--&gt;,   NS--&gt;NS2[\"NS\"]   N--&gt;LQ[']   N--&gt;S1(\"s(k1)\")   N--&gt;RQ[']   N--&gt;:   N--&gt;J2[J]   J2--&gt;i[\"i(1)\"]   NS2--&gt;N2[N]   N2--&gt;LQ2[']   N2--&gt;S2[\"s(k2)\"]   N2--&gt;RQ2[']   N2--&gt;CL2[\":\"]   N2--&gt;J3[J]   J3--&gt;LSQ[\"[\"]   J3--&gt;RSQ[\"]\"];  <p>From the above example, it shows that we could implement a parsing routine by recursively searching for a matching production rule based on the current input string and non-terminal (LHS).</p> <p>This algorithm is easy to understand but it has some flaws.</p> <ol> <li>It does not terminate when the grammar contains left recursion.</li> <li>It involves some trial-and-error (back-tracking), hence it is not efficient</li> </ol>"},{"location":"notes/syntax_analysis/#ambiguous-grammar","title":"Ambiguous Grammar","text":"<p>A grammar is said to be ambiguous if parsing it with an input can produce two different parse trees.</p> <p>Consider the following</p> <pre><code>&lt;&lt;Grammar 3&gt;&gt;\nE ::= E + E \nE ::= E * E\nE ::= i\n</code></pre> <p>Consider the input <code>1 + 2 * 3</code>. Parsing this input with the above grammar produces</p> <pre><code>graph\n  E--&gt;E1[\"E\"]\n  E--&gt;+ \n  E--&gt;E2[\"E\"] \n  E1--&gt;i1[\"i(1)\"]\n  E2--&gt;E3[\"E\"]\n  E2--&gt;*\n  E2--&gt;E4[\"E\"]\n  E3--&gt;i2[\"i(2)\"]\n  E4--&gt;i3[\"i(3)\"];</code></pre> <p>or</p> <pre><code>graph\n  E--&gt;E1[\"E\"]\n  E--&gt;* \n  E--&gt;E2[\"E\"] \n  E1--&gt;E3[\"E\"]\n  E1--&gt;+\n  E1--&gt;E4[\"E\"]\n  E2--&gt;i3[\"i(3)\"]\n  E3--&gt;i1[\"i(1)\"]\n  E4--&gt;i2[\"i(2)\"];</code></pre> <p>To resolve ambiguity, the language designers need to make decision to give priority to certain production rules by rewriting it. For example, we argue that <code>*</code> should bind stronger than <code>+</code>. Thus we should rewrite (or rather \"restrict\") the ambiguous Grammar 3 into the following subset</p> <pre><code>&lt;&lt;Grammar 4&gt;&gt;\nE::= T + E\nE::= T\nT::= T * F \nT::= F\nF::= i\n</code></pre> <p>As a result, the input <code>1 + 2 * 3</code> is parsed as</p> <pre><code>graph\n  E--&gt;T1[\"T\"]\n  E--&gt;+ \n  E--&gt;E1[\"E\"] \n  T1--&gt;F1[\"F\"]\n  F1--&gt;i1[\"i(1)\"]\n  E1--&gt;T2[\"T\"]\n  T2--&gt;F2[\"F\"]\n  F2--&gt;i2[\"i(2)\"]\n  T2--&gt;*\n  T2--&gt;F3[\"F\"]\n  F3--&gt;i3[\"i(3)\"]\n  ;</code></pre>"},{"location":"notes/syntax_analysis/#grammar-with-left-recursion","title":"Grammar with Left Recursion","text":"<p>Let's try to run a top-down recursive parsing algorithm over the following grammar</p> <pre><code>&lt;&lt;Grammar 5&gt;&gt;\nE ::= E + T\nE ::= T\nT ::= i\n</code></pre> <p><code>i</code> and <code>+</code> are terminals, and <code>E</code> and <code>T</code> are the non-terminals. <code>i</code> denotes an integer.</p> <p>Consider applying the top-down recursive parsing mentioned above to the input <code>1</code>, if the first production rule is always selected, the algorithm would not terminate. The issue is that the first production rule containing a recursion at the left-most position. To eliminate left recursion in general, we consider the following transformation.</p> <p>Let <code>N</code> be a non-terminal, \\(\\alpha_i\\) and \\(\\beta_j\\) be sequences of symbols (consist of terminals and non-terminals)</p> <p>Left recursive grammar rules</p> \\[ \\begin{array}{rcl} N &amp; ::= &amp; N\\alpha_1 \\\\ &amp; ... &amp; \\\\ N &amp; ::= &amp; N\\alpha_n \\\\ N &amp; ::= &amp; \\beta_1 \\\\ &amp; ... &amp; \\\\ N &amp; ::= &amp; \\beta_m \\end{array} \\] <p>can be transformed into</p> \\[ \\begin{array}{rcl} N &amp; ::= &amp; \\beta_1 N' \\\\ &amp; ... &amp; \\\\ N &amp; ::= &amp; \\beta_m N' \\\\ N' &amp; ::= &amp; \\alpha_1 N' \\\\ &amp; ... &amp; \\\\ N' &amp; ::= &amp; \\alpha_n N' \\\\ N' &amp; ::= &amp; \\epsilon \\end{array} \\] <p>Now apply the above to our running example.</p> <ul> <li>\\(N\\) is <code>E</code> and</li> <li>\\(\\alpha_1\\) is <code>+ T</code>,</li> <li><code>T</code> is \\(\\beta_1\\).</li> </ul> <pre><code>&lt;&lt;Grammar 6&gt;&gt;\nE ::= TE'\nE' ::= + TE'\nE' ::= epsilon\nT ::= i\n</code></pre> <p>The resulting Grammar 6 is equivalent the original Grammar 5.  Note that epsilon (\\(\\epsilon\\)) is a special terminal which denotes an empty sequence.</p> <p>There are few points to take note</p> <ol> <li>For indirect left recursion, some substitution steps are required before applying the above transformation. For instance</li> </ol> <pre><code>&lt;&lt;Grammar 7&gt;&gt;\nG ::= H + G\nH ::= G + i\nH ::= i\n</code></pre> <p>We need to substitute <code>H</code> into the first production rule.</p> <pre><code>&lt;&lt;Grammar 8&gt;&gt;\nG ::= G + i + G\nG ::= i + G\n</code></pre> <ol> <li>Since we have changed the grammar production rules, we need to use the transformed grammar for parsing, resulting in the parse trees being generated in the shape of the transformed grammar. We need to perform an extra step of (backwards) transformation to turn the parse trees back to the original grammar. For example, parsing the input <code>1 + 1</code> with Grammar 6 yields the following parse tree</li> </ol>  graph   E--&gt;T1[\"T\"]   E--&gt;Ep1[E']   T1--&gt;i1[\"i(1)\"]   Ep1--&gt;+   Ep1--&gt;T2[T]   Ep1--&gt;Ep2[E']   T2--&gt;i2[\"i(1)\"]   Ep2--&gt;eps1[\u03b5]  <p>which needs to be transformed back to</p>  graph   E--&gt;E1[\"E\"]   E--&gt;+   E--&gt;T1[\"T\"]   E1--&gt;T2[\"T\"]   T1--&gt;i1[\"i(1)\"]   T2--&gt;i2[\"i(1)\"]"},{"location":"notes/syntax_analysis/#predictive-recursive-parsing","title":"Predictive Recursive Parsing","text":"<p>Next we address the inefficiency issue with our naive parsing algorithm. One observation from the derivation example we've seen earlier is that if we are able to pick the \"right\" production rule without trial-and-error, we would eliminate the backtracking.</p> <p>In order to do that we need to ensure the grammar we work with is a particular class of grammar, which is also known as <code>LL(k)</code> grammar. Here <code>k</code> refers to the number of leading symbols from the input we need to check in order to identify a particular production rule to apply without back-tracking.</p> <p>BTW, <code>LL(k)</code> stands for left-to-right, left-most derivation with <code>k</code> tokens look-ahead algorithm.</p> <p>Let \\(\\sigma\\) denote a symbol, (it could be a terminal or a non-terminal). Let \\(\\overline{\\sigma}\\) denote a sequence of symbols. Given a grammar \\(G\\) we define the following functions \\(null(\\overline{\\sigma},G)\\), \\(first(\\overline{\\sigma},G)\\) and \\(follow(\\sigma, G)\\)</p> <p>\\(null(\\overline{\\sigma},G)\\) checks whether the language denoted by \\(\\overline{\\sigma}\\) contains the empty sequence.</p> \\[ \\begin{array}{rcl} null(t,G) &amp; = &amp; false \\\\ null(\\epsilon,G) &amp; = &amp; true \\\\ null(N,G) &amp; = &amp; \\bigvee_{N::=\\overline{\\sigma} \\in G} null(\\overline{\\sigma},G) \\\\ null(\\sigma_1...\\sigma_n,G) &amp; = &amp; null(\\sigma_1,G) \\wedge ... \\wedge null(\\sigma_n,G) \\end{array} \\] <p>\\(first(\\overline{\\sigma},G)\\) computes the set of leading terminals from the language denoted by \\(\\overline{\\sigma}\\).</p> \\[ \\begin{array}{rcl} first(\\epsilon, G) &amp; = &amp; \\{\\} \\\\ first(t,G) &amp; = &amp; \\{t\\} \\\\ first(N,G) &amp; = &amp; \\bigcup_{N::=\\overline{\\sigma} \\in G} first(\\overline{\\sigma},G) \\\\ first(\\sigma\\overline{\\sigma},G) &amp; = &amp;   \\left [     \\begin{array}{ll}       first(\\sigma,G) \\cup first(\\overline{\\sigma},G) &amp; {\\tt if}\\ null(\\sigma,G) \\\\       first(\\sigma,G) &amp; {\\tt otherwise}       \\end{array}   \\right . \\end{array} \\] <p>\\(follow(\\sigma,G)\\) finds the set of terminals that immediately follows symbol \\(\\sigma\\) in any derivation derivable from \\(G\\).</p> \\[ \\begin{array}{rcl} follow(\\sigma,G) &amp; = &amp; \\bigcup_{N::=\\overline{\\sigma}\\sigma{\\overline{\\gamma}} \\in G}   \\left [     \\begin{array}{ll}       first(\\overline{\\gamma}, G) \\cup follow(N,G) &amp; {\\tt if}\\ null(\\overline{\\gamma}, G) \\\\       first(\\overline{\\gamma}, G) &amp; {\\tt otherwise}     \\end{array}   \\right . \\end{array} \\] <p>Sometimes, for convenience we omit the second parameter \\(G\\).</p> <p>For example, let \\(G\\) be Grammar 6, then</p> \\[ \\begin{array}{l} null(E) = null(TE') = null(T) \\wedge null(E') = false \\wedge null(E') = false \\\\ null(E') = null(+TE') \\vee null(\\epsilon) = null(+TE') \\vee true = true \\\\ null(T) = null(i) = false \\\\ \\\\ first(E) = first(TE') = first(T) =  \\{i\\} \\\\ first(E') = first(+TE') \\cup first(\\epsilon) = first(+TE') = \\{+\\} \\\\ first(T) = \\{i\\} \\\\ \\\\ follow(E) = \\{\\} \\\\ follow(E') = follow(E) \\cup follow(E') = \\{\\} \\cup follow(E') \\\\ follow(T) = first(E') \\cup follow(E') = \\{+\\} \\cup follow(E') \\end{array} \\] <p>When computing \\(follow(E')\\) and \\(follow(T)\\) we encounter an infinite sequence of \\(\\cup follow(E')\\) which leads to a fix-point. That is, an infinite sequence of repeated operation that does not add any value to the existing result. We can conclude that \\(follow(E') = \\{\\}\\) and \\(follow(T) = \\{+\\}\\).</p> <p>We will discuss fix-point in-depth in some lesson later.</p> <p>Given \\(null\\), \\(first\\) and \\(follow\\) computed, we can construct a predictive parsing table to check whether the grammar is in <code>LL(k)</code>. For simplicity, we check the case <code>k = 1</code>, we construct the following predictive parsing table where each row is indexed a non-terminal, and each column is indexed by a terminal.</p> i + E E' T <p>For each production rule \\(N ::= \\overline{\\sigma}\\), we put the production rule in</p> <ul> <li>cell \\((N,t)\\) if \\(t \\in first(\\overline{\\sigma})\\)</li> <li>cell \\((N,t')\\) if \\(null(\\overline{\\sigma})\\) and \\(t' \\in follow(N)\\)</li> </ul> <p>We fill up the table</p> i + E E ::= TE' E' E' ::= + TE' T T ::= i We conclude that a grammar is in <code>LL(1)</code> if it contains no conflicts. A conflict arises when there are more than one production rule to be applied given a non-terminal and a leading symbol. Given a <code>LL(1)</code> grammar, we can perform predictive top-down parsing by selecting the right production rule by examining the leading input symbol. <p>In general, there are two kinds of conflicts found in grammar that violates the <code>LL(1)</code> grammar requirements.</p> <ol> <li>first-first conflict</li> <li>first-follow conflict</li> </ol>"},{"location":"notes/syntax_analysis/#first-first-conflict","title":"First-first Conflict","text":"<p>Consider the grammar</p> <pre><code>&lt;&lt;Grammar 9&gt;&gt;\nS ::= Xb\nS ::= Yc\nX ::= a\nY ::= a \n</code></pre> <p>We compute \\(null\\), \\(first\\) and \\(follow\\).</p> \\[ \\begin{array}{l} null(S) = null(Xb) = false \\\\ null(X) = null(a) = false \\\\ null(Y) = null(a) = false \\\\ \\\\ first(S) = first(Xb) \\cup first(Yc) = \\{a\\} \\\\ first(X) = first(a) = \\{a \\} \\\\ first(Y) = first(a) = \\{a \\} \\\\ \\\\ follow(S) = \\{\\} \\\\ follow(X) = \\{b \\} \\\\ follow(Y) = \\{c \\} \\end{array} \\] <p>We fill up the following predictive parsing table</p> a b c S S::=Xb, S::=Yc X X::=a Y Y::=a <p>From the above we find that there are two production rules in the cell <code>(S,a)</code>, namely <code>S::=Xb</code>, and <code>S::=Yc</code>. This is a first-first conflict, since both production rules' first set contains <code>a</code>. This prevents us from constructing a predictable parser by observing the leading symbol from the input.</p> <p>First-first conflict can be eliminated by applying left-factoring (not to be confused with left recursion).</p> <p>From our running example, we find that the cell <code>(S,a)</code> has more than one production rule applicable. This is caused by the fact that both <code>X::=a</code> and <code>Y::=a</code> start with the non-terminal <code>a</code>. We could apply substitution to eliminate <code>X</code> and <code>Y</code>.</p> <pre><code>&lt;&lt;Grammar 10&gt;&gt;\nS ::= ab\nS ::= ac\n</code></pre> <p>Then we could introduce a new non-terminal <code>Z</code> which capture the following languages after <code>a</code>.</p> <pre><code>&lt;&lt;Grammar 11&gt;&gt;\nS ::= aZ\nZ ::= b\nZ ::= c\n</code></pre>"},{"location":"notes/syntax_analysis/#first-follow-conflict","title":"First-Follow Conflict","text":"<p>Consider the following grammar</p> <pre><code>&lt;&lt;Grammar 12&gt;&gt;\nS ::= Xd \nX ::= C \nX ::= Ba\nC ::= epsilon\nB ::= d\n</code></pre> <p>and the \\(null\\), \\(first\\) and \\(follow\\) functions</p> \\[ \\begin{array}{l} null(S) = null(Xd) = null(X) \\wedge null(d) = false \\\\ null(X) = null(C) \\vee null(Ba) = true \\\\ null(C) = null(\\epsilon) = true \\\\ null(B) = null(d) = false \\\\ \\\\ first(S) = first(Xd) = first(X) \\cup first(d) = \\{d\\} \\\\ first(X) = first(C) \\cup first(Ba) = \\{d\\} \\\\ first(C) = \\{\\}\\\\ first(B) = \\{d\\} \\\\ \\\\ follow(S) = \\{\\} \\\\ follow(X) = \\{d\\} \\\\ follow(C) = follow(X) = \\{d\\} \\\\ follow(B) = \\{d\\} \\end{array} \\] <p>We construct the predictive parsing table as follows</p> a d S S::=Xd X X::=Ba, X::=C(S::=Xd) C C::=epsilon (S::=Xd) B B::=d <p>In the cell of <code>(X,d)</code> we find two production rules <code>X::=Ba</code> and <code>X::=C (S::=Xd)</code>. It is a first-follow conflict, because the first production rule is discovered through the <code>first(X)</code> set and the second one is from the <code>follow(X)</code> set.</p> <p>Since first-follow conflicts are introduced by epsilon production rule, we could apply substitution to eliminate first-follow conflicts</p> <p>Substitute <code>B</code> and <code>C</code></p> <pre><code>&lt;&lt;Grammar 13&gt;&gt;\nS ::= Xd \nX ::= epsilon \nX ::= da\n</code></pre> <p>Substitute <code>X</code></p> <pre><code>&lt;&lt;Grammar 14&gt;&gt;\nS ::= d\nS ::= dad\n</code></pre> <p>However, as we can observe, eliminating first-follow conflict by substitution might introduce a new first-first conflict.</p>"},{"location":"notes/syntax_analysis/#to-ll1-or-not-ll1","title":"To <code>LL(1)</code> or not <code>LL(1)</code>","text":"<p>Given a grammar, we could get a <code>LL(1)</code> grammar equivalent in most of the cases.</p> <ol> <li>Disambiguate the grammar if it is ambiguous</li> <li>Eliminate the left recursion</li> <li>Apply left-factoring if there exists some first-first conflict</li> <li>Apply substitution if there exists some first-follow conflict</li> <li>repeat 3 if first-first conflict is introduced</li> </ol> <p>Step 1 is often done manually, (there is no general algorithm to do so.) Steps 2-4 (and 5) can be automated by some algorithm.</p> <p>Let's consider another example (a subset of Grammar 3).</p> <pre><code>&lt;&lt;Grammar 15&gt;&gt;\nE ::= E + E\nE ::= i\n</code></pre> <p>Note that this grammar is ambiguous. Let's suppose we skip this step and directly eliminate the left-recursion</p> <pre><code>&lt;&lt;Grammar 16&gt;&gt;\nE ::= iE'\nE' ::= + EE'\nE' ::= epsilon\n</code></pre> <p>Next we compute the predictive parsing table.</p> \\[ \\begin{array}{l} null(E) = null(iE) = null(i) \\wedge null(E') = false \\wedge null(E') = false \\\\ null(E') = null(+EE') \\vee null(\\epsilon) = null(+E') \\vee true = true \\\\ \\\\ first(E) = first(iE') = \\{i\\} \\\\ first(E') = first(+EE') \\cup first(\\epsilon) = first(+EE') = \\{+\\} \\\\ \\\\ follow(E) = first(E') \\cup follow(E') = \\{+\\} \\cup follow(E') \\\\ follow(E') = follow(E) \\cup follow(E') = \\{+\\} \\cup follow(E') \\cup follow(E') \\end{array} \\] i + E E::= iE' E' E'::= +EE', E'::= epsilon <p>As shown from the above, the grammar contains a first-follow conflict, therefore it is not a <code>LL(1)</code>. It is not possible to perform substitution to eliminate the first-follow conflict because it will lead to infinite expansion.</p>"},{"location":"notes/syntax_analysis/#a-short-summary-so-far-for-top-down-recursive-parsing","title":"A short summary so far for top-down recursive parsing","text":"<p>Top-down parsing is simple, however might be inefficient.</p> <p>We need to rewrite the grammar into a more specific (a subset) if the grammar is ambiguous. No general algorithm exists.</p> <p>We need to eliminate left recursion so that the parsing will terminate.</p> <p>We construct the predictive parsing table to check whether the grammar is in <code>LL(k)</code>. If the grammar is in <code>LL(k)</code> we can always pick the right production rule given the first <code>k</code> leading symbols from the input.</p> <p>For most of the cases, <code>LL(1)</code> is sufficient for practical use.</p> <p>We also can conclude that a <code>LL(k+1)</code> grammar is also a <code>LL(k)</code> grammar, but the other way does not hold.</p> <p>Given a particular <code>k</code> and a grammar <code>G</code>, we can check whether <code>G</code> is <code>LL(k)</code>. However given a grammar <code>G</code> to find a <code>k</code> such that <code>G</code> is <code>LL(k)</code> is undecidable.</p>"},{"location":"notes/syntax_analysis/#summary","title":"Summary","text":"<p>We have covered</p> <ul> <li>The roles and functionalities of lexers and parsers in a compiler pipeline</li> <li>There are two major types of parser, top-down parsing and bottom-up parsing</li> <li>How to eliminate left-recursion from a grammar,</li> <li>How to apply left-factoring</li> <li>How to construct a <code>LL(1)</code> predictive parsing table</li> </ul>"},{"location":"notes/syntax_analysis_2/","title":"50.054 - Syntax Analysis 2","text":""},{"location":"notes/syntax_analysis_2/#learning-outcome","title":"Learning Outcome","text":"<p>By the end of this lesson, you should be able to</p> <ol> <li>Construct a <code>LR(0)</code> parsing table</li> <li>Explain shift-reduce conflict</li> <li>Construct a <code>SLR</code> parsing table</li> </ol>"},{"location":"notes/syntax_analysis_2/#bottom-up-parsing","title":"Bottom-up parsing","text":"<p>An issue with <code>LL(k)</code> parsing is that we always need to make sure that we can pick the correct production rule by examining the first <code>k</code> tokens from the input. There is always a limit of how many tokens we should look ahead to pick a particular production rule without relying on backtracking.</p> <p>What if we consider multiple production rules when \"consuming\" input tokens and decide which one to pick when we have enough information? Answering this question leads to bottom-up parsing.</p> <p><code>LR(k)</code> stands for left-to-right, right-most derivation with <code>k</code> lookahead tokens.</p> <p>In essence, <code>LR(k)</code> relies on a parsing table and a stack to decide which production rule to be applied given the current (partial) input. A stack is storing the symbols have been consumed so far, each element in the stack also stores the state of the parser.</p> <p>To understand <code>LR(k)</code> parsing, let's assume that we are given the parsing table. (We will consider how to construct the parsing table shortly.)</p> <p>Let's recall Grammar 6</p> <pre><code>&lt;&lt;Grammar 6&gt;&gt;\n1 S' ::= E$ \n2 E ::= TE'\n3 E' ::= + TE'\n4 E' ::= epsilon\n5 T ::= i\n</code></pre> <p>We added number to each production rule, and we introduce a top level production rule <code>S' ::= E$</code> where <code>$</code> denotes the end of input symbol.</p> <p>Let's consider the following parsing table for Grammar 6.</p> + i $ S' E E' T 10 shift 13 goto 12 goto 11 11 shift 14 reduce 4 goto 17 12 accept 13 reduce 5 reduce 5 14 shift 13 goto 15 15 shift 14 reduce 4 goto 16 16 reduce 3 17 reduce 2 <p>Each cell in the above table is indexed by a symbol of the grammar, and a state. To avoid confusion with the production rule IDs,  we assume that state IDs are having 2 digits, and state <code>10</code> is the starting state. In each cell, we find a set of parsing actions.</p> <ol> <li><code>shift s</code> where <code>s</code> dentes a state ID. Given <code>shift s</code> in a cell (<code>s'</code>, <code>t</code>), we change the parser state from <code>s'</code> to <code>s</code> and consume the leading token <code>t</code> from the input and store it in the stack.</li> <li><code>accept</code>. Given <code>accept</code> found in a cell (<code>s</code>, <code>$</code>), the parsing is completed successfully.</li> <li><code>goto s</code> where <code>s</code> denotes a state ID. Given <code>goto s</code> in a cell (<code>s'</code>, <code>t</code>), we change the parser's state to <code>s</code>.</li> <li><code>reduce p</code> where <code>p</code> denotes a production rule ID. Given <code>reduce p</code> in a cell (<code>s</code>, <code>t</code>), lookup production rule <code>LHS::=RHS</code> from the grammar by <code>p</code>. We pop the items from top of the stack by reversing <code>RHS</code>. Given the state of the current top element of the stack, let's say <code>s'</code>, we lookup the goto action in cell (<code>s'</code>, <code>LHS</code>) and push <code>LHS</code> to the stack and perform the goto action.</li> </ol> <p>Consider the parsing the input <code>1+2+3</code></p> stack input action rule (10) 1+2+3$ shift 13 (10) i(13) +2+3$ reduce 5 T::=i (10) T(11) +2+3$ shift 14 (10) T(11) +(14) 2+3$ shift 13 (10) T(11) +(14) i(13) +3$ reduce 5 T::=i (10) T(11) +(14) T(15) +3$ shift 14 (10) T(11) +(14) T(15) +(14) 3$ shift 13 (10) T(11) +(14) T(15) +(14) i(13) $ reduce 5 T::=i (10) T(11) +(14) T(15) +(14) T(15) $ reduce 4 E'::=epsilon (10) T(11) +(14) T(15) +(14) T(15) E' (16) $ reduce 3 E'::=+TE' (10) T(11) +(14) T(15) E'(16) $ reduce 3 E'::=+TE' (10) T(11) E'(17) $ reduce 2 E::=TE' (10) E(12) $ accept S'::=E$ <p>We start with state (10) in the stack.</p> <ol> <li>Given the first token from the input is <code>1</code> (i.e. an <code>i</code> token), we look up the parsing table and find the <code>shift 13</code> action in cell (<code>10</code>, <code>i</code>). By executing this action, we push <code>i(13)</code> in the stack.</li> <li>The next input is <code>+</code>. Given the current state is (13), we apply the smae strategy to find action <code>reduce 5</code> in cell (<code>13</code>, <code>+</code>). Recall that the production rule with id 5 is <code>T::=i</code>, we pop the <code>i(13)</code> from the stack, and check for the correspondent action in cell (<code>10</code>, <code>T</code>), we find <code>goto 11</code>. Hence we push <code>T(11)</code> into the stack.</li> </ol> <p>We follow the remaining steps to parse the input when we meet the accept action.</p> <p>One interesting observation is that the order of the rules found in the <code>rule</code> column is the reverse order of the list of rules we used in <code>LL(k)</code> parsing.</p> <p>Next we consider how to construct the parsing tables. It turns out that there are multiple ways of construct the parsing tables for <code>LR(k)</code> grammars.</p>"},{"location":"notes/syntax_analysis_2/#lr0-parsing","title":"LR(0) Parsing","text":"<p>We first consider the simplest parsing table where we ignore the leading token from the input, <code>LR(0)</code>.</p> <p>The main idea is that the actions (which define the change and update of the state and stack) are output based on the current state and the current stack. If we recall that this is a form of state machine.</p> <p>From this point onwards, we use pseudo Haskell syntax illustrate the algorithm behind the parsing table construction.</p> <p>Let <code>.</code> denote a meta symbol which indicate the current parsing context in a production rule.</p> <p>For instance for production rule 3 <code>E' ::= +TE'</code>, we have four possible contexts</p> <ul> <li><code>E' ::= .+TE'</code></li> <li><code>E' ::= +.TE'</code></li> <li><code>E' ::= +T.E'</code></li> <li><code>E' ::= +TE'.</code></li> </ul> <p>We call each of these possible contexts an <code>Item</code>.</p> <p>We define <code>Items</code> to be a set of <code>Item</code>s, <code>Grammar</code> to be a set of production rules (whose definition is omitted, we use the syntax <code>LHS::=RHS</code> directly in the pseudo-code.)</p> <pre><code>type Items = Set Item\ntype Grammar = Set Prod\n</code></pre> <p>We consider the following operations.</p> <pre><code>closure :: Items -&gt; Grammar -&gt; Items \nclosure items grammar = \n  let set1 = do \n      { (N ::= alpha . X beta) &lt;- items\n      ; (X ::= gamma)          &lt;- gramma\n      ; return (X ::= . gamma) \n      }\n      set2 = do \n      { (N ::= . epsilon) &lt;- items\n      ; return (N ::= epsilon .)\n      }\n      newItems = set1 `union` set 2\n  in if newItems `isSubsetOf` items\n     then items\n     else closure (items `union` newItems) grammar\n\ngoto :: Items -&gt; Grammar -&gt; Symbol -&gt; Items \ngoto items grammar sym = \n   let j = do \n      (M ::= alpha . X beta) &lt;- items\n      return (N ::= alpha X . beta)\n   in closure j grammar\n</code></pre> <p>Function <code>closure</code> takes an item set <code>items</code> and a grammar then returns the closure of <code>items</code>. For each item of shape <code>N::=alpha . X beta</code> in <code>items</code>, we look for the correspondent production rule <code>X ::= gamma</code> in <code>grammar</code> if <code>X</code> is a non-terminal, add <code>X::= . gamma</code> to the new item sets if it is not yet included in <code>items</code>.</p> <p>Note that Haskell by default does not support pattern such as <code>(N ::= alpha . X beta)</code> and <code>(X::= gamma)</code>. In this section, let's pretend that these patterns are allowed in Haskell so that we can explain the algorithm in Haskell style pseudo-code.</p> <p>Function <code>goto</code> takes an item set <code>items</code> and searches for item inside of shape <code>N::= alpha . X beta</code> then add <code>N::=alpha X. beta</code> as the next set <code>j</code>. We compute the closure of <code>j</code> and return it as result.</p> <pre><code>type State = Items\ntype Transition = (State, Symbol, State)\ndata StateMachine = StateMachine { states::Set State, transitions::Set Transition , accepts::Set State} \n\nbuildSM :: State -&gt; Grammar -&gt; StateMachine \nbuildSM init grammar = \n  case step1 (Set.fromList [init]) Set.Empty of \n    { (states, trans) -&gt; StateMachine states trans (step2 states) }\n  where step1 :: Set State -&gt; Set Transition -&gt; (Set State, Set Transition)  \n        step1 states trans = -- compute all states and transitions\n          let newStateTrans = do \n              { i                      &lt;- states\n              ; (A ::= alpha . X beta) &lt;- i\n              ; let j = goto i grammar X\n              ; return (j, (i, X, j))\n              }\n              newStates = map fst newStateTrans\n              newTrans = map snd newStateTrans\n          in if newStates `isSubsetOf` states\n             then (states, trans) \n             else step1 (states `union` newStates) (trans `union` newTrans)\n        step2 :: Set State -&gt; Set State \n        step2 states = -- compute all final states\n          filter (\\i -&gt; exists (\\item -&gt; case item of \n            { (N :: alpha . $ ) -&gt; True; _ -&gt; False }) i) states\n</code></pre> <p>Function <code>buildSM</code> consists of two steps. In <code>step1</code> we start with the initial state <code>init</code> and compute all possible states and transitions by applying <code>goto</code>. In <code>step2</code>, we compute all final states.</p> <p>By applying <code>buildSM</code> to Grammar 6 yields the following state diagram.</p> <pre><code>graph\n  State10[\"State10 &lt;br/&gt; S'::=.E$ &lt;br/&gt; E::=.TE' &lt;br/&gt; T::=i \"]--T--&gt;State11[\"State11 &lt;br/&gt; E::=T.E' &lt;br/&gt; E'::=+TE' &lt;br/&gt; E'::= . epsilon &lt;br/&gt; E'::= epsilon.\"]\n  State10--E--&gt;State12[\"State12 &lt;br/&gt; S'::=E.$\"]\n  State10--i--&gt;State13[\"State13 &lt;br/&gt; T::=i.\"]\n  State11--+--&gt;State14[\"State14 &lt;br/&gt; E'::= +.TE' &lt;br/&gt; T::=.i\"]\n  State11--E'--&gt;State17[\"State17 &lt;br/&gt; S'::=E.$\"]\n  State14--i--&gt;State13\n  State14--T--&gt;State15[\"State15 &lt;br/&gt; E'::= +T.E' &lt;br/&gt; E'::=.+TE' &lt;br/&gt; E'::=.epsilon &lt;br/&gt; E'::=epsilon . \"]\n  State15--+--&gt;State14\n  State15--E'--&gt;State16[\"State16 &lt;br/&gt; E'::=+TE'.\"]</code></pre> <pre><code>reduce :: [State] -&gt; [(Items, Prod)] \nreduce states = \n  foldl (\\(acc, items) -&gt; \n    foldl (\\(acc2,item) -&gt; case item of \n      { (N::= alpha . ) -&gt; acc2 ++ [(item, N::=alpha)]\n      ; _  -&gt; acc2\n      } acc (toList items))\n  ) [] states\n</code></pre> <p>Function <code>reduce</code> takes a list of states and search for item set that contains an item of shape <code>N::= alpha .</code>.</p> <pre><code>data Action = Shift State | Reduce Prdo | Accept | Goto State\n\nptable :: Grammar -&gt; Prod -&gt; [(State, Symbol, Action)] \nptable grammar (S ::= X$) = \n  let init = closure (Set.fromList [S ::=.X$]) grammar\n  in case buildSM init grammar of \n  { StateMachine states trans finals -&gt; \n    let shifts = do \n        { (i, x, j) &lt;- trans\n        ; if isTerminal x\n        ; return (i, x, (Shift j))\n        }\n\n        gotos = do\n        { (i, x, j) &lt;- grans\n        ; if not (isTerminal x)\n        ; return (i, x, (Goto j))\n        }\n\n        reduces = do \n        { (i, N::=alpha) &lt;- reduce states\n        ; x &lt;- allTerminals grammar \n        ; return (i, x, reduce (N::=alpha))\n        }\n\n        accepts = do \n        { i &lt;- finals\n        ; return (i, $, Accept) \n        }\n    in shifts ++ gotos ++ reduces ++ accepts\n  }\n</code></pre> <p>Function <code>ptable</code> computes the <code>LR(0)</code> parsing table by making use of the functions defined earlier.</p> <p>Applying <code>ptable</code> to Grammar 6 yields</p> + i $ S' E E' T 10 shift 13 goto 12 goto 11 11 shift 14 / reduce 4 reduce 4 reduce 4 goto 17 12 accept 13 reduce 5 reduce 5 14 shift 13 goto 15 15 shift 14 / reduce 4 reduce 4 reduce 4 goto 16 16 reduce 3 reduce 3 reduce 3 17 reduce 3 reduce 3 reduce 2 <p>The above parsing table is generated by filling up the cells based on the state machine diagram by differentiating the transition via a terminal symbol (<code>shift</code>) and a non-terminal symbol (<code>goto</code>).</p>"},{"location":"notes/syntax_analysis_2/#slr-parsing","title":"SLR parsing","text":"<p>One issue with the above <code>LR(0)</code> parsing table is that we see conflicts in cells with multiple actions, e.g. cell (<code>11</code>, <code>+</code>). This is also known as the shift-reduce conflict. It is caused by the over-approximation of the <code>ptable</code> function. In the <code>ptable</code> function, we blindly assign reduce actions to current state w.r.t. to all symbols.</p> <p>A simple fix to this problem is to consider only the symbols that follows the LHS non-terminal.</p> <pre><code>reduce :: [State] -&gt; [(Items, Symbol, Prod)] \nreduce states = \n  foldl (\\(acc, items) -&gt; \n    foldl (\\(acc1, item) -&gt; case item of \n      { (N::= alpha .) -&gt; acc1 ++ (map (\\s -&gt; (I, s N::=alpha) (follow N grammar)) -- the fix\n      ; _ -&gt; acc1)\n      }) acc (toList items)\n    ) [] states \n\nptable :: Grammar -&gt; Prod -&gt; [(State, Symbol, Action)] \nptable gramamr (S ::= X$) = \n  let init = closure (Set.fromList [S ::=.X$]) grammar\n  in case buildSM init grammar of \n  { StateMachine states trans finals -&gt; \n    let shifts = do \n        { (i, x, j) &lt;- trans\n        ; if isTerminal x\n        ; return (i, x, (Shift j))\n        }\n\n        gotos = do\n        { (i, x, j) &lt;- grans\n        ; if not (isTerminal x)\n        ; return (i, x, (Goto j))\n        }\n\n        reduces = do \n        { (i, x, N::=alpha) &lt;- reduce states --  the fix\n        ; return (i, x, reduce (N::=alpha))\n        }\n\n        accepts = do \n        { i &lt;- finals\n        ; return (i, $, Accept) \n        }\n    in shifts ++ gotos ++ reduces ++ accepts\n  }\n</code></pre> <p>The helper function <code>follow</code> compute the set of following symbols from the given non-terminal and grammar. Its definition is omitted as the details were discussed in the last lesson (top-down parsing).</p> <p>Given this fix, we are able to generate the conflict-free parsing table that we introduced earlier in this section.</p>"},{"location":"notes/syntax_analysis_2/#lr1-parsing-bonus-materials","title":"LR(1) Parsing (Bonus materials)","text":"<p>Besides <code>SLR</code>, <code>LR(1)</code> parsing also eliminates many conflicts found in <code>LR(0)</code>. The idea is to re-define item to include the look ahead token.</p> <p>For instance for production rule 3 <code>E' ::= +TE'</code>, we have 12 possible items</p> <ul> <li>(<code>E' ::= .+TE'</code>, <code>+</code>)</li> <li>(<code>E' ::= +.TE'</code>, <code>+</code>)</li> <li>(<code>E' ::= +T.E'</code>, <code>+</code>)</li> <li>(<code>E' ::= +TE'.</code>, <code>+</code>)</li> <li>(<code>E' ::= .+TE'</code>, <code>i</code>)</li> <li>(<code>E' ::= +.TE'</code>, <code>i</code>)</li> <li>(<code>E' ::= +T.E'</code>, <code>i</code>)</li> <li>(<code>E' ::= +TE'.</code>, <code>i</code>)</li> <li>(<code>E' ::= .+TE'</code>, <code>$</code>)</li> <li>(<code>E' ::= +.TE'</code>, <code>$</code>)</li> <li>(<code>E' ::= +T.E'</code>, <code>$</code>)</li> <li>(<code>E' ::= +TE'.</code>, <code>$</code>)</li> </ul> <p>We adjust the definition of <code>closure</code> and <code>goto</code></p> <pre><code>closure :: Items -&gt; Grammar -&gt; Items \nclosure items grammar = \n  let newItems = do \n      { (N ::= alpah . X beta, t) &lt;- items \n      ; (X ::= gamma)             &lt;- grammar\n      ; w                         &lt;- first (beta t) grammar\n      ; let js = do \n                 { (N ::= . epsilon, t) &lt;- items\n                 ; return (N::= epsilon ., t)}\n      ; return ( X ::= . gamma, w) `union` js}\n  if (newItems `isSubsetOf` items)\n  then items \n  else closure (items `union` newItems) grammar\n\ngoto :: Items -&gt; Grammar -&gt; Symbol -&gt; Items \ngoto items grammar sym = \n  let j = do \n        { (N ::= alpha . X beta, t) &lt;- items\n        ; return (N ::= alpha X . beta, t)\n        }\n  in closure j grammar\n</code></pre> <p>When computing the closure of an item <code>(N ::= alpha . X beta, t)</code>, we look up production rule <code>X ::= gamma</code>, to add <code>X ::= .gamma</code> into the new item set, we need to consider the possible leading terminal tokens coming from <code>beta</code>, and <code>t</code> in case <code>beta</code> accepts epsilon. The helper function <code>first</code> finds the set of first terminals in a sequence of symbols in the given grammar, which was discussed in the last unit (in top-down parsing.) We skip the details definition of <code>first</code> function. </p> <p>Applying the adjusted definition, we have the follow state diagram</p> <pre><code>graph\n  State10[\"State10 &lt;br/&gt; S'::=.E$, ? &lt;br/&gt; E::=.TE', $ &lt;br/&gt; T::=i, $ &lt;br/&gt; T::=i, + \"]--T--&gt;State11[\"State11 &lt;br/&gt; E::=T.E', $ &lt;br/&gt; E'::=+TE', $ &lt;br/&gt; E'::= . epsilon, $ &lt;br/&gt; E'::= epsilon., $\"]\n  State10--E--&gt;State12[\"State12 &lt;br/&gt; S'::=E.$, ?\"]\n  State10--i--&gt;State13[\"State13 &lt;br/&gt; T::=i., + &lt;br/&gt; T::=i., $\"]\n  State11--+--&gt;State14[\"State14 &lt;br/&gt; E'::= +.TE', $ &lt;br/&gt; T::=.i, + \"]\n  State11--E'--&gt;State17[\"State17 &lt;br/&gt; S'::=E.$, ?\"]\n  State14--i--&gt;State13\n  State14--T--&gt;State15[\"State15 &lt;br/&gt; E'::= +T.E', $ &lt;br/&gt; E'::=.+TE', $ &lt;br/&gt; E'::=.epsilon, $ &lt;br/&gt; E'::=epsilon., $\"]\n  State15--+--&gt;State14\n  State15--E'--&gt;State16[\"State16 &lt;br/&gt; E'::=+TE'., $\"]</code></pre> <p>For the top-most production rule, there is no leading token, we put a special symbol <code>?</code>, which does not affect the parsing.</p> <p>To incorporate item's new definition, we adjust the <code>reduce</code> function as follows</p> <pre><code>reduce :: [State] -&gt; [(Items, Symbol, Prod)] = \nreduce states = \n  foldl ( \\(acc, items) -&gt; \n    foldl (\\(acc1, item) -&gt; case item of \n      { (N :: =alpha ., t) -&gt; acc1 ++ [(I, t, N::=alpha)]\n      ;  _ -&gt; acc1 }) acc (toList items)\n    ) [] states\n</code></pre> <p><code>buildSM</code> and <code>ptable</code> function remain unchanged as per <code>SLR</code> parsing.</p> <p>By applying <code>ptable</code> we obtain the same parsing table as <code>SLR</code> parsing.</p>"},{"location":"notes/syntax_analysis_2/#slr-vs-lr1","title":"SLR vs LR(1)","text":"<p><code>LR(1)</code> covers a larger set of grammar than <code>SLR</code>. For example consider the following grammar.</p> <pre><code>&lt;&lt;Grammar 16&gt;&gt;\n1 S' ::= S$\n2 S ::= A a \n3 S ::= b A c\n4 S ::= d c \n5 S ::= b d a\n6 A ::= d\n</code></pre> <p><code>SLR</code> produces the following state diagram and parsing table.</p> <pre><code>graph\n  State10[\"State10 &lt;br/&gt; S'::=.S$&lt;br/&gt; S::=.Aa &lt;br/&gt; S::= .bAc&lt;br/&gt; S::=.dc &lt;br/&gt; S::=.bda &lt;br/&gt; A::=.d \"]--S--&gt;State11[\"State11 &lt;br/&gt; S'::=S.$\"]\n  State10--A--&gt;State12[\"State12 &lt;br/&gt; S::A.a\"]\n  State10--b--&gt;State13[\"State13 &lt;br/&gt; S::=b.Ac&lt;br/&gt; S::=b.da &lt;br/&gt; A::=.d\"]\n  State10--d--&gt;State14[\"State14 &lt;br/&gt; S::= d.c &lt;br/&gt; A::=d.\"]\n  State12--a--&gt;State15[\"State15 &lt;br/&gt; S::=Aa.\"]\n  State13--A--&gt;State16[\"State16 &lt;br/&gt; S::=bA.c\"]\n  State13--d--&gt;State17[\"State17 &lt;br/&gt; A::=d. &lt;br/&gt; S::=bd.a\"]\n  State14--c--&gt;State18[\"State18 &lt;br/&gt; S::=dc.\"]\n  State16--c--&gt;State19[\"State19 &lt;br/&gt; S::=bAc.\"]\n  State17--a--&gt;State20[\"State20 &lt;br/&gt; S::=bda.\"]  </code></pre> a b c d $ S' S A 10 shift 13 shift 14 goto 11 goto 12 11 accept 12 shift 15 13 shift 17 goto 16 14 reduce 6 shift 18 / reduce 6 15 reduce 2 16 shift 19 17 shift 20 / reduce 6 reduce 6 18 reduce 4 19 reduce 3 20 reduce 5 <p>There exist shift-reduce conflict. This is because in the closure computation when the item  <code>X::= . gamma</code> is added to the closure, we approximate the next leading token by <code>follow(X)</code>. However there might be other alternative production rule for <code>X</code> in the grammar. This introduces extraneous reduce actions.</p> <p><code>LR(1)</code> produces the following state diagram and parsing table.</p> <pre><code>graph\n  State10[\"State10 &lt;br/&gt; S'::=.S$, ? &lt;br/&gt; S::=.Aa, $ &lt;br/&gt; S::= .bAc, $ &lt;br/&gt; S::=.dc, $ &lt;br/&gt; S::=.bda, $ &lt;br/&gt; A::=.d, a \"]--S--&gt;State11[\"State11 &lt;br/&gt; S'::=S.$, ?\"]\n  State10--A--&gt;State12[\"State12 &lt;br/&gt; S::A.a, $\"]\n  State10--b--&gt;State13[\"State13 &lt;br/&gt; S::=b.Ac, $&lt;br/&gt; S::=b.da, $ &lt;br/&gt; A::=.d, c\"]\n  State10--d--&gt;State14[\"State14 &lt;br/&gt; S::= d.c, $ &lt;br/&gt; A::=d., a\"]\n  State12--a--&gt;State15[\"State15 &lt;br/&gt; S::=Aa., $\"]\n  State13--A--&gt;State16[\"State16 &lt;br/&gt; S::=bA.c,$\"]\n  State13--d--&gt;State17[\"State17 &lt;br/&gt; A::=d.,c &lt;br/&gt; S::=bd.a, $\"]\n  State14--c--&gt;State18[\"State18 &lt;br/&gt; S::=dc., $\"]\n  State16--c--&gt;State19[\"State19 &lt;br/&gt; S::=bAc., $\"]\n  State17--a--&gt;State20[\"State20 &lt;br/&gt; S::=bda., $\"]  </code></pre> a b c d $ S' S A 10 shift 13 shift 14 goto 11 goto 12 11 accept 12 shift 15 13 shift 17 goto 16 14 reduce 6 shift 18 15 reduce 2 16 shift 19 17 shift 20 reduce 6 18 reduce 4 19 reduce 3 20 reduce 5 <p>In which the shift-reduce conflicts are eliminated because when given an item <code>(N ::= alpha . X beta, t)</code>, we add <code>X ::= . gamma</code> into the closure, by computing <code>first (beta t) grammar</code>. This is only specific to this production rule <code>X::= gamma</code> and not other alternative.  </p>"},{"location":"notes/syntax_analysis_2/#lr1-and-left-recursion","title":"LR(1) and left recursion","text":"<p><code>LR(1)</code> can't handle all grammar with left recursion. For example processing Grammar 5  (from the previous unit) with <code>LR(1)</code> will result in some shift-reduce conflict.</p>"},{"location":"notes/syntax_analysis_2/#summary","title":"Summary","text":"<p>We have covered</p> <ul> <li>How to construct a <code>LR(0)</code> parsing table</li> <li>How to construct a <code>SLR</code> parsing table</li> </ul>"},{"location":"notes/syntax_analysis_annex/","title":"Syntax analysis annex","text":"Rule   Parse tree   Symbols   Input   (5)   graph   J--&gt;LB[\"{\"]   J--&gt;NS   J--&gt;RB[\"}\"]  {  NS }  {  ' k  1 ' : 1 , ' k 2 ' : [ ] }   graph   J--&gt;LB[\"{\"]   J--&gt;NS   J--&gt;RB[\"}\"]     NS }    ' k  1 ' : 1 , ' k 2 ' : [ ] }   (8)   graph   J--&gt;LB[\"{\"]   J--&gt;NS   J--&gt;RB[\"}\"]   NS--&gt;N   NS--&gt;,   NS--&gt;NS2[\"NS\"]   N,NS }    ' k  1 ' : 1 , ' k 2 ' : [ ] }   (10)   graph   J--&gt;LB[\"{\"]   J--&gt;NS   J--&gt;RB[\"}\"]   NS--&gt;N   NS--&gt;,   NS--&gt;NS2[\"NS\"]   N--&gt;LQ[']   N--&gt;s   N--&gt;RQ[']   N--&gt;:   N--&gt;J2[J]  ' s':J, NS }  ' k  1 ' : 1 , ' k 2 ' : [ ] }   graph   J--&gt;LB[\"{\"]   J--&gt;NS   J--&gt;RB[\"}\"]   NS--&gt;N   NS--&gt;,   NS--&gt;NS2[\"NS\"]   N--&gt;LQ[']   N--&gt;s   N--&gt;RQ[']   N--&gt;:   N--&gt;J2[J]  s':J, NS }  k 1 ' : 1 , ' k 2 ' : [ ] }   graph   J--&gt;LB[\"{\"]   J--&gt;NS   J--&gt;RB[\"}\"]   NS--&gt;N   NS--&gt;,   NS--&gt;NS2[\"NS\"]   N--&gt;LQ[']   N--&gt;S1(\"s(k1)\")   N--&gt;RQ[']   N--&gt;:   N--&gt;J2[J]  ':J, NS }  ' : 1 , ' k 2 ' : [ ] }   graph   J--&gt;LB[\"{\"]   J--&gt;NS   J--&gt;RB[\"}\"]   NS--&gt;N   NS--&gt;,   NS--&gt;NS2[\"NS\"]   N--&gt;LQ[']   N--&gt;S1(\"s(k1)\")   N--&gt;RQ[']   N--&gt;:   N--&gt;J2[J]  :J, NS }  : 1 , ' k 2 ' : [ ] }   graph   J--&gt;LB[\"{\"]   J--&gt;NS   J--&gt;RB[\"}\"]   NS--&gt;N   NS--&gt;,   NS--&gt;NS2[\"NS\"]   N--&gt;LQ[']   N--&gt;S1(\"s(k1)\")   N--&gt;RQ[']   N--&gt;:   N--&gt;J2[J];   J, NS }   1 , ' k 2 ' : [ ] }   (1)   graph   J--&gt;LB[\"{\"]   J--&gt;NS   J--&gt;RB[\"}\"]   NS--&gt;N   NS--&gt;,   NS--&gt;NS2[\"NS\"]   N--&gt;LQ[']   N--&gt;S1(\"s(k1)\")   N--&gt;RQ[']   N--&gt;:   N--&gt;J2[J]   J2--&gt;i;  i, NS }  1 , ' k 2 ' : [ ] }   graph   J--&gt;LB[\"{\"]   J--&gt;NS   J--&gt;RB[\"}\"]   NS--&gt;N   NS--&gt;,   NS--&gt;NS2[\"NS\"]   N--&gt;LQ[']   N--&gt;S1(\"s(k1)\")   N--&gt;RQ[']   N--&gt;:   N--&gt;J2[J]   J2--&gt;i[\"i(1)\"];  , NS }  , ' k 2 ' : [ ] }   graph   J--&gt;LB[\"{\"]   J--&gt;NS   J--&gt;RB[\"}\"]   NS--&gt;N   NS--&gt;,   NS--&gt;NS2[\"NS\"]   N--&gt;LQ[']   N--&gt;S1(\"s(k1)\")   N--&gt;RQ[']   N--&gt;:   N--&gt;J2[J]   J2--&gt;i[\"i(1)\"];   NS }   ' k 2 ' : [ ] }   (9)   graph   J--&gt;LB[\"{\"]   J--&gt;NS   J--&gt;RB[\"}\"]   NS--&gt;N   NS--&gt;,   NS--&gt;NS2[\"NS\"]   N--&gt;LQ[']   N--&gt;S1(\"s(k1)\")   N--&gt;RQ[']   N--&gt;:   N--&gt;J2[J]   J2--&gt;i[\"i(1)\"]   NS2--&gt;N2[N];   N }   ' k 2 ' : [ ] }   (10)   graph   J--&gt;LB[\"{\"]   J--&gt;NS   J--&gt;RB[\"}\"]   NS--&gt;N   NS--&gt;,   NS--&gt;NS2[\"NS\"]   N--&gt;LQ[']   N--&gt;S1(\"s(k1)\")   N--&gt;RQ[']   N--&gt;:   N--&gt;J2[J]   J2--&gt;i[\"i(1)\"]   NS2--&gt;N2[N]   N2--&gt;LQ2[']   N2--&gt;S2[s]   N2--&gt;RQ2[']   N2--&gt;CL2[\":\"]   N2--&gt;J3[J];  's':J }  ' k 2 ' : [ ] }   graph   J--&gt;LB[\"{\"]   J--&gt;NS   J--&gt;RB[\"}\"]   NS--&gt;N   NS--&gt;,   NS--&gt;NS2[\"NS\"]   N--&gt;LQ[']   N--&gt;S1(\"s(k1)\")   N--&gt;RQ[']   N--&gt;:   N--&gt;J2[J]   J2--&gt;i[\"i(1)\"]   NS2--&gt;N2[N]   N2--&gt;LQ2[']   N2--&gt;S2[s]   N2--&gt;RQ2[']   N2--&gt;CL2[\":\"]   N2--&gt;J3[J];  s':J }  k 2 ' : [ ] }   graph   J--&gt;LB[\"{\"]   J--&gt;NS   J--&gt;RB[\"}\"]   NS--&gt;N   NS--&gt;,   NS--&gt;NS2[\"NS\"]   N--&gt;LQ[']   N--&gt;S1(\"s(k1)\")   N--&gt;RQ[']   N--&gt;:   N--&gt;J2[J]   J2--&gt;i[\"i(1)\"]   NS2--&gt;N2[N]   N2--&gt;LQ2[']   N2--&gt;S2[\"s(k2)\"]   N2--&gt;RQ2[']   N2--&gt;CL2[\":\"]   N2--&gt;J3[J];  ':J }  ' : [ ] }   graph   J--&gt;LB[\"{\"]   J--&gt;NS   J--&gt;RB[\"}\"]   NS--&gt;N   NS--&gt;,   NS--&gt;NS2[\"NS\"]   N--&gt;LQ[']   N--&gt;S1(\"s(k1)\")   N--&gt;RQ[']   N--&gt;:   N--&gt;J2[J]   J2--&gt;i[\"i(1)\"]   NS2--&gt;N2[N]   N2--&gt;LQ2[']   N2--&gt;S2[\"s(k2)\"]   N2--&gt;RQ2[']   N2--&gt;CL2[\":\"]   N2--&gt;J3[J];  :J }  : [ ] }   graph   J--&gt;LB[\"{\"]   J--&gt;NS   J--&gt;RB[\"}\"]   NS--&gt;N   NS--&gt;,   NS--&gt;NS2[\"NS\"]   N--&gt;LQ[']   N--&gt;S1(\"s(k1)\")   N--&gt;RQ[']   N--&gt;:   N--&gt;J2[J]   J2--&gt;i[\"i(1)\"]   NS2--&gt;N2[N]   N2--&gt;LQ2[']   N2--&gt;S2[\"s(k2)\"]   N2--&gt;RQ2[']   N2--&gt;CL2[\":\"]   N2--&gt;J3[J];   J }   [ ] }   (3)   graph   J--&gt;LB[\"{\"]   J--&gt;NS   J--&gt;RB[\"}\"]   NS--&gt;N   NS--&gt;,   NS--&gt;NS2[\"NS\"]   N--&gt;LQ[']   N--&gt;S1(\"s(k1)\")   N--&gt;RQ[']   N--&gt;:   N--&gt;J2[J]   J2--&gt;i[\"i(1)\"]   NS2--&gt;N2[N]   N2--&gt;LQ2[']   N2--&gt;S2[\"s(k2)\"]   N2--&gt;RQ2[']   N2--&gt;CL2[\":\"]   N2--&gt;J3[J]   J3--&gt;LSQ[\"[\"]   J3--&gt;RSQ[\"]\"];  [ ] } [ ] }  graph   J--&gt;LB[\"{\"]   J--&gt;NS   J--&gt;RB[\"}\"]   NS--&gt;N   NS--&gt;,   NS--&gt;NS2[\"NS\"]   N--&gt;LQ[']   N--&gt;S1(\"s(k1)\")   N--&gt;RQ[']   N--&gt;:   N--&gt;J2[J]   J2--&gt;i[\"i(1)\"]   NS2--&gt;N2[N]   N2--&gt;LQ2[']   N2--&gt;S2[\"s(k2)\"]   N2--&gt;RQ2[']   N2--&gt;CL2[\":\"]   N2--&gt;J3[J]   J3--&gt;LSQ[\"[\"]   J3--&gt;RSQ[\"]\"];"},{"location":"notes/syntax_analysis_parser_combinator/","title":"50.054 - Top-down recursive parsing using Parser Combinators","text":""},{"location":"notes/syntax_analysis_parser_combinator/#learning-outcome","title":"Learning Outcome","text":"<p>By the end of this class, you should be able to </p> <ul> <li>Implement a top-down recursive parser with backtracking</li> <li>Implement a top-down recursive parser with on-demand backtracking </li> </ul>"},{"location":"notes/syntax_analysis_parser_combinator/#recap-top-down-parsing","title":"(Recap) Top-down parsing","text":"<p>In this secion we are going to focus on implementing Top-down parser.</p> <p>Recall the grammar of a math expression. </p> <pre><code>&lt;&lt;grammar 4&gt;&gt; \nE::= T + E\nE::= T\nT::= T * F \nT::= F\nF::= i    \n</code></pre>"},{"location":"notes/syntax_analysis_parser_combinator/#abstract-syntax-tree","title":"Abstract Syntax Tree","text":"<p>To implement top-down parsing, we first consider how to represent a parse tree in Haskell.  It's natural to implement the parse trees in terms of some algebraic datatype.  The Grammar 4 can be encoded with the following Haskell data type.</p> <pre><code>data Exp = TermExp Term | PlusExp Term Exp \n    deriving (Show, Eq) \n\ndata Term = FactorTerm Factor | MultTerm Term Factor \n    deriving (Show, Eq)\n\ndata Factor = Factor Int\n    deriving (Show, Eq)\n</code></pre>"},{"location":"notes/syntax_analysis_parser_combinator/#left-recursion-elimination","title":"Left Recursion Elimination","text":"<p>Recall Grammar 4 defined above contains some left recursion. </p> <p>To eliminate the left recursion, we apply the same trick by rewriting left recursive grammar rules $$ \\begin{array}{rcl} N &amp; ::= &amp; N\\alpha_1 \\ &amp; ... &amp; \\ N &amp; ::= &amp; N\\alpha_n \\ N &amp; ::= &amp; \\beta_1 \\  &amp; ... &amp; \\ N &amp; ::= &amp; \\beta_m  \\end{array} $$</p> <p>into</p> \\[ \\begin{array}{rcl} N &amp; ::= &amp; \\beta_1 N' \\\\ &amp; ... &amp; \\\\ N &amp; ::= &amp; \\beta_m N' \\\\ N' &amp; ::= &amp; \\alpha_1 N' \\\\  &amp; ... &amp; \\\\ N' &amp; ::= &amp; \\alpha_n N' \\\\ N' &amp; ::= &amp; \\epsilon \\end{array} \\] <p>Grammar 4 can be rewritten into</p> <pre><code>E  ::= T + E\nE  ::= T\nT  ::= FT'\nT' ::= *FT'\nT' ::= epsilon\nF  ::=i\n</code></pre> <p>None of the production rules above contains common leading terminal symbols, hence there is no need to apply left-factorization.</p> <p>Note that in the above Grammar 4 with left recursion eliminated, the only rules affected are thos with non-terminal <code>T</code>,</p> <p>Hence we only need to added the following enum type</p>"},{"location":"notes/syntax_analysis_parser_combinator/#additional-abstract-syntax-tree","title":"Additional Abstract Syntax Tree","text":"<p>We could model it using Algebraic data type. <pre><code>data TermLE = TermLE Factor TermLEP \n\ndata TermLEP = MultTermLEP Factor TermLEP | Eps \n</code></pre></p> <p>The main idea is when parsing a <code>Term</code>, instead of parsing directly, we parse a <code>TermLE</code> then convert it back to <code>Term</code>.</p> <p><pre><code>[IntTok 1, PlusTok, IntTok 2, AsterixTok, IntTok 3]\n</code></pre> A parser method <code>parseExp</code> should generate</p> <p><pre><code>PlusExp (FactorTerm (Factor 1)) (TermExp (MultTerm (FactorTerm (Factor 2)) (Factor 3)))\n</code></pre> where </p> <ul> <li>sub term <code>IntTok 1</code> was first parsed as <code>TermLE (Factor 1) Eps</code> then converted to <code>FactorTerm (Factor 1)</code>, and </li> <li>sub term <code>IntTok 2, AsterixTok, IntTok 3</code> was first parsed as <code>TermLE (Factor 2) (MultTerm (Factor 3) Eps)</code> and converted to <code>MultTerm (Factor 2) (Factor 3)</code>.</li> </ul>"},{"location":"notes/syntax_analysis_parser_combinator/#parser-combinator-with-backtracking","title":"Parser Combinator with Backtracking","text":"<p>We consider implementing the naive top-down recursive parser in Haskell.  Let's start with the simplest cases. Let's say we would like to write a parsing function that takes a list of lexical tokens and \"consumes\" a token, then return the rest.</p> <p><pre><code>data Result a = Failed String | Ok a \n\nitem :: [LToken] -&gt; Result (LToken,[LToken])\nitem [] = Failed \"item is called with an empty input\"\nitem (t:ts) = Ok (t,ts)\n</code></pre> We define a variant of the <code>Maybe</code> datatype, <code>Result</code> which is either a failure with an error message, or an \"Ok\" result. The <code>item</code> function is what we would like to implement. It returns the extracted leading token with the rest of input if the input is non-empty, and signals a failure otherwise. </p> <p>Apply the same idea we could define a conditional parsing function.</p> <pre><code>sat :: [LToken] -&gt; (LToken -&gt; Bool) -&gt; Result (LToken,[LToken])\nsat [] _ = Failed \"sat is called with an empty input\"\nsat (t:ts) p = if p t\n    then Ok (t,ts) \n    else Failed \"sat is called with an input that does not satisfy the input predicate.\"\n</code></pre> <p>We may want to combine these basic parsing functions to form a larger parsing task, e.g.</p> <pre><code>aBitMoreComplexParser :: [LToken] -&gt; Result (LToken,[LToken])\naBitMoreComplexParser toks = case item toks of \n    { Failed msg -&gt; Failed msg \n    ; Ok (_, toks2) -&gt; sat toks2 (\\t -&gt; case t of \n          { AsterixTok -&gt; True\n          ; _          -&gt; False}) \"Expecting an asterix.\"\n    }\n</code></pre> <p>In the above, we define a parsing task which skips the first token and searches for the following asterix. We could imagine that to build a practical parser, we would need many of the basic parsing functions like <code>sat</code> and <code>item</code>, and combine them.</p> <p>What we can observe from the above is that there are some similarity between <code>sat</code> and <code>item</code>, i.e. they both take in a list of tokens and returns the remaining tokens. If we view the lists of tokens as states, we could think of using the State Monad. We would also need this top-down parser to be able to backtrack in case of parsing failures. Recall the <code>MonadError</code> type class</p> <p><pre><code>class Monad m =&gt; MonadError e m | m -&gt; e where\n    throwError :: e -&gt; m a \n    catchError :: m a -&gt; (e -&gt; m a) -&gt; m a\n</code></pre> We define the <code>Parser</code> data type as follows, similar to the <code>State</code> case data type in the State Monad, except that we replace the <code>state</code> by the (parametric) parser environment type <code>env</code> </p> <pre><code>type Error = String \n\ndata Result a = Ok a\n    | Failed String\n    deriving (Show, Eq)\n\nnewtype Parser env a =\n    Parser { run :: env -&gt; Result (a, env) }\n</code></pre> <p><code>newtype</code> is like a special <code>data</code> type definition where there is no alternative.</p> <p>Next we provide the required type class instances to make <code>Parser env</code> into a <code>MonadError</code> instance. </p> <pre><code>instance Functor (Parser env) where\n    fmap f (Parser ea) = Parser ( \\env -&gt; case ea env of\n    { Failed err -&gt; Failed err\n    ; Ok (a, env1) -&gt; Ok (f a, env1)\n    })\n\n\ninstance Applicative (Parser env) where\n    pure a = Parser (\\env -&gt; Ok (a, env))\n    (Parser eab) &lt;*&gt; (Parser ea) = Parser (\\env -&gt; case eab env of\n        { Failed msg -&gt; Failed msg\n        ; Ok (f, env1) -&gt; case ea env1  of\n            { Failed msg -&gt; Failed msg\n            ; Ok (a, env2) -&gt; Ok (f a, env2)\n            }\n        })\n\ninstance Monad (Parser env) where\n    (Parser ea) &gt;&gt;= f = Parser (\\env -&gt; case ea env of\n        { Failed err   -&gt; Failed err\n        ; Ok (a, env1) -&gt; case f a of\n            { Parser eb -&gt; eb env1 }\n        })\n\ninstance MonadError Error (Parser env) where\n    throwError msg = Parser (\\env -&gt;  Failed msg)\n    catchError (Parser ea) handle = Parser (\\env -&gt; case ea env of\n        { Failed msg -&gt; case handle msg of\n            { Parser ea2 -&gt; ea2 env }\n        ; Ok v -&gt;  Ok v\n        })\n</code></pre> <p>With the Monad instance in-place, let's consider the \"requirements\" of the parser environment type <code>env</code>. We use the following type class to constraint any implementation of the parser environment must fulfill these requirements. </p> <pre><code>class ParserEnv env tok | env -&gt; tok\n    where\n        getTokens :: env -&gt; [tok]\n        getLine :: env -&gt; Int\n        getCol :: env -&gt; Int\n        setTokens :: [tok] -&gt; env -&gt; env\n        setLine :: Int -&gt; env -&gt; env\n        setCol :: Int -&gt; env -&gt; env\n        isNextTokNewLine :: env -&gt; Bool\n</code></pre> <p>Now we can re-define the <code>item</code> and <code>sat</code> function by making use of the monad (error) abstraction. </p> <pre><code>-- | The `item` combinator unconditionally parse the leading token. \nitem :: ParserEnv env tok =&gt; Parser env tok\nitem = Parser (\\env -&gt;\n    let toks = getTokens env\n        ln   = getLine env\n        col  = getCol env\n    in case toks of\n        { [] -&gt; Failed \"item is called with an empty token stream.\"\n        ; (c : cs) | isNextTokNewLine env -&gt;\n            let env1 = setLine (ln+1) env\n                env2 = setCol 1 env1\n                env3 = setTokens cs env2\n            in Ok (c, env3)\n                   | otherwise -&gt;\n            let env1 = setCol (col+1) env\n                env2 = setTokens cs env1\n            in Ok (c, env2)\n        })\n\n-- | The `sat` combinator consume the leading token if it satifies the predicate `p`.\nsat :: ParserEnv env tok =&gt; (tok -&gt; Bool) -&gt; Error -&gt; Parser env tok\nsat p dbgMsg = Parser (\\env -&gt;\n    let toks = getTokens env\n        ln   = getLine env\n        col  = getCol env\n    in case toks of\n        { [] -&gt; Failed (\"sat is called with an empty token stream at line \" ++ show ln ++ \", col \" ++ show col ++ \". \" ++ dbgMsg)\n        ; (c:cs) | p c &amp;&amp; isNextTokNewLine env -&gt;\n            let env1 = setLine (ln+1) env\n                env2 = setCol 1 env1\n                env3 = setTokens cs env2\n            in Ok (c, env3)\n        ; (c:cs) | p c -&gt;\n            let env1 = setCol (col+1) env\n                env2 = setTokens cs env1\n            in Ok (c, env2)\n        ; (c:cs) -&gt; Failed (\"sat is called with an unsatisfied predicate at line \" ++ show ln ++ \", col \" ++ show col ++ \". \" ++ dbgMsg)\n        }\n    )\n</code></pre> <p>The <code>| condExp</code> defines a pattern guard in Haskell. For instance, in the <code>item</code> function above, the pattern case <code>(c : cs) | isNextTokNewLine env</code> is triggered  when the incoming tokens <code>toks</code> are of shape <code>c:cs</code> and the <code>isNextTokenNewLine env</code> is <code>True</code>. Compared to the naive version of <code>item</code> defined earlier, this version abstracts away the details of the tokens retrieval and update. It also handles the book-keeping of the current token position in the source file. Like-wise <code>sat</code> function is updated to support the same features. </p> <p>More importantly, we could define more generic and useful combinators</p> <pre><code>choice :: Parser env a -&gt; Parser env a -&gt; Parser env a\nchoice p q = catchError p (\\e -&gt; q)\n</code></pre> <p>The <code>choice</code> combinator takes two parsers <code>p</code> and <code>q</code>. It tries to run <code>p</code>. In case <code>p</code> fails, it backtracks (by restoring the original state) and runs <code>q</code>. </p> <p>Now we can make use of <code>choice</code> to define an <code>optional</code> combinator <pre><code>optional :: Parser env a -&gt; Parser env (Either () a)\noptional pa =\n    let p1 = Right &lt;$&gt; pa\n        p2 = return (Left ())\n    in choice p1 p2\n</code></pre></p> <p><code>optional</code> takes a parser <code>pa</code> and tries to execute it with the current input. If it fails, it restores the original state and returns <code>()</code>.</p> <p>Let's try to write a parser for the simple arithmetic expression </p> <p>Recall the nice property of a top-down parser is that the parser is correspondent to the top-down traversal of the production rules.</p> <p>We provide the concrete definition of the parser environment data type as follows,</p> <pre><code>data PEnv = PEnv { toks:: [LToken]} deriving (Show, Eq) \n\ninstance ParserEnv PEnv LToken where\n    getCol penv           = -1 -- ^ not in used. \n    getLine penv          = -1 -- ^ not in used. \n    setTokens ts penv     = penv{toks= ts}\n    setLine _ penv        = penv -- ^ not in used.\n    setCol _ penv         = penv -- ^ not in used. \n    isNextTokNewLine penv = False -- ^ not in used. \n    getTokens             = toks   \n</code></pre> <p>Recall the grammar 4 of Math expression with left recursion.</p> <pre><code>E::= T + E\nE::= T\nT::= T * F \nT::= F\nF::= i    \n</code></pre> <pre><code>parseExp :: Parser PEnv Exp \nparseExp = choice parsePlusExp parseTermExp\n\nparsePlusExp :: Parser PEnv Exp\nparsePlusExp = do\n    t    &lt;- parseTerm \n    plus &lt;- parsePlusTok\n    e    &lt;- parseExp \n    return (PlusExp t e)\n\nparseTermExp :: Parser PEnv Exp \nparseTermExp = do \n    t &lt;- parseTerm \n    return (TermExp t)\n</code></pre> <p>Up to this point we are ok as production rules with <code>E</code> on the LHS are not left recursive. It gets tricky when paarsing <code>T</code> which contains left recursion. Recall the modified grammar of  <code>T</code> having left-recursion eliminated.</p> <pre><code>T  ::= FT'  \nT' ::= *FT'\nT' ::= epsilon\n</code></pre> <p>In terms of Haskell data type, we refer to them as <code>TermLE</code> and <code>TermLEP</code>. Hence the parser <code>parserTerm</code> has to be defined in terms of <code>parseTermLE</code>, then  convert the result of <code>TermLE</code> back to <code>Term</code></p> <pre><code>parseTerm :: Parser PEnv Term \nparseTerm = do \n    tle &lt;- parseTermLE \n    return (fromTermLE tle) \n</code></pre> <p>Where <code>parseTermLE</code> can be implemented using parsec, </p> <pre><code>parseTermLE :: Parser PEnv TermLE \nparseTermLE = do \n    f  &lt;- parseFactor\n    tp &lt;- parseTermP \n    return (TermLE f tp)\n\nparseTermP :: Parser PEnv TermLEP \nparseTermP = do \n    omt &lt;- optional parseMultTermP\n    case omt of \n        Left _ -&gt; return Eps\n        Right t -&gt; return t\n\n\nparseMultTermP :: Parser PEnv TermLEP \nparseMultTermP = do \n    asterix &lt;- parseAsterixTok \n    f       &lt;- parseFactor\n    tp      &lt;- parseTermP \n    return (MultTermLEP f tp)\n\nparseFactor :: Parser PEnv Factor \nparseFactor = do \n    i &lt;- parseIntTok \n    f &lt;- justOrFail i ( \\ itok -&gt; case itok of \n        { IntTok v -&gt; Just (Factor v)\n        ; _ -&gt; Nothing \n        }) \"parseFactor fail: expect to parse an integer token but it is not an integer.\"\n    return f\n\n\nparsePlusTok :: Parser PEnv LToken\nparsePlusTok = sat ( \\x -&gt; case x of \n    { PlusTok -&gt; True\n    ; _       -&gt; False\n    }) \"parsePlusTok failed, expecting a plus token.\"\n\nparseAsterixTok :: Parser PEnv LToken \nparseAsterixTok = sat ( \\x -&gt; case x of \n    AsterixTok -&gt; True\n    _          -&gt; False ) \"parseAsterixTok failed, expecting an asterix token.\"\n\n\nparseIntTok :: Parser PEnv LToken \nparseIntTok = sat ( \\x -&gt; case x of \n    IntTok v -&gt; True\n    _        -&gt; False ) \"parseIntTok failed, expecting an integer token.\"\n</code></pre> <p>Finally the <code>TermLE</code> to <code>Term</code> conversion is an inversed in order traversal, as the parse tree of <code>TermLE</code></p> <pre><code>    T\n   / \\\n  f   T'\n     /|\\\n    * f T'\n       /|\\\n      * f T'\n          |\n         eps\n</code></pre> <p>and the parse tree of <code>Term</code> is</p> <pre><code>        T\n       /|\\\n      T * f\n     /| \\\n    T * f \n    |\n    f\n</code></pre> <p>The implementation can be found as follows,</p> <pre><code>fromTermLE :: TermLE -&gt; Term \nfromTermLE (TermLE f tep) = fromTermLEP (FactorTerm f) tep \n\nfromTermLEP :: Term -&gt; TermLEP -&gt; Term \nfromTermLEP t1 Eps = t1 \nfromTermLEP t1 (MultTermLEP f2 tp2) = \n    let t2 = MultTerm t1 f2\n    in fromTermLEP t2 tp2\n</code></pre> <p>And here is some test cases</p> <pre><code>it \"test parse 1+2*3\" $\n    let toks = [IntTok 1, PlusTok, IntTok 2, AsterixTok, IntTok 3]\n        result = run parseExp (PEnv toks) \n        expected = Ok (PlusExp (FactorTerm (Factor 1)) (TermExp (MultTerm (FactorTerm (Factor 2)) (Factor 3))), PEnv [])\n    in result `shouldBe` expected\n</code></pre>"},{"location":"notes/syntax_analysis_parser_combinator/#parser-combinator-without-backtracking-in-spirit-of-ll1","title":"Parser Combinator without backtracking (In spirit of LL(1))","text":"<p>Let's extend our parser combinator to support <code>LL(1)</code> parsing without backtracking.</p> <p>We introduce the following algebraic datatype to label an (intermediate) parsing result  <pre><code>data Progress a = Consumed a\n    | Empty a\n    deriving Show\n</code></pre></p> <p>The partial is is <code>Consumed</code> when there has been input tokens consumed, otherwise, <code>Empty</code>.</p> <p>We adjust the definition of the <code>Parser</code> case class as follows</p> <pre><code>newtype Parser env a =\n    Parser { run :: env -&gt; Progress (Result (a, env)) }\n</code></pre> <p>Next we update the type class instances accordingly. </p> <pre><code>instance Functor (Parser env) where\n    fmap f (Parser ea) = Parser ( \\env -&gt; case ea env of\n    { Empty (Failed err) -&gt; Empty (Failed err)\n    ; Empty (Ok (a, env1)) -&gt; Empty (Ok (f a, env1))\n    ; Consumed (Failed err) -&gt; Consumed (Failed err)\n    ; Consumed (Ok (a, env1)) -&gt; Consumed (Ok (f a, env1))\n    })\n\n\n\ninstance Applicative (Parser env) where\n    pure a = Parser (\\env -&gt; Empty (Ok (a, env)))\n    (Parser eab) &lt;*&gt; (Parser ea) = Parser (\\env -&gt; case eab env of\n        { Consumed v -&gt;\n            let cont = case v of\n                { Failed msg -&gt; Failed msg\n                ; Ok (f, env1) -&gt; case ea env1 of\n                    { Consumed (Failed msg) -&gt; Failed msg\n                    ; Consumed (Ok (a, env2)) -&gt; Ok (f a, env2)\n                    ; Empty (Failed msg) -&gt; Failed msg\n                    ; Empty (Ok (a, env2)) -&gt; Ok (f a, env2)\n                    }\n                }\n            in Consumed cont\n        ; Empty (Failed msg) -&gt; Empty (Failed msg)\n        ; Empty (Ok (f, env1)) -&gt; case ea env1  of\n            { Consumed (Failed msg) -&gt; Consumed (Failed msg)\n            ; Consumed (Ok (a, env2)) -&gt; Consumed (Ok (f a, env2))\n            ; Empty (Failed msg) -&gt; Empty (Failed msg)\n            ; Empty (Ok (a, env2)) -&gt; Empty (Ok (f a, env2))\n            }\n        })\n\n\ninstance Monad (Parser env) where\n    (Parser ea) &gt;&gt;= f = Parser (\\env -&gt; case ea env of\n            { Consumed v -&gt;\n                let cont = case v of\n                    { Failed msg -&gt; Failed msg\n                    ; Ok (a, env1)  -&gt; case f a of\n                        { Parser eb -&gt; case eb env1 of\n                            { Consumed x -&gt; x\n                            ; Empty x    -&gt; x\n                            }\n                        }\n                    }\n                in Consumed cont\n            ; Empty v -&gt; case v of\n                { Failed err   -&gt; Empty (Failed err)\n                ; Ok (a, env1) -&gt; case f a of\n                    { Parser eb -&gt; eb env1 }\n                }\n            })\n</code></pre> <p>Let's look at the <code>&gt;&gt;=</code>, the Monadic bind. It takes the first parser (<code>Parser ea</code>) and pattern-matches it against <code>Parser(p)</code>. In the output parser, we first apply <code>ea</code> to the input environment <code>env</code>. </p> <ul> <li>If the result's progress is <code>Empty v</code>, we check whether <code>v</code> is an error or <code>Ok</code>. When it is an error, it will be propogated, otherwise, we apply <code>f</code> to the output of <code>a</code>. That will give us the second parser to continue with, <code>Parser eb</code>. We then apply <code>eb</code> to <code>env1</code> which should be the same as <code>env</code> since nothing has been consumed. </li> <li>If the result's progress is <code>Consumed v</code>, some part of input tokens in the environment <code>env</code> has been parsed. The parser's behaviour here should be similar to the previous case, except that <code>eb env1</code> progress result will always be updated as <code>Consumed</code> regardless whether <code>eb</code> has consumed anything. Thanks Haskell lazy evaluation, the result held by variable <code>cont</code> is delayed until it is actually needed. Such an optimization allows us to return the progress information <code>Consumed</code> without actually executing <code>eb  env1</code> when its result is not needed.</li> </ul> <p>Similar observation applies to <code>&lt;*&gt;</code> function in the applicative instance.</p> <p>The defintion of the <code>MonadError</code> type class instance is adjusted to recognize  the <code>Consumed</code> and <code>Empty</code> data. </p> <pre><code>instance MonadError Error (Parser env) where\n    throwError msg = Parser (\\env -&gt; Empty (Failed msg))\n    catchError (Parser ea) handle = Parser (\\env -&gt; case ea env of\n        { Consumed v -&gt; Consumed v -- we don't backtrack when something is already consumed.\n        ; Empty (Failed msg) -&gt; case handle msg of\n            { Parser ea2 -&gt; ea2 env\n            }\n        ; Empty (Ok v) -&gt; -- LL(1) parser will also attempt to look at f if fa does not consume anything \n            case handle \"faked error\" of\n                {  Parser ea2 -&gt; case ea2 env of\n                    { Empty _ -&gt; Empty (Ok v) -- if handle also fails, we report the same error.\n                    ; consumed -&gt; consumed\n                    }\n                }\n        })\n</code></pre> <p>The highlight is in the <code>catchError</code> function,  we do not backtrack when the  the first parser <code>(Parser ea)</code> has consumed some input.  The error recovery method <code>handle</code> is applied only when the progress of the parsing so far is <code>Empty</code>. In other words, given a choice of two parsers <code>choice p1 p2</code>, it will not backtrack to <code>p2</code> if <code>p1</code> has consumed some token. This is in-sync with predictive parsing.</p> <p>All other combinators such as <code>choice</code>, <code>item</code>, <code>sat</code>, <code>optional</code> can be adjusted in the same fashion. We refer to the cohort exercise template code <code>Parsec.hs</code> for details. </p> <p>We know that not all languages are in <code>LL(1)</code>.  It is undecidable to find out which <code>k</code> of <code>LL(k)</code> that a language is in. Thus, the above parser might not be very useful since it only works if the given language is in <code>LL(1)</code>. </p> <p>Thanks to the Monadic design, it is very easy to extend the parser to accept non <code>LL(1)</code> language by supporting backtracking on-demand. That is, the parser by default is not backtracking, however, it could if we want it to backtrack explicitly.</p> <p><pre><code>-- | The `attempt` combinator explicitly tries a parser and backtracks if it fails.\nattempt :: Parser env a -&gt; Parser env a\nattempt (Parser ea) = Parser (\\env -&gt; case ea env of\n    { Consumed (Failed err) -&gt; Empty (Failed err) -- undo the consumed effect if it fails. \n    ; other -&gt; other\n    })\n</code></pre> The <code>attempt</code> combinator takes a parser <code>Parser ea</code> and runs it. If its result is <code>Consumed</code> but <code>Failed</code>, it will reset the progress as <code>Empty</code>.  </p>"}]}